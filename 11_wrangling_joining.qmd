# Wrangling and Joining Data {#sec-11wrangling-joining}

As a data scientists one of the things that you are asked to do is to transform data and extend it further than just the observations presented into a flat file, such as a spreadsheet program.  *How* you transform the data depends on the scientific questions you are exploring and will determine which operations you will use (which can include computing summary statistics, applying a mathematical model). While a spreadsheet program can do all the aforementioned tasks, data scientists typically work with multiple variables (such as the stream data presented in Chapter XXX). The number of observations is typically too large to easily view in a spreadsheet program; Microsoft Excel is limited to about a million observations and 16 thousand variables - which may seem like a lot, but can easily get swamped when you have several sites collated together. Developing skills in data wrangling helps build some efficiency of scale - and places the emphasis on *action* - what do you want to do with your data today?  This chapter is focused on data wrangling and connecting two data tables together. Let's begin.

## What's the weather like?
Minnesota loves to talk about its weather [LINK](https://www.mprnews.org/story/2024/03/05/weatherobsessed-minnesota-leads-the-nation-in-weather-observers), which is a source of pride for many residents (some would say misplaced pride because it can be so cold ...). But the good part is that pride has produced a strong tradition in long-term, high quality datasets. 
The context that we will be working with is the environmental dataset from Minneapolis St Paul, which is daily weather observations across an entire year.  (https://mesonet.agron.iastate.edu/request/download.phtml?network=MN_ASOS) This dataset is an ideal place to start because it contains a mixture of 30 variables.  Let's say that you have read in the dataset (as described in previous chapters).  Some good questions we could ask of these data are:


- Number of observations per day?
- Cumulative precipitation (rain and snow) for the year?
- Warmest day of the year? Coldest day of the year?
- Warmest day of the month? Coldest day of the month? (top 5?)
- What is the average daily high / low.
- What is the average monthly temperature? [[ mean value theorem??? Because the data aren’t regular?]]
- Difference between windchill and measured temperatures?

One observation is thinking about dewpoint - because of the wide range in temperature, Minnesota can experience wide swings in heat temperature when air from the Gulf of Mexico spreads north - and sends our modestly cool temperature into muggy value.  We measure that with dewpoints, which is a function of temperature and relative humidity (show formula)

Similar to what we did in previous chapters is to read the dataset in
The first step is to see your data once it is imported in - this is a very important step!  In tidyverse the glimpse function provides the ability to scan your data in a neat compact way (compare this to printing out the entire dataset at once).  For larger datasets this is important - you don’t want to display all the data, just make sure it read in correctly  (alternatives are just to refer the first rows [1:10,] or info or scan (??? unsure of the equivalent).

The next step is to define good questions to work with your data.  Even though the dewpoint is given, we can compute it from the temperature and relative humidity: https://en.wikipedia.org/wiki/Dew_point.  Essentially we are creating a new variable here.  This is a good check to practice our vectorized thinking!

If dewpoint wasn’t given to us, then we would need to approximate the dewpoint with the formula (several are given, but let’s see how that works).  The idea of creating a new variable from existing ones is mutating (tidyverse) or transforming (python).  We can do this for any combination of variables that we want to create.  [[ Exercise is the windchill formulas … ]]



## Vectors on my mind
The first key aspect to data wrangling is moving from a single observation to a collection of information - vectorizing the data.  Tidy data (CHAPTER REFERENCE) is the first key do doing that, but rather than saying you have a XXXX observations of a field season, you consider the group of measurements from a plot together with several variables.  Schematically this looks like this:

DIAGRAM HERE


While this may seem like sleight of hand, it can help reduce cognitive load for analysis. In this way we can consider a single action to a variable (which consists of a set of observations) - rather than actions on each individual operation.


Let's work with the soil temperature data. It is provided to you in Celsius, but you want to convert it to Kelvin.  For each data point you will add 273.15 - it is the same for each observation. If you were in a spreadsheet program, you would perhaps create a new column with a formula and reference to the cell in the previous column over, and then "fill-in" the formula, which applies the same consistent formula across everything. This works especially well for medium sized datasets.

In a statistical software program, generalizing the approach helps simplify everything.  The formula for a single observation (where $\vec{C}$ is the temperature in Celsius, $\vec{K}$ for Kelvin) is $\vec{K}=\vec{C}+273.15$, where the arrows adorning the variables represent a set (or collection) of observations. If we refer to the variable as an entire vector (say `temp_celsius`), this is pretty easy to write out (`temp_kelvin <- temp_celsius + 273.15`).


A second approach is to compute the mean of a dataset - so what is the average temperature (in Celsius) across the year.  For most programs (even spreadsheets) - you can compute this using the function `mean`.  What you are doing in this case (especially for the simple mean) - is take the entire set of observations to produce a single result (the formula in this case is more mathematical: $\displaystyle \overline{C}=\sum_{i=1}^{N} \frac{1}{N} \; C_{i}$, where $C_{i}$ is an individual observation of $\vec{C}$, where there are $N$ of them).  Another way to represent this mathematically is the function $\overline{C}=f(\vec{C})$, where the function $f$ represents the mean.

While this mathematical notation is useful, in the language of data science what you are doing in the first case is mutating the dataset (transformation), and in the second case you are summarizing or "rolling up" the data - creating a single summary observation.

There is a third option two, where you want to create a vector of observations from a nested group - perhaps you want to compute monthly temperature averages.  This is an example of grouping or iteration, which we discuss in chapter XXX.


IFor a small dataset it may be easy enough to do by hand - if you have data cross an entire year (approximately 17000 observations, that gets too tricky!) One approach is to manually t.  The provided temperature is given in Celsius, but you want to know the temperature of the data in Kelvin

Here is where it gets interesting.  Depending on the action you may either want to (1) transform each individual data point for an observation or (2) generate a single result from the set of observations.  In the first case you are *mutate* your data to create a new result. Perhaps one example is a unit conversion - temperature was measured in Celsius and you need to convert it to Kelvin for example.  The conversion is the same for each observation - you are just adding 273.15.  It is pretty easy to write a function, or (3) new observation Mathematically what you are doing are considering *vector-valued functions* - so if you wanted to apply the mean for example, you take a set of variables to produce a single output - similar to this diagram.




The important part of thinking is to move from an individual observation to thinking in vectorized format - a data variable has several observations, but when we talk about it, we are considering the entire set of observations as a vector.  That is the fundamental unit of measurement here.  Once we get through thinkin about a vector of a variable, then we can rely on efficiency of scale.  This is not an easy jump to make at times - depending on the mathematical background you have you may be used to functions having a single input and single output - but what we are talking about is vector valued functions - where we have multiple inputs working on multiple outputs  [[ some type of explanatory diagram will be nice ]].  The alternative if we didn’t have this efficiency of scale is to write loops, which we will see tend to slow us down.


## Code switching
There are several variables that you may work with, and mutating or summarizing are not the only onews that you are working on.  Perhaps you want to rename your variables, or also reorder the columns.  Both tidyverse (R) and pandas (python) can do these, they just have different names.  Table @tbl-action-comparison summarizes things:

https://dplyr.tidyverse.org/articles/base.html → for easy of use just refer here and then delete the baseR?
https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html

::: {#tbl-action-comparison}

| Action | Base R | tidyverse (R) | pandas (python) |
|----|----|------------|------|
| Create new variables| `x$var_x <- FORMULA` | `mutate` | `transform` |
| Extract variables | `x[,'name']` | `select` | `x[['col1']]` |
| Arrange variables | `order` | `arrange` |   |
| Change variable name |  | `rename` |  |
| Change variable order in data table |  | `relocate` |  |
| Roll up data | `by` with `do.call` | `summarize` | `aggregate` or `agg` |
| Grouping | `aggregate` | `group_by` | `.groupby` |



Comparison of different data actions.

::: 

## Joins
This whole time we have been focused on a single data table but in many cases all the information that you need for a project isn't contained in a single data table - and this is the power of data science is the combination of different data tables together.

Joins is also a helpful data manaagement tool because you are not trying to cram evertying all into one - if you have several sammller data tables the cofgnitivie load is much lighter for you to work on.  Also memory management is much easier too - when reading in the file the computer cna handle several smaller files versus a single larger one (and in that case it also is helpful because then you can just identify where a file goes bad - if you are loading up one large data file that fails, then you need to do additional trouble shooting.  Loading in a set of files becomes a lot more easier - but perhaps we shoul save this for the section on iteration (this would be a good action for it).

But for the moment let's say we have two data tables combined together to work on.  That are connected in some way, shape or form.  Perhaps you want to look at temperature observations and in additiona to that percipitation - the rule number one is that when you are comigin data tables it may be just the copy and paste the columns togehter (this is called the risky business join) - but what if the observations are matched up or missing? It may be eas to spot with sa smaller data table in s apready sheet, but you aren't manually going to check over 15000 observations and then say it iall is goo - correction.  What needs to happen here is to rely on progrmas to join the to match oservations.

The funcamental way to join variables together is to the presence of a keuy - which is are variable sfrom two different datables that are meauring the same thing.  In moany cases they are lableed the same name - which makes it easy to identyin.  However if they are not, then you need to do some digging.  Another special case is if theyaren't the same formate - Dats can be notorisulay trkcy in that regard, but that is the topic for another time (or point to something where that can be.

The most common type of join is the intersection or what we call the inner join.  In this case it takes two data tables (say A and B) and then with the variable in common, looks for where they have a common obsevation, and connects the to together.  With venn diagrams, this would be represented like this).

The benefits: this can be easy and quick to do - the size of your data set may get smaller in terms of the number of observations (base R: `merge`; tidyverse R: `inner_join`, python pandas: `merge`).  The number of columns in your data frame will increase at least by one from the second data frame.

The join functions are optimized to work well and look for any commonalities.  Special cases include when you have repeated data, which we leave for the exercises.  There are other joins beyond that, which correspond to other various set theory operations (left join, semi joins, etc).  But mastering joins can help you move forward with data science operations.

## Exercises
