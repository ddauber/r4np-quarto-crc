[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Non-Programmers: A Guide for Social Scientists",
    "section": "",
    "text": "Welcome\nWelcome to R for Non-Programmers: A Guide for Social Scientists (R4NP). This book provides a helpful resource for anyone looking to use R for their research projects, regardless of their level of programming or statistical analysis experience. Each chapter presents essential concepts in data analysis in a concise, thorough, and user-friendly manner. R4NP will guide you through your first steps on a possibly endless journey full of ‘awe and wonder’.\nThis book is designed to be accessible to beginners while also providing valuable insights for experienced analysts looking to transition to R from other computational software. You don’t need any prior knowledge or programming skills to get started. R4NP provides a comprehensive entry into R programming, regardless of your background.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00_about_the_author.html",
    "href": "00_about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "I am an Associate Professor at the University of Warwick. My research interfaces with multiple disciplines, such as International Management, Organisational Behaviour, Cross-cultural Psychology and Intercultural Communication. Most of my research investigates real-life issues in organisational contexts using a diagnostic angle with an emphasis to help improve organisational members’ experiences and organisational performance.\nFor more than 13 years, I enjoy teaching students how to turn data into meaningful recommendations for businesses, universities and the broader society by conveying relevant theory-driven knowledge as well as research methods skills. In addition, I am enthusiastic about new approaches to analysing data, whether it is quantitative or qualitative.\nMy most recent publications can be found on ResearchGate, and I regularly post tutorials about R, productivity and other hidden gems of academic life on my blog: The Social Science Sofa. If you like to connect with me, you can find me on Twitter.",
    "crumbs": [
      "About the author"
    ]
  },
  {
    "objectID": "01_before_you_get_started.html",
    "href": "01_before_you_get_started.html",
    "title": "1  Readme. before you get started",
    "section": "",
    "text": "1.1 A starting point and reference book\nMy journey with R began rather suddenly one night. After many months of contemplating learning R and too many excuses not to get started, I put all logic aside and decided to use R on my most significant project to date. Admittedly, at the time, I had some knowledge of other programming languages and felt maybe overconfident learning a new tool. This is certainly not how I would recommend learning R. In hindsight, though, it enabled me to do things that pushed the project much further than I could have imagined. However, I invested a lot of additional time on top of my project responsibilities to learn this programming language. In many ways, R opened the door to research I would not have thought of a year earlier. Today, I completely changed my style of conducting research, collaborating with others, composing my blog posts and writing my research papers.\nIt is true what everyone told me back then: R programming has a steep learning curve and is challenging. To this date, I would agree with this sentiment if I ignored all the available resources. The one thing I wish I had to help me get started was a book dedicated to analysing data from a Social Scientist perspective and which guides me through the analytical steps I partially knew already. Instead, I spent hours searching on different blogs and YouTube to find solutions to problems in my code. However, at no time I was tempted to revert to my trusted statistics software of choice, i.e. SPSS. I certainly am an enthusiast of learning programming languages, and I do not expect that this is true for everyone. Nevertheless, the field of Social Sciences is advancing rapidly and learning at least one programming language can take you much further than you think.\nThus, the aim of this book is narrowly defined: A springboard into the world of R without having to become a full-fledged R programmer or possess abundant knowledge in other programming languages. This book will guide you through the most common challenges in empirical research in the Social Sciences. Each chapter is dedicated to a common task we have to achieve to answer our research questions. At the end of each chapter, exercises are provided to hone your skills and allow you to revisit key aspects. In addition, the appendix offers several in-depth case studies that showcase how a research project would be carried out from start to finish in R using real datasets.\nThis book likely caters to your needs irrespective of whether you are a seasoned analyst and want to learn a new tool or have barely any knowledge about data analysis. However, it would be wrong to assume that the book covers everything you possibly could know about R or the analytical techniques covered. There are dedicated resources available to you to dig deeper. Several of these resources are cited in this book or are covered in Chapter @ref(next-steps). The primary focal point of the book is on learning R in the context of Social Sciences research projects. As such, it serves as a starting point on what hopefully becomes an enriching, enjoyable, adventurous, and lasting journey.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`Readme.` before you get started</span>"
    ]
  },
  {
    "objectID": "01_before_you_get_started.html#sec-download-the-companion-r-package",
    "href": "01_before_you_get_started.html#sec-download-the-companion-r-package",
    "title": "1  Readme. before you get started",
    "section": "1.2 Download the companion R package",
    "text": "1.2 Download the companion R package\nThe learning approach of this book is twofold: Convey knowledge and offer opportunities to practice. Therefore, more than 50% of the book are dedicated to examples, case studies, exercises and code you can directly use yourself. To facilitate this interactive part, this book is accompanied by a so-called ‘R package’ (see Chapter @ref(r-packages)), which contains all datasets used in this book and enables you to copy and paste any code snippet1 and work along with the book.\nOnce you worked through Chapter @ref(setting-up-r-and-rstudio) you can easily download the package r4np using the following code snippet in your console (see Chapter @ref(the-console-window)):\n\ndevtools::install_github(\"ddauber/r4np\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`Readme.` before you get started</span>"
    ]
  },
  {
    "objectID": "01_before_you_get_started.html#sec-a-tidyverse-approach-with-some-basic-r",
    "href": "01_before_you_get_started.html#sec-a-tidyverse-approach-with-some-basic-r",
    "title": "1  Readme. before you get started",
    "section": "1.3 A ‘tidyverse’ approach with some basic R",
    "text": "1.3 A ‘tidyverse’ approach with some basic R\nAs you likely know, every language has its flavours in the form of dialects. This is no different to programming languages. The chosen ‘dialect’ of R in this book is the tidyverse approach. Not only is it a modern way of programming in R, but it is also a more accessible entry point. The code written with tidyverse reads almost like a regular sentence and, therefore, is much easier to read, understand, and remember. Unfortunately, if you want a comprehensive introduction to learning the underlying basic R terminology, you will have to consult other books. While it is worthwhile to learn different ways of conducting research in R, basic R syntax is much harder to learn, and I opted against covering it as an entry point for novice users. After working through this book, you will find exploring some of the R functions from other ‘dialects’ relatively easy, but you likely miss the ease of use from the tidyverse approach.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`Readme.` before you get started</span>"
    ]
  },
  {
    "objectID": "01_before_you_get_started.html#sec-formatting-of-this-book",
    "href": "01_before_you_get_started.html#sec-formatting-of-this-book",
    "title": "1  Readme. before you get started",
    "section": "1.4 Understanding the formatting of this book",
    "text": "1.4 Understanding the formatting of this book\nThe formatting of this book carries special meaning. For example, you will find actual R code in boxes like these.\n\nname &lt;- \"Daniel\"\nfood &lt;- \"Apfelstrudel\"\n\npaste(\"My name is \", name, \", and I love \", food, \".\", sep = \"\")\n\n[1] \"My name is Daniel, and I love Apfelstrudel.\"\n\n\nYou can easily copy this code chunk by using the button in the top-right corner. Of course, you are welcome to write the code from scratch, which I would recommend because it accelerates your learning.\nBesides these blocks of code, you sometimes find that certain words are formatted in a particular way. For example, datasets, like imdb_top_250, included in the R package r4np, are highlighted. Every time you find a highlighted word, it refers to one of the following:\n\nA dataset,\nA variable,\nA data type,\nThe output of code,\nThe name of an R package,\nThe name of a function or one of its components.\n\nThis formatting style is consistent with other books and resources on R and, therefore, easy to recognise when consulting other content, such as those covered in Chapter @ref(next-steps).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`Readme.` before you get started</span>"
    ]
  },
  {
    "objectID": "01_before_you_get_started.html#footnotes",
    "href": "01_before_you_get_started.html#footnotes",
    "title": "1  Readme. before you get started",
    "section": "",
    "text": "A code snippet is a piece of programming code that can be used directly as is.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`Readme.` before you get started</span>"
    ]
  },
  {
    "objectID": "02_why_start_learning_a_programming_language.html",
    "href": "02_why_start_learning_a_programming_language.html",
    "title": "2  Why learn a programming language as a non-programmer?",
    "section": "",
    "text": "2.1 Learning new tools to analyse your data is always essential\nTheories change over time, and new insights into certain social phenomena are published every day. Thus, your knowledge might get outdated quite quickly. This is not so much the case for research methods knowledge. Typically, analytical techniques remain over many years. We still use the mean, mode, quartiles, standard deviation, etc., to describe our quantitative data. However, there are always new computational methods that help us to crunch the numbers even more. R is a tool that allows you to venture into new analytical territory because it is open source. Thousands of developers provide cutting-edge research methods free of charge for you to try with your data. You can find them on platforms like GitHub. R is like a giant supermarket, where all products are available for free. However, to read the labels on the product packaging and understand what they are, you have to learn the language used in this supermarket.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why learn a programming language as a non-programmer?</span>"
    ]
  },
  {
    "objectID": "02_why_start_learning_a_programming_language.html#sec-programming-languages-enhance-your-conceptual-thinking",
    "href": "02_why_start_learning_a_programming_language.html#sec-programming-languages-enhance-your-conceptual-thinking",
    "title": "2  Why learn a programming language as a non-programmer?",
    "section": "2.2 Programming languages enhance your conceptual thinking",
    "text": "2.2 Programming languages enhance your conceptual thinking\nWhile I have no empirical evidence for this, I am very certain it is true. I would argue that my conceptual thinking is quite good, but I would not necessarily say that I was born with it. Programming languages are very logical. Any error in your code will make you fail to execute it properly. Sometimes you face challenges in creating the correct code to solve a problem. Through creative abstract thinking (I should copyright this term), you start to approach your problems differently, whether it is a coding problem or a problem in any other context. For example, I know many students enjoy the process of qualitative coding. However, they often struggle to detach their insights from the actual data and synthesise ideas on an abstract and more generic level. Qualitative researchers might refer to this as challenges in ’second-order deconstruction of meaning’. This process of abstraction is a skill that needs to be honed, nurtured and practised. From my experience, programming languages are one way to achieve this, but they might not be recognised for this just yet. Programming languages, especially functions (see Chapter @ref(functions)), require us to generalise from a particular case to a generic one. This mental mechanism is also helpful in other areas of research or work in general.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why learn a programming language as a non-programmer?</span>"
    ]
  },
  {
    "objectID": "02_why_start_learning_a_programming_language.html#sec-programming-languages-allow-you-to-look-at-your-data-from-a-different-angle",
    "href": "02_why_start_learning_a_programming_language.html#sec-programming-languages-allow-you-to-look-at-your-data-from-a-different-angle",
    "title": "2  Why learn a programming language as a non-programmer?",
    "section": "2.3 Programming languages allow you to look at your data from a different angle",
    "text": "2.3 Programming languages allow you to look at your data from a different angle\nThere are commonly known and well-established techniques regarding how you should analyse your data rigorously. However, it can be quite some fun to try techniques outside your discipline. This does not only apply to programming languages, of course. Sometimes, learning about a new research method enables you to look at your current tools in very different ways too. One of the biggest challenges for any researcher is to reflect on one’s own work. Learning new and maybe even ‘strange’ tools can help with this. Admittedly, sometimes you might find out that some new tools are also a dead-end. Still, you likely have learned something valuable through the process of engaging with your data differently. So shake off the rust of your analytical routine and blow some fresh air into your research methods.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why learn a programming language as a non-programmer?</span>"
    ]
  },
  {
    "objectID": "02_why_start_learning_a_programming_language.html#sec-learning-any-programming-language-will-help-you-learn-other-programming-languages.",
    "href": "02_why_start_learning_a_programming_language.html#sec-learning-any-programming-language-will-help-you-learn-other-programming-languages.",
    "title": "2  Why learn a programming language as a non-programmer?",
    "section": "2.4 Learning any programming language will help you learn other programming languages.",
    "text": "2.4 Learning any programming language will help you learn other programming languages.\nOnce you understand the logic of one language, you will find it relatively easy to understand new programming languages. Of course, if you wanted to, you could become the next ’Neo’ (from ‘The Matrix’) and change the reality of your research forever. On a more serious note, though, if you know any programming language already, learning R will be easier because you have accrued some basic understanding of these particular types of languages.\nHaving considered everything of the above, do you feel ready for your next foreign language?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why learn a programming language as a non-programmer?</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html",
    "href": "03_setting_up_r_rstudio.html",
    "title": "3  Setting up R and RStudio",
    "section": "",
    "text": "3.1 Installing R\nThe core module of our programming is R itself, and since it is an open-source project, it is available for free on Windows, Mac and Linux computers. So, here is what you need to do to install it properly on your computer of choice:\nThis was relatively easy. You now have R installed. Technically you can start using R for your research, but there is one more tool I strongly advise installing: RStudio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#sec-installing-r",
    "href": "03_setting_up_r_rstudio.html#sec-installing-r",
    "title": "3  Setting up R and RStudio",
    "section": "",
    "text": "Go to www.r-project.org\n\nClick on CRAN where it says Download.\nChoose a server in your country (all of them work, but downloads will perform quicker if you choose your country or one that is close to where you are).\n\nSelect the operating system for your computer, for example Download R for macOS.\n\nSelect the version you want to install (I recommend the latest version)\n\nOpen the downloaded file and follow the installation instructions. I recommend leaving the suggested settings as they are.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#sec-installing-rstudio",
    "href": "03_setting_up_r_rstudio.html#sec-installing-rstudio",
    "title": "3  Setting up R and RStudio",
    "section": "3.2 Installing RStudio",
    "text": "3.2 Installing RStudio\nR by itself is just the ‘beating heart’ of R programming, but it has no particular user interface. If you want buttons to click and actually ‘see’ what you are doing, there is no better way than RStudio. RStudio is an integrated development environment (IDE) and will be our primary tool to interact with R. It is the only software you need to do all the fun parts and, of course, to follow along with the examples of this book. To install RStudio perform the following steps:\n\nGo to http://posit.co and click on DOWNLOAD RSTUDIO.\n\nClick on DOWNLOAD RSTUDIO on this page.\n\nThis leads you to the page where you can install R as a first step and RStudio as a second step. Since we installed R already, we can click on DOWNLOAD RSTUDIO DESKTOP FOR WINDOWS (or for Mac if you are not using a PC).\n\nIf for some reason the version for your operating system is not showing up correctly, you can scroll down and find other version ready to be installed.\n\nOpen the downloaded file and follow the installation instructions. Again, keep it to the default settings as much as possible.\n\nCongratulations, you are all set up to learn R. From now on you only need to start RStudio and not R. Of course, if you are the curious type, nothing shall stop you to try R without RStudio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#sec-when-you-first-start-rstudio",
    "href": "03_setting_up_r_rstudio.html#sec-when-you-first-start-rstudio",
    "title": "3  Setting up R and RStudio",
    "section": "3.3 When you first start RStudio",
    "text": "3.3 When you first start RStudio\nBefore you start programming away, you might want to make some tweaks to your settings right away to have a better experience (in my humble opinion). To open the Rstudio settings you have to click on\n\nRStudio &gt; Tools &gt; Global Options or press ⌘ + , if you are on a Mac.\nRStudio &gt; Tools &gt; Global Options or press Ctrl + , if you work on a Windows computer.\n\nI recommend to at least make the following changes to set yourself up for success right from the beginning:\n\nAlready on the first tab, i.e. General &gt; Basic, we should make one of the most significant changes. Deactivate every option that starts with Restore. This will ensure that every time you start RStudio, you begin with a clean slate. At first sight, it might sound counter-intuitive not to restart everything where you left off, but it is essential to make all your projects easily reproducible. Furthermore, if you work together with others, not restoring your personal settings also ensures that your programming works across different computers. Therefore, I recommend having the following unticked:\n\nRestore most recently opened project at startup,\nRestore previsouly open source documents at startup,\nRestore .Rdata into workspace at startup\n\n\nIn the same tab under Workspace, select Never for the setting Save workspace to .RData on exit. One might think it is wise to keep intermediary results stored from one R session to another. However, I often found myself fixing issues due to this lazy method, and my code became less reliable and, therefore, reproducible. With experience, you will find that this avoids many headaches.\nIn the Code &gt; Editing tab, make sure to have at least the first five options ticked, especially the Auto-indent code after paste. This setting will save time when trying to format your coding appropriately, making it easier to read. Indentation is the primary way of making your code look more readable and less like a series of characters that appear almost random.\n\nIn the Display tab, you might want to have the first three options selected. In particular, Highlight selected line is helpful because, in more complicated code, it is helpful to see where your cursor is.\n\n\nOf course, if you wish to customise your workspace further, you can do so. The visually most impactful way to alter the default appearance of RStudio is to select Appearance and pick a completely different colour theme. Feel free to browse through various options and see what you prefer. There is no right or wrong here. Just make it your own.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#sec-updating-r-and-rstudio",
    "href": "03_setting_up_r_rstudio.html#sec-updating-r-and-rstudio",
    "title": "3  Setting up R and RStudio",
    "section": "3.4 Updating R and RStudio: Living at the pulse of innovation",
    "text": "3.4 Updating R and RStudio: Living at the pulse of innovation\nWhile not strictly something that helps you become a better programmer, this advice might come in handy to avoid turning into a frustrated programmer. When you update your software, you need to update R and RStudio separately from each other. While both R and RStudio work closely with each other, they still constitute separate pieces of software. Thus, it is essential to keep in mind that updating RStudio will not automatically update R. This can become problematic if specific tools you installed via RStudio (like a fancy learning algorithm) might not be compatible with earlier versions of R. Also, additional R packages (see Chapter @ref(r-packages)) developed by other developers are separate pieces which require updating too, independently from R and RStudio.\nI know what you are thinking: This already sounds complicated and cumbersome. However, rest assured, we take a look at how you can easily update all your packages with RStudio. Thus, all you need to remember is: R needs to be updated separately from everything else.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#sec-rstudio-cloud",
    "href": "03_setting_up_r_rstudio.html#sec-rstudio-cloud",
    "title": "3  Setting up R and RStudio",
    "section": "3.5 RStudio Cloud",
    "text": "3.5 RStudio Cloud\nRStudio Cloud is an application that runs in your web browser. It allows you to write and access R code wherever you go. It even works on your tablet because it does not require installation. However, do expect it to run slower if your internet connection is not fast or stable. To work with RStudio Cloud, you also need an internet connection. However, there are many benefits to using RStudio in the cloud, such as running time-consuming scripts without using your own device. Also, it is much easier to collaborate with others, and since no installation is required, you can work on projects on any device as long as you are connected to the internet. However, RStudio Cloud’s most significant advantage is that you can get started with programming within seconds compared to a desktop installation. Still, I prefer my locally-run offline version of RStudio, because I appreciate working offline as much as online. Nevertheless, I recommend setting up an account because you never know when you need it.\nTo get started with RStudio Cloud, we have to undertake a couple of steps:\n\nOpen your web browser of choice and navigate to https://rstudio.cloud.\n\nClick on Sign Up to create your account.\nOn the next page, make sure you have the free plan selected and click on Sign up.\n\nTo finalise the registration process, you are required to provide your credentials.\n\nOnce you complete your registration, you are redirected to Your Workspace, the central hub for all your projects. As you can tell, I already added another workspace called R for Non-Programmers. However, it is fine to use the default one.\n\nTo start a new R project, you can click on New Project &gt; New RStudio Project. This will open RStudio in the Cloud, which looks identical to the desktop version. You can immediately start writing your code. The example below computes a plot.1\n\nNow we can execute the code as we would on the desktop version (see Chapter @ref(r-basics-the-very-fundamentals)).\n\n\nNo matter whether you choose to use a desktop version of RStudio or RStudio Cloud, you will be able to follow along in this book with no problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "03_setting_up_r_rstudio.html#footnotes",
    "href": "03_setting_up_r_rstudio.html#footnotes",
    "title": "3  Setting up R and RStudio",
    "section": "",
    "text": "RStudio and RStudio Cloud warn you if you need to install certain R packages to successfully run code. More about R packages and what they are can be found in Chapter @ref(r-packages).↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting up *R* and RStudio</span>"
    ]
  },
  {
    "objectID": "04_rstudio_interface.html",
    "href": "04_rstudio_interface.html",
    "title": "4  The RStudio Interface",
    "section": "",
    "text": "4.1 The Console window\nThe console is located in the bottom-left, and it is where you often will find the output of your coding and computations. It is also possible to write code directly into the console. Let’s try the following example by calculating the sum of 10 + 5. Click into the console with your mouse, type the calculation into your console and hit Enter/Return ↵ on your keyboard. The result should be pretty obvious:\n# We type the below into the console\n10 + 5\n\n[1] 15\nHere is a screenshot of how it should look like at your end in RStudio:\nYou just successfully performed your first successful computation. I know, this is not quite impressive just yet. R is undoubtedly more than just a giant calculator.\nIn the top right of the console, you find a symbol that looks like a broom. This one is quite an important one because it clears your console. Sometimes the console can become very cluttered and difficult to read. If you want to remove whatever you computed, you can click the broom icon and clear the console of all text. I use it so frequently that I strongly recommend learning the keyboard shortcut, which is Ctrl + L on PC and Mac.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The RStudio Interface</span>"
    ]
  },
  {
    "objectID": "04_rstudio_interface.html#sec-the-source-window",
    "href": "04_rstudio_interface.html#sec-the-source-window",
    "title": "4  The RStudio Interface",
    "section": "4.2 The Source window",
    "text": "4.2 The Source window\nIn the top left, you can find the source window. The term ‘source’ can be understood as any type of file, e.g. data, programming code, notes, etc. The source panel can fulfil many functions, such as:\n\nInspect data in an Excel-like format (see also Chapter @ref(import-your-data))\nOpen programming code, e.g. an R Script (see Chapter @ref(creating-an-r-script))\nOpen other text-based file formats, e.g.\n\nPlain text (.txt),\nMarkdown (.md),\nWebsites (.html),\nLaTeX (.tex),\nBibTex (.bib),\n\nEdit scripts with code in it,\nRun the analysis you have written.\n\n\nIn other words, the source window will show you whatever file you are interested in, as long as RStudio can read it - and no, Microsoft Office Documents are not supported. Another limitation of the source window is that it can only show text-based files. So opening images, etc. would not work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The RStudio Interface</span>"
    ]
  },
  {
    "objectID": "04_rstudio_interface.html#sec-the-environment-history-connections-tutorial-window",
    "href": "04_rstudio_interface.html#sec-the-environment-history-connections-tutorial-window",
    "title": "4  The RStudio Interface",
    "section": "4.3 The Environment / History / Connections / Tutorial window",
    "text": "4.3 The Environment / History / Connections / Tutorial window\nThe window in the top right shows multiples panes. The first pane is called Environment and shows you objects which are available for computation. One of the first objects you will create is your dataset because, without data, we cannot perform any analysis. Another object could be a plot showing the number of male and female participants in your study. To find out how to create objects yourself, you can take a glimpse at it (see Chapter @ref(inspecting-raw-data)). Besides datasets and plots, you will also find other objects here, e.g. lists, vectors and functions you created yourself. Don’t worry if none of these words makes sense at this point. We will cover each of them in the upcoming chapters. For now, remember this is a place where you can find different objects you created.\n\nThe History pane is very easy to understand. Whatever computation you run in the console will be stored. So you can go back and see what you coded and rerun that code. Remember the example from above where we computed the sum of 10+5? This computation is stored in the history of RStudio, and you can rerun it by clicking on 10+5 in the history pane and then click on To Console. This will insert 10+5 back into the console, and we can hit Return ↵ to retrieve the result. You also have the option to copy the code into an existing or new R Script by clicking on To Source. By doing this, you can save this computation on your computer and reuse it later. Finally, if you would like to store your history, you can do so by clicking on the floppy disk symbol. There are two more buttons in this pane, one allows you to delete individual entries in the history (the white page with the red circle on it), and the last one, a broom, clears the entire history (irrevocably).\n\nThe pane Connections allows you to tab into external databases directly. This can come in handy when you work collaboratively on the same data or want to work with extensive datasets without having to download them. However, for an introduction to R, we will not use this feature of RStudio.\n\nThe last pane is called Tutorial. Here you can find additional materials to learn R and RStudio. If you search for more great content to learn R, this serves as a great starting point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The RStudio Interface</span>"
    ]
  },
  {
    "objectID": "04_rstudio_interface.html#sec-the-files-plots-packages-help-viewer-window",
    "href": "04_rstudio_interface.html#sec-the-files-plots-packages-help-viewer-window",
    "title": "4  The RStudio Interface",
    "section": "4.4 The Files / Plots / Packages / Help / Viewer window",
    "text": "4.4 The Files / Plots / Packages / Help / Viewer window\nThe last window consists of five essential panes. The first one is the Files pane. As the name indicates, it lists all the files and folders in your root directory. A root directory is the default directory where RStudio saves your files, for example, your analysis. However, we will look at how you can properly set up your projects in RStudio in Chapter @ref(starting-your-r-projects). Thus, the Files pane is an easy way to load data into RStudio and create folders to keep your research project well organised.\n\nSince the Console cannot reproduce data visualisations, RStudio offers a way to do this very easily. It is through the Plots pane. This pane is exclusively designed to show you any plots you have created using R. Here is a simple example that you can try. Type into your console boxplot(mtcars$hp).\n\n# Here we create a nice boxplot using a dataset called 'mtcars'\nboxplot(mtcars$hp)\n\n\n\n\n\n\n\nFigure 4.1\n\n\n\n\n\nAlthough this is a short piece of coding, it performs quite a lot of steps:\n\nit uses a function called boxplot() to draw a boxplot of\na variable called hp (for horsepower), which is located in\na dataset named mtcars,\nand it renders the graph in your Plots pane\n\nThis is how the plot should look like in your RStudio Plots pane.\n\nIf you wish to delete the plot, you can click on the red circle with a white x symbol. This will delete the currently visible plot. If you wish to remove all plots from this pane, you can use the broom. There is also an option to export your plot and move back and forth between different plots.\nThe next pane is called Packages. Packages are additional tools you can import and use when performing your analysis. A frequent analogy people use to explain packages is your phone and the apps you install. Each package you download is equivalent to an app on your phone. It can enhance different aspects of working in R, such as creating animated plots, using unique machine learning algorithms, or simply making your life easier by doing multiple computations with just one single line of code. You will learn more about R packages in Chapter @ref(r-packages).\n\nIf you are in dire need of help, RStudio provides you with a Help pane. You can search for specific topics, for example how certain computations work. The Help pane also has documentation on different datasets that are included in R, RStudio or R packages you have installed. If you want a more comprehensive overview of how you can find help, have a look at CRAN’s ‘Getting Help with R’ webpage.\n\nSo, for example, if you want to know what the mtcars dataset is, you can either use the search window in the Help pane or, much easier, use a ? in the console to search for it:\n\n# Type a '?' followed by the name of a dataset/function/etc.\n# to look up helpful information about it.\n?mtcars\n\nThis will open the Help pane and give you more information about this dataset:\n\nThere are many different ways of how you can find help with your coding beyond RStudio and this book. My top three platforms to find solutions to my programming problems are:\n\nGoogle\nstackoverflow.com\nTwitter (with #RStats)\n\nLastly, we have the Viewer pane. Not every data visualisation we create in R is a static image. You can create dynamic data visualisations or even websites with R. This type of content is displayed in the Viewer pane rather than in the Plots pane. Often these visualisations are based on HTML and other web-based programming languages. As such, it is easy to open them in your browser as well. However, in this book, we mainly focus on two-dimensional static plots, which are the ones you likely need most of the time, either for your assignments, thesis, or publication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The RStudio Interface</span>"
    ]
  },
  {
    "objectID": "04_rstudio_interface.html#sec-customise-your-user-interface",
    "href": "04_rstudio_interface.html#sec-customise-your-user-interface",
    "title": "4  The RStudio Interface",
    "section": "4.5 Customise your user interface",
    "text": "4.5 Customise your user interface\nAs a last remark in this chapter, I would like to make you aware that you can modify each window. There are three basic adjustments you can make:\n\nHide panes by clicking on the window symbol in the top right corner of each window,\nResize panes by dragging the border of a window horizontally or vertically, or\nAdd and remove panes by going to RStudio &gt; Preferences &gt; Pane Layout, or use the keyboard shortcut ⌘ + , if you are on a Mac. Unfortunately, there is no default shortcut for PC users.\n\nIf you want a fully customised experience you can also alter the colour scheme of RStudio itself (RStudio &gt; Preferences &gt; Appearance) and if the themes offered are not enough for you, you can create a custom theme here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The RStudio Interface</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html",
    "href": "05_r_basics.html",
    "title": "5  R Basics: The very fundamentals",
    "section": "",
    "text": "5.1 Basic computations in R\nThe most basic computation you can do in R is arithmetic operations. In other words, addition, subtraction, multiplication, division, exponentiation and extraction of roots. In short, R can be used like your pocket calculator, or more likely the one you have on your phone. For example, in Chapter @ref(the-console-window) we already performed an addition. Thus, it might not come as a surprise how their equivalents work in R. Let’s take a look at the following examples:\n# Addition\n10 + 5\n\n[1] 15\n# Subtraction\n10 - 5\n\n[1] 5\n# Multiplication\n10 * 5\n\n[1] 50\n# Division\n10 / 5\n\n[1] 2\n# Exponentiation\n10 ^ 2\n\n[1] 100\n# Square root\nsqrt(10)\n\n[1] 3.162278\nThey all look fairly straightforward except for the extraction of roots. As you probably know, extracting the root would typically mean we use the symbol \\(\\sqrt{}\\) on our calculator. To compute the square root in R, we have to use a function instead to perform the computation. So we first put the name of the function sqrt and then the value 10 within parenthesis (). This results in the following code: sqrt(10). If we were to write this down in our report, we would write \\(\\sqrt[2]{10}\\).\nFunctions are an essential part of R and programming in general. You will learn more about them in Chapter @ref(functions). Besides arithmetic operations, there are also logical queries you can perform. Logical queries always return either the value TRUE or FALSE. Here are some examples which make this clearer:\n#1 Is it TRUE or FALSE?\n1 == 1\n\n[1] TRUE\n#2 Is 45 bigger than 55?\n45 &gt; 55\n\n[1] FALSE\n#3 Is 1982 bigger or equal to 1982?\n1982 &gt;= 1982\n\n[1] TRUE\n#4 Are these two words NOT the same?\n\"Friends\" != \"friends\"\n\n[1] TRUE\n#5 Are these sentences the same?\n\"I love statistics\" == \"I love statistícs\"\n\n[1] FALSE\nReflecting on these examples, you might notice three important aspects of logical queries:\nOne of the most common mistakes of R novices is the confusion around the == and = notation. While == represents equal to, = is used to assign a value to an object (for more details on assignments see Chapter @ref(assigning-values-to-objects)). However, in practice, most R programmers tend to avoid = since it can easily lead to confusion with ==. As such, you can strike = out of your R vocabulary for now.\nThere are many different logical operations you can perform. Table @ref(tab:logical-operators-r) lists the most frequently used logical operators for your reference. These will become important, for example when we filter our data for analysis, e.g. include only female or male participants.\nLogical operators in R\n\n\nOperator\nDescription\n\n\n\n\n==\nis equal to\n\n\n!=\nis not equal to\n\n\n&gt;=\nis bigger or equal to\n\n\n&gt;\nis bigger than\n\n\n&lt;=\nis smaller or equal to\n\n\n&lt;\nis smaller than\n\n\na | b\na or b\n\n\na & b\na and b\n\n\n!a\nis not a",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-basic-computations-in-r",
    "href": "05_r_basics.html#sec-basic-computations-in-r",
    "title": "5  R Basics: The very fundamentals",
    "section": "",
    "text": "We have to use == instead of =,\nWe can compare non-numerical values, i.e. text, which is also known as character values, with each other,\nThe devil is in the details (consider #5).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-assigning-values-to-objects",
    "href": "05_r_basics.html#sec-assigning-values-to-objects",
    "title": "5  R Basics: The very fundamentals",
    "section": "5.2 Assigning values to objects: ‘<-’",
    "text": "5.2 Assigning values to objects: ‘&lt;-’\nAnother common task you will perform is assigning values to an object. An object can be many different things:\n\na dataset,\nthe results of a computation,\na plot,\na series of numbers,\na list of names,\na function,\netc.\n\nIn short, an object is an umbrella term for many different things which form part of your data analysis. For example, objects are handy when storing results that you want to process further in later analytical steps. We have to use the assign operator &lt;- to assign a value to an object. Let’s have a look at an example.\n\n# I have a friend called \"Fiona\"\nfriends &lt;- \"Fiona\"\n\nIn this example, I created an object called friends and added \"Fiona\" to it. Since \"Fiona\" represents a string, we need to use \"\". So, if you wanted to read this line of code, you would say, ‘friends gets the value \"Fiona\"’. Alternatively, you could also say ‘\"Fiona\" is assigned to friends’.\nIf you look into your environment pane, you will find the object friends. You can see it carries the value \"Fiona\". We can also print values of an object in the console by simply typing the name of the object friends and hit Return ↵.\n\n# Who are my friends?\nfriends\n\n[1] \"Fiona\"\n\n\nSadly, it seems I only have one friend. Luckily we can add some more, not the least to make me feel less lonely. To create objects with multiple values, we can use the function c(), which stands for ‘concatenate’. The Cambridge Dictionary (2021) defines this word as follows:\n\n‘concatenate’,\nto put things together as a connected series.\n\nLet’s concatenate some more friends into our friends object.\n\n# Adding some more friends to my life\nfriends &lt;- c(\"Fiona\",\n             \"Lukas\",\n             \"Ida\",\n             \"Georg\",\n             \"Daniel\",\n             \"Pavel\",\n             \"Tigger\")\n\n# Here are all my friends\nfriends\n\n[1] \"Fiona\"  \"Lukas\"  \"Ida\"    \"Georg\"  \"Daniel\" \"Pavel\"  \"Tigger\"\n\n\nTo concatenate values into a single object, we need to use a comma , to separate each value. Otherwise, R will report an error back.\n\nfriends &lt;- c(\"Fiona\" \"Ida\")\n\nError: &lt;text&gt;:1:22: unexpected string constant\n1: friends &lt;- c(\"Fiona\" \"Ida\"\n                         ^\n\n\nR’s error messages tend to be very useful and give meaningful clues to what went wrong. In this case, we can see that something ‘unexpected’ happened, and it shows where our mistake is. You can also concatenate numbers\nBtw, if you add () around your code, you can automatically print the content of the object to the console. Thus,\n\n(milestones_of_my_life &lt;- c(1982, 2006, 2011, 2018, 2020)) is the same as\nmilestones_of_my_life &lt;- c(1982, 2006, 2011, 2018, 2020) followed by milestones_of_my_life.\n\nYou can copy and paste the following example to illustrate what I explained.\n\n# Important years in my life\nmilestones_of_my_life &lt;- c(1982, 2006, 2011, 2018, 2020)\nmilestones_of_my_life\n\n[1] 1982 2006 2011 2018 2020\n\n# The same as above - no second line of code needed\n(milestones_of_my_life &lt;- c(1982, 2006, 2011, 2018, 2020))\n\n[1] 1982 2006 2011 2018 2020\n\n\nFinally, we can also concatenate numbers and character values into one object:\n\n(names_and_years &lt;- c(\"Fiona\", 1988, \"Daniel\", 1982))\n\n[1] \"Fiona\"  \"1988\"   \"Daniel\" \"1982\"  \n\n\nThis last example is not necessarily something I would recommend to do, because it likely leads to undesirable outcomes. For example, if you look into your environment pane you currently have three objects: friends, milestones_of_my_life, and names_and_years.\n\nThe friends object shows that all the values inside the object are classified as chr, which denominates character. For this object, this is correct because it only includes the names of my friends. On the other hand, the object milestones_of_my_life only includes numeric values, and therefore it says num in the environment pane. However, for the object names_and_years we know we want to have numeric and character values included. Still, R recognises them as character values because values inside objects are meant to be of the same type. Remember that in R, numbers can always be interpreted as character and numeric, but text only can be considered as character.\nConsequently, mixing different types of data into one object is likely a bad idea. This is especially true if you want to use the numeric values for computation. We will return to data types in Chapter @ref(change-data-types) where we learn how to change them, but for now we should ensure that our objects are all of the same data type.\nHowever, there is an exception to this rule. ‘Of course’, you might say. There is one object that can have values of different types: a list. As the name indicates, a list object holds several items. These items are usually other objects. In the spirit of ‘Inception’, you can have lists inside lists, which contain more lists or other objects.\nLet’s create a list called x_files using the list function and place all our objects inside.\n\n# This creates our list of objects\nx_files &lt;- list(friends,\n                milestones_of_my_life,\n                names_and_years)\n\n# Let's have a look what is hidden inside the x_files\nx_files\n\n[[1]]\n[1] \"Fiona\"  \"Lukas\"  \"Ida\"    \"Georg\"  \"Daniel\" \"Pavel\"  \"Tigger\"\n\n[[2]]\n[1] 1982 2006 2011 2018 2020\n\n[[3]]\n[1] \"Fiona\"  \"1988\"   \"Daniel\" \"1982\"  \n\n\nYou will notice in this example that I do not use \"\" for each value in the list. This is because friends is not a character value, but an object. When we refer to objects, we do not need quotation marks.\nWe will encounter list objects quite frequently when we perform our analysis. Some functions return the results in the format of lists. This can be very helpful because otherwise our environment pane will be littered with objects and we would not necessarily know how they relate to each other, or worse, to which analysis they belong. Looking at the list item in the environment page (Figure 5.1), you can see that the object x_files is classified as a List of 3, and if you click on the blue icon, you can inspect the different objects inside.\n\n\n\n\n\n\nFigure 5.1: The environment pane showing our objects and our list x_files\n\n\n\nIn Chapter (basic-computations-in-r?) , I mentioned that we should avoid using the = operator and explained that it is used to assign values to objects. You can, if you want, use = instead of &lt;-. They fulfil the same purpose. However, as mentioned before, it is not wise to do so. Here is an example that shows that, in principle, it is possible.\n\n# DO\n(avengers1 &lt;- c(\"Iron Man\",\n                \"Captain America\",\n                \"Black Widow\",\n                \"Vision\"))\n\n[1] \"Iron Man\"        \"Captain America\" \"Black Widow\"     \"Vision\"         \n\n# DON'T\n(avengers2 = c(\"Iron Man\",\n               \"Captain America\",\n               \"Black Widow\",\n               \"Vision\"))\n\n[1] \"Iron Man\"        \"Captain America\" \"Black Widow\"     \"Vision\"         \n\n\nOn a final note, naming your objects is limited. You cannot chose any name. First, every name needs to start with a letter. Second, you can only use letters, numbers _ and . as valid components of the names for your objects (see also Chapter 4.2 in Wickham and Grolemund 2016). I recommend to establish a naming convention that you adhere to. Personally I prefer to only user lower-case letters and _ to separate/connect words. Ideally, you want to keep names informative, succinct and precise. Here are some examples of what some might consider good and bad choices for names.\n\n# Good choices\nincome_per_annum\nopen_to_exp          # for 'openness to new experiences'\nsoc_int              # for 'social integration'\n \n# Bad choices\nIncomePerAnnum\nmeasurement_of_boredom_of_watching_youtube\nSleep.per_monthsIn.hours\n\nUltimately, you need to be able to effectively work with your data and output. Ideally, this should be true for others as well who want or need to work with your analysis and data as well, e.g. your co-investigator or supervisor. The same is applies to column names in datasets (see Chapter @ref(colnames-cleaning)). Some more information about coding style and coding etiquette can be found in Chapter @ref(coding-etiquette).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-functions",
    "href": "05_r_basics.html#sec-functions",
    "title": "5  R Basics: The very fundamentals",
    "section": "5.3 Functions",
    "text": "5.3 Functions\nI used the term ‘function’ multiple times, but I never thoroughly explained what they are and why we need them. In simple terms, functions are objects. They contain lines of code that someone has written for us or we have written ourselves. One could say they are code snippets ready to use. Someone else might see them as shortcuts for our programming. Functions increase the speed with which we perform our analysis and write our computations and make our code more readable and reliable. Consider computing the mean of values stored in the object pocket_money.\n\n# First we create an object that stores our desired values\npocket_money &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89)\n\n#1 Manually compute the mean\nsum &lt;- 0 + 1 + 1 + 2 + 3 + 5 + 8 + 13 + 21 + 34 + 55 + 89\nsum / 12 # There are 12 items in the object\n\n[1] 19.33333\n\n\n\n#2 Use a function to compute the mean\nmean(pocket_money)\n\n[1] 19.33333\n\n\n\n#3 Let's make sure #1 and #2 are actually the same\nsum / 12 == mean(pocket_money)\n\n[1] FALSE\n\n\nIf we manually compute the mean, we first calculate the sum of all values in the object pocket_money1. Then we divide it by the number of values in the object, which is 12. This is the traditional way of computing the mean as we know it from primary school. However, by simply using the function mean(), we not only write considerably less code, but it is also much easier to understand because the word ‘mean’ does precisely what we would expect. Which approach do you find easier: Using the function mean() or compute it by hand?\nTo further illustrate how functions look like, let’s create one ourselves and call it my_mean.\n\nmy_mean &lt;- function(numbers){\n  # Compute the sum of all values in 'numbers'\n  sum &lt;- sum(numbers)\n  \n  # Divide the sum by the number of items in 'numbers'\n  result &lt;- sum/length(numbers)\n  \n  # Return the result in the console\n  return(result)\n}\n\nmy_mean(pocket_money)\n\n[1] 19.33333\n\n\nDon’t worry if half of this code does not make sense to you. Writing functions is an advanced R skill. However, it is good to know how functions look on the ‘inside’. You certainly can see the similarities between the code we have written before, but instead of using actual numbers, we work with placeholders like numbers. This way, we can use a function for different data and do not have to rewrite it every time. Writing and using functions relates to the skill of abstraction mentioned in Chapter @ref(programming-languages-enhance-your-conceptual-thinking).\nAll functions in R share the same structure. They have a name followed by (). Within these parentheses, we put arguments, which have specific values. For example, a generic structure of a function would look something like this:\n\nname_of_function(argument_1 = value_1,\n                 argument_2 = value_2,\n                 argument_3 = value_3)\n\nHow many arguments there are and what kind of values you can provide is very much dependent on the function you use. Thus, not every function takes every value. In the case of mean(), the function takes an object which holds a sequence of numeric values. It would make very little sense to compute the mean of our friends object, because it only contains names. R would return an error message:\n\nmean(friends)\n\nWarning in mean.default(friends): argument is not numeric or logical: returning\nNA\n\n\n[1] NA\n\n\nNA refers to a value that is ‘not available’. In this case, R tries to compute the mean, but the result is not available, because the values are not numeric but a character. In your dataset, you might find values that are NA, which means there is data missing. If a function attempts a computation that includes even just a single value that is NA, R will return NA. However, there is a way to fix this. You will learn more about how to deal with NA values in Chapter @ref(dealing-with-missing-data).\nSometimes you will also get a message from R that states NaN. NaN stands for ‘not a number’ and is returned when something is not possible to compute, for example:\n\n# Example 1\n0 / 0\n\n[1] NaN\n\n\n\n# Example 2\nsqrt(-9)\n\nWarning in sqrt(-9): NaNs produced\n\n\n[1] NaN",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-r-packages",
    "href": "05_r_basics.html#sec-r-packages",
    "title": "5  R Basics: The very fundamentals",
    "section": "5.4 R packages",
    "text": "5.4 R packages\nR has many built-in functions that we can use right away. However, some of the most interesting ones are developed by different programmers, data scientists and enthusiasts. To add more functions to your repertoire, you can install R packages. R packages are a collection of functions that you can download and use for your own analysis. Throughout this book, you will learn about and use many different R packages to accomplish various tasks. To give you another analogy,\n\nR is like a global supermarket where everyone can offer their products,\nRStudio is like my shopping cart where I can put the products I like, and\nR packages are the products I can pick from the shelves.\n\nLuckily, R packages are free to use, so I do not have to bring my credit card. For me, these additional functions, developed by some of the most outstanding scientists, is one of many reasons that keeps me addicted to performing my research in R.\nR packages do not only include functions but often include datasets and documentation of what each function does. This way, you can easily try every function right away, even without your own dataset and read through what each function in the package does. Figure 5.2 shows the documentation of an R package called ggplot2.\n\n\n\n\n\n\nFigure 5.2: The R package documentation for ‘ggplot2’\n\n\n\nHowever, how do you find those R packages? They are right at your fingertips. You have two options:\n\nUse the function install.packages(), or\nUse the packages pane in RStudio.\n\n\n5.4.1 Installing packages using install.packages()\nThe simplest and fastest way to install a package is calling the function install.packages(). You can either use it to install a single package or install a series of packages all at once using our trusty c() function. All you need to know is the name of the package. This approach works for all packages that are on CRAN (remember CRAN from Chapter @ref(installing-r)?). \n\n# Install a single package\ninstall.packages(\"tidyverse\")\n\n# Install multiple packages at once\ninstall.packages(c(\"tidyverse\", \"naniar\", \"psych\"))\n\nIf a package is not available from CRAN, chances are you can find them on GitHub. GitHub is probably the world’s largest global platform for programmers from all walks of life, and many of them develop fantastic R packages that make programming and data analysis not just easier but a lot more fun. As you continue to work in R, you should seriously consider creating your own account to keep backups of your projects (see also Chapter @ref(next-steps-github)).\nAn essential companion for this book is r4np, which contains all datasets for this book and some useful functions to get you up and running in no time. Since it is currently only available on Github, you can use the following code snippet to install it. However, you have to install the package devtools first to use the function install_github().\n\n# Install the 'devtools' package first\ninstall.packages(\"devtools\")\n\n# Then install the 'r4np' package from GitHub\ndevtools::install_github(\"ddauber/r4np\")\n\n\n\n5.4.2 Installing packages via RStudio’s package pane\nRStudio offers a very convenient way of installing packages. In the packages pane, you cannot only see your installed packages, but you have two more buttons: Install and Update. The names are very self-explanatory. To install an R package you can follow the following steps:\n\nClick on Install.\nIn most cases, you want to make sure you have Repository (CRAN) selected.\n\nType in the name of the package you wish to install. RStudio offers an auto-complete feature to make it even easier to find the package you want.\n\nI recommend NOT to change the option which says Install to library. The default library settings will suffice.\nFinally, I recommend to select Install dependencies, because some packages need other packages to function properly. This way, you do not have to do this manually.\n\n\nThe only real downside of using the packages pane is that you cannot install packages hosted on GitHub only. However, you can download them from there and install them directly from your computer using this option. This is particularly useful if you do not have an internet connection but you already downloaded the required packages onto a hard drive. However, in 99.9% of cases it is much easier to use the function devtools::install_github().\n\n\n5.4.3 Install all necessary R packages for this book\nLastly, if you would like to install the necessary packages to follow the examples in this book, the r4np package comes with a handy function to install them all at once. Of course, you need to have r4np installed first, as shown above.\n\n# Install all R packages used in this book\nr4np::install_r4np()\n\n\n\n5.4.4 Using R Packages\nNow that you have a nice collection of R packages, the next step would be to use them. While you only have to install R packages once, you have to ‘activate’ them every time you start a new session in RStudio. This process is also called ‘loading an R package’. Once an R package is loaded, you can use all its functions. To load an R package, we have to use the function library().\n\nlibrary(tidyverse)\n\nThe tidyverse package is a special kind of package. It contains multiple packages and loads them all at once. Almost all included packages you will use at some point when working through this book.\nI know what you are thinking. Can you use c() to load all your packages at once? Unfortunately not. However, there is a way to do this, but it goes beyond the scope of this book to fully explain this. If you are curious, you can take a peek at the code here on stackoverflow.com.\nBesides, it is not always advisable to load all functions of an entire package. One reason could be that two packages contain a function with the same name but with a different purpose. Two functions with the same name create a conflict between these two packages, and one of the functions would not be usable. Another reason could be that you only need to use the function once, and loading the whole package to use only one specific function seems excessive. Instead, you can explicitly call functions from packages without loading the package. For example, we might want to use the vis_miss() function from the naniar package to show where data is missing in our dataset airquality. Using functions without library() is also much quicker than loading the package and then calling the function if you don’t use it repeatedly. Make sure you have naniar installed (see above). We will work with this package when we explore missing data in Chapter @ref(dealing-with-missing-data). To use a function from an R package without loading it, we have to use :: between the package’s name and the function we want to use. Copy the code below and try it yourself.\n\n# Here I use the dataset 'airquality', which comes with R\nnaniar::vis_miss(airquality)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-coding-etiquette",
    "href": "05_r_basics.html#sec-coding-etiquette",
    "title": "5  R Basics: The very fundamentals",
    "section": "5.5 Coding etiquette",
    "text": "5.5 Coding etiquette\nNow you know everything to get started, but before we jump into our first project, I would like to briefly touch upon coding etiquette. This is not something that improves your analytical or coding skills directly, but is essential in building good habbits and making your life and those of others a little easier. Consider writing code like growing plants in your garden. You want to nurture the good plants, remove the weed and add labels that tell you which plant it is that you are growing. At the end of the day, you want your garden to be well-maintained. Treat you programming code the same way.\nA script (see Chapter @ref(creating-an-r-script)) of programming code should always have at least the following qualities:\n\nOnly contains code that is necessary,\nIs easy to read and understand,\nIs self-contained.\n\nWith simple code this is easily achieved. However, what about more complex and longer code representing a whole set of analytical steps?\n\n# Very messy code\n\nlibrary(tidyverse)\nlibrary(jtools)\nmodel1 &lt;- lm(covid_cases_per_1m ~ idv, data = df)\nsumm(model1, scale = TRUE, transform.response = TRUE, vifs = TRUE)\ndf |&gt; ggplot(aes(x = covid_cases_per_1m, y = idv, col = europe, label = country))+\ntheme_minimal()+ geom_label(nudge_y = 2) + geom_point()\nmodel2 &lt;- lm(cases_per_1m ~ idv + uai + idv*europe + uai*europe, data = df)\nsumm(model2, scale = TRUE, transform.response = TRUE, vifs = TRUE)\nanova(model1, model2)\n\nHow about the following in comparison?\n\n# Nicely structured code\n\n# Load required R packages\nlibrary(tidyverse)\nlibrary(jtools)\n\n# ---- Modelling COVID-19 cases ----\n\n## Specify and run a regression\nmodel1 &lt;- lm(covid_cases_per_1m ~ idv, data = df)\n\n## Retrieve the summary statistics of model1\nsumm(model1,\n     scale = TRUE,\n     transform.response = TRUE,\n     vifs = TRUE)\n\n# Does is matter whether a country lies in Europe?\n\n## Visualise rel. of covid cases, idv and being a European country\ndf |&gt;\n  ggplot(aes(x = covid_cases_per_1m,\n             y = idv,\n             col = europe,\n             label = country)) +\n  theme_minimal() +\n  geom_label(nudge_y = 2) +\n  geom_point()\n\n## Specify and run a revised regression\nmodel2 &lt;- lm(cases_per_1m ~ idv + uai + idv*europe + uai*europe,\n                 data = df)\n\n## Retrieve the summary statistics of model2\nsumm(model2,\n     scale = TRUE,\n     transform.response = TRUE,\n     vifs = TRUE)\n\n## Test whether model2 is an improvement over model1\nanova(model1, model2)\n\nI hope we can agree that the second example is much easier to read and understand even though you probably do not understand most of it yet. For once, I separated the different analytical steps from each other like paragraphs in a report. Apart from that, I added comments with # to provide more context to my code for someone else who wants to understand my analysis. Admittedly, this example is a little excessive. Usually, you might have fewer comments. Commenting is an integral part of programming because it allows you to remember what you did. Ideally, you want to strike a good balance between commenting on and writing your code. How many comments you need will likely change throughout your R programming journey. Think of comments as headers for your programming script that give it structure.\nWe can use # not only to write comments but also to tell R not to run particular code. This is very helpful if you want to keep some code but do not want to use it yet. There is also a handy keyboard shortcut you can use to ‘deactivate’ multiple lines of code at once. Select whatever you want to ‘comment out’ in your script and press Ctrl+Shift+C (PC) or Cmd+Shift+C (Mac).\n\n# mean(pocket_money) # R will NOT run this code\nmean(pocket_money)   # R will run this code\n\nRStudio helps a lot with keeping your coding tidy and properly formatted. However, there are some additional aspects worth considering. If you want to find out more about coding style, I highly recommend to read through ‘The tidyverse style guide’ (Wickham 2021).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#sec-exercises-r_basics",
    "href": "05_r_basics.html#sec-exercises-r_basics",
    "title": "5  R Basics: The very fundamentals",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nIt is time to practice the newly acquired skills. The r4np package comes with interactive tutorials. In order to start them, you have two options. If you have installed the r4np package already and used the function install_r4np() you can use Option 1:\n\n# Option 1:\nlearnr::run_tutorial(\"ex_r_basics\", package = \"r4np\")\n\nIf you have not installed the r4np package and/or not run the function install_r4np(), you will have to do this first using Option 2:\n\n# Option 2:\n## Install 'r4np' package\ndevtools::install_github(\"ddauber/r4np\")\n\n## Install all relevant packages for this book\nr4np::install_r4np()\n\n## Start the tutorial for this chapter\nlearnr::run_tutorial(\"ex_r_basics\", package = \"r4np\")\n\n\n\n\n\nCambridge Dictionary. 2021. “Concatenate.” https://dictionary.cambridge.org/dictionary/english/concatenate.\n\n\nWickham, Hadley. 2021. The Tidyverse Style Guide. https://style.tidyverse.org.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "05_r_basics.html#footnotes",
    "href": "05_r_basics.html#footnotes",
    "title": "5  R Basics: The very fundamentals",
    "section": "",
    "text": "If you find the order of numbers suspicious, it is because it represents the famous Fibonacci sequence.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*R* Basics: The very fundamentals</span>"
    ]
  },
  {
    "objectID": "06_starting_r_projects.html",
    "href": "06_starting_r_projects.html",
    "title": "6  Starting your R projects",
    "section": "",
    "text": "6.1 Creating an R Project file\nWhen working on a project, you likely create many different files for various purposes, especially R Scripts (see Chapter @ref(creating-an-r-script)). If you are not careful, this file is stored in your system’s default location, which might not be where you want them to be. RStudio allows you to manage your entire project intuitively and conveniently through R Project files. Using R Project files comes with a couple of perks, for example:\nFor now, the most important reason to use R Project files is the convenience of the organisation of files and the ability to share it easily with co-investigators, your supervisor, or your students.\nTo create an R Project, you need to perform the following steps:\nIf you look carefully, you can see that your RStudio is now ‘branded’ with your project name. At the top of the window, you see the project name, the files pane shows the root directory where all your files will be, and even the console shows on top the file path of your project. You could set all this up manually, but I would not recommend it, not the least because it is easy and swift to work with R Projects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Starting your *R* projects</span>"
    ]
  },
  {
    "objectID": "06_starting_r_projects.html#sec-creating-an-r-project",
    "href": "06_starting_r_projects.html#sec-creating-an-r-project",
    "title": "6  Starting your R projects",
    "section": "",
    "text": "All the files that you generate are in the same place. Your data, your coding, your exported plots, your reports, etc., all are in one place together without you having to manage the files manually. This is due to the fact that RStudio sets the root directory to whichever folder your project is saved to.\nIf you want to share your project, you can share the entire folder, and others can quickly reproduce your research or help fix problems. This is because all file paths are relative and not absolute.\nYou can, more easily, use GitHub for backups and so-called ‘version control’, which allows you to track changes you have made to your code over time (see also Chapter @ref(next-steps-github)).\n\n\n\n\nSelect File &gt; New Project… from the menu bar.\n\nSelect New Directory from the popup window.\n\nNext, select New Project.\n\nPick a meaningful name for your project folder, i.e. the Directory Name. Ensure this project folder is created in the right place. You can change the subdirectory by clicking on Browse…. Ideally the subdirectory is a place where you usually store your research projects.\n\nYou have the option to Create a git repository. This is only relevant if you already have a GitHub account and wish to use version control. For now, you can happily ignore it if you do not use GitHub.\nLastly, tick Open in new session. This will open your R Project in a new RStudio window.\n\nOnce you are happy with your choices, you can click Create Project. This will open a new R Session, and you can start working on your project.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Starting your *R* projects</span>"
    ]
  },
  {
    "objectID": "06_starting_r_projects.html#sec-organising-your-projects",
    "href": "06_starting_r_projects.html#sec-organising-your-projects",
    "title": "6  Starting your R projects",
    "section": "6.2 Organising your projects",
    "text": "6.2 Organising your projects\nThis section is not directly related to RStudio, R or data analysis in general. Instead, I want to convey to you that a good folder structure can go a long way. It is an excellent habit to start thinking about folder structures before you start working on your project. Placing your files into dedicated folders, rather than keeping them loosely in one container, will speed up your work and save you from the frustration of not finding the files you need. I have a template that I use regularly. You can either create it from scratch in RStudio or open your file browser and create the folders there. RStudio does not mind which way you do it. If you want to spend less time setting this up, you might want to use the function create_project_folder() from the r4np package. It creates all the folders as shown in Figure @ref(fig:folder-structure).\n\n\n# Install 'r4np' from GitHub\ndevtools::install_github(\"ddauber/r4np\")\n\n# Create the template structure\nr4np::create_project_folder()\n\nTo create a folder, click on New Folder in the Files pane. I usually have at least the following folders for every project I am involved in:\n\nA folder for my raw data. I store ‘untouched’ datasets in it. With ‘untouched’, I mean they have not been processed in any way and are usually files I downloaded from my data collection tool, e.g. an online questionnaire platform.\nA folder with ‘tidy’ data. This is usually data I exported from R after cleaning it, i.e. after some basic data wrangling and cleaning (see Chapter @ref(data-wrangling)).\nA folder for my R scripts\nA folder for my plots\nA folder for reports.\n\nThus, in RStudio, it would look something like this:\n\n\n\n\n\n\nFigure 6.1: An example of a scalable folder structure for your project\n\n\n\nYou probably noticed that my folders have numbers in front of them. I do this to ensure that all folders are in the order I want them to be, which is usually not the alphabetical order my computer suggests. I use two digits because I may have more than nine folders for a project, and folder ten would otherwise be listed as the third folder in this list. With this filing strategy in place, it will be easy to find whatever I need. Even others can easily understand what I stored where. It is simply ‘tidy’, similar to how we want our data to be.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Starting your *R* projects</span>"
    ]
  },
  {
    "objectID": "06_starting_r_projects.html#sec-creating-an-r-script",
    "href": "06_starting_r_projects.html#sec-creating-an-r-script",
    "title": "6  Starting your R projects",
    "section": "6.3 Creating an R Script",
    "text": "6.3 Creating an R Script\nCode quickly becomes long and complex. Thus, it is not very convenient to write it in the console. So, instead, we can write code into an R Script. An R Script is a document that RStudio recognises as R programming code. Files that are not R Scripts, like .txt, .rtf or .md, can also be opened in RStudio, but any code written in it will not be automatically recognised.\nWhen opening an R script or creating a new one, it will display in the Source window (see Chapter @ref(the-source-window)). Some refer to this window as the ‘script editor’. An R script starts as an empty file. Good coding etiquette (see Chapter @ref(coding-etiquette)) demands that we use the first line to indicate what this file does by using a comment #. Here is an example for a ‘TidyTuesday’ R Project.\n\nAll examples in this book can easily be copied and pasted into your own R script. However, for some code you will have to install the R package r4np (see above). Let’s try it with the following code. The plot this code creates reveals which car manufacturer produces the most efficient cars. Copy and paste this code into your R script.\n\nlibrary(tidyverse)\n\nmpg |&gt; ggplot(aes(x = reorder(manufacturer, desc(hwy), FUN = median),\n                   y = hwy,\n                   fill = manufacturer)) +\n  geom_boxplot() +\n  coord_flip() +\n  theme_minimal() +\n  xlab(\"Manufacturer\") +\n  ylab(\"Highway miles per gallon\")\n\n\n\n\n\n\n\n\nYou are probably wondering where your plot has gone. Copying the code will not automatically run it in your R script. However, this is necessary to create the plot. If you tried pressing Return ↵, you would only add a new line. Instead, you need to select the code you want to run and press Ctrl+Return ↵ (PC) or Cmd+Return ↵ (Mac). You can also use the Run command at the top of your source window, but it is much more efficient to press the keyboard shortcut. Besides, you will remember this shortcut quickly, because we need to use it very frequently. If all worked out, you should see the following:\n\nAs you can see, cars from Honda appear to drive furthest with the same amount of fuel (a gallon) compared to other vehicles. Thus, if you are looking for very economical cars, you now know where to find them.\nThe R script editor has some conveniences for writing your code that are worth pointing out. You probably noticed that some of the code we have pasted is blue, and some other code is in green. These colours help to make your code more readable because they carry a specific meaning. In the default settings, green stands for any values in \"\", which usually stands for characters. Automatic colouring of our progamming code is also called ‘syntax highlighting’.\nMoreover, code in R scripts will be automatically indented to facilitate reading. If, for whatever reason, the indentation does not happen, or you accidentally undo it, you can reindent a line with Ctrl+I (PC) or Cmd+I (Mac).\nLastly, the console and the R script editor both feature code completion. This means that when you start typing the name of a function, R will provide suggestions. These are extremely helpful and make programming a lot faster. Once you found the function you were looking for, you press Return ↵ to insert it. Here is an example of what happens when you have the package tidyverse loaded and type ggpl. Only functions that are loaded via packages or any object in your environment pane benefit from code completion.\n\nNot only does RStudio show you all the available options, but it also tells you which package this function is from. In this case, all listed functions are from the ggplot2 package. Furthermore, when you select one of the options but have not pressed Return ↵ yet, you also get to see a yellow box, which provides you with a quick reference of all the arguments that this function accepts. So you do not have to memorise all the functions and their arguments.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Starting your *R* projects</span>"
    ]
  },
  {
    "objectID": "06_starting_r_projects.html#sec-r-markdown-and-r-notebooks",
    "href": "06_starting_r_projects.html#sec-r-markdown-and-r-notebooks",
    "title": "6  Starting your R projects",
    "section": "6.4 Using R Markdown",
    "text": "6.4 Using R Markdown\nThere is too much to say about R Markdown, which is why I only will highlight that it exists and point out the one feature that might convince you to choose this format over plain R scripts: They look like a Word document (almost).\nAs the name indicates, R Markdown files are a combination of R scripts and ’Markdown’. ‘Markdown’ is a way of writing and formatting text documents without needing software like MS Word. Instead, you write everything in plain text. Such plain text can be converted into many different document types such as HTML websites, PDF or Word documents. If you would like to see how it works, I recommend looking at the R Markdown Cheatsheet. To create an R Markdown file click on File &gt; New File &gt; R Markdown....\nAn R Markdown file works oppositely to an R script. By default, an R script considers everything as code and only through commenting # we can include text to describe what the code does. This is what you have seen in all the coding examples so far. On the other hand, an R Markdown file considers everything as text, and we have to specify what is code. We can do so by inserting ‘code chunks’. Therefore, there is less of a need to use comments # in R Markdown files because you can write about it. Another convenience of R Markdown files is that results from your analysis are immediately shown underneath the code chunk and not in the console.\n\nIf you switch your view to the Visual Editor, it almost looks like you are writing a report in MS Word.\n\n\nSo, when should you use an R script, and when should you use R Markdown. The rule-of-thumb is that if you intend to write a report, thesis or another form of publication, it might be better to work in an R Markdown file. If this does not apply, you might want to use an R Script. As mentioned above, R Markdown files emphasise text, while R scripts primarily focus on code. In my projects, I often have a mixture of both. I use R Scripts to carry out data wrangling and my primary analysis and then use R Markdown files to present the findings, e.g. creating plots, tables, and other components of a report. By the way, this book is written in R Markdown using the bookdown package.\nNo matter your choice, it will neither benefit nor disadvantage you in your R journey or when working through this book. The choice is all yours. You likely will come to appreciate both formats for what they offer. If you want to find out more about R Markdown and how to use it, I highly recommend taking a look at ‘R Markdown: The Definitive Guide’ (Xie, Allaire, and Grolemund 2018).\n\n\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Starting your *R* projects</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html",
    "href": "07_data_wrangling.html",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "7.1 Import your data\nThe r4np package hosts several different datasets to work with, but at some point, you might want to apply your R knowledge to your own data. Therefore, an essential first step is to import your data into RStudio. There are three different methods, all of which are very handy:\nWe will use the readr package or all three options. Using this package we can import a range of different file formats, including .csv, .tsv, .txt. If you want to import data from an .xlsx file, you need to use another package called readxl. Similarly, if you used SPSS, Stata or SAS files before and want to use them in R you can use the haven package. The following sections will primarily focus on using readr via RStudio or directly in your Console or R script.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#import-your-data",
    "href": "07_data_wrangling.html#import-your-data",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "Click on your data file in the Files pane and choose Import Dataset.\nUse the Import Dataset button in the Environment pane.\nImport your data calling one of the readr functions in the Console or R script.\n\n\n\n7.1.1 Import data from the Files pane\nThis approach is by far the easiest. Let’s assume you have a dataset called gender_age.csv in your 00_raw_data folder. If you wish to import it, you can do the following:\n\nClick on the name of the file.\nSelect Import Dataset.\n\nA new window will open, and you can choose different options. You also see a little preview of how the data looks like. This is great if you are not sure whether you did it correctly.\n\nYou can change how data should be imported, but the default should be fine in most cases. Here is a quick breakdown of the most important options:\n\nName allows you to change the object name, i.e. the name of the object this data will be assigned to. I often use df_raw (df stand for data frame, which is how R calls such rectangular datasets).\nSkip is helpful if your data file starts with several empty rows at the top. You can remove them here.\nFirst Row as Names is ticked by default. In most Social Science projects, we tend to have the name of the variables as the first row in our dataset.\nTrim Spaces removes any unnecessary whitespace in your dataset. Leave it ticked.\nOpen Data Viewer allows you to look at your imported dataset. I use it rarely, but it can be helpful at times.\nDelimiter defines how your columns are separated from each other in your file. If it is a .csv file, it would imply it is a ‘comma-separated value’, i.e. ,. This setting can be changed for different files, depending on how your data is delimited. You can even use the option Other… to specify a custom separation option.\nNA specifies how missing values in your data are acknowledged. By default, empty cells in your data will be recognised as missing data.\n\n\nOnce you are happy with your choices, you can click on Import.\nYou will find your dataset in the Environment pane.\n\n\nIn the Console, you can see that R also provides the Column specification, which we need later when inspecting ‘data types’. readr automatically imports all text-based columns as chr, i.e. character values. However, this might not always be the correct data type. We will cover more of this aspect of data wrangling in Chapter @ref(change-data-types).\n\n\n7.1.2 Importing data from the Environment pane\nThe process of importing datasets from the Environment pane follows largely the one from the Files pane. Click on Import Dataset &gt; From Text (readr)…. The only main difference lies in having to find the file using the Browse… button. The rest of the steps are the same as above.\nBe aware that you will have to use the Environment pane for importing data from specific file types, e.g. .txt. The File pane would only open the file but not import the data for further processing.\n\n\n7.1.3 Importing data using functions directly\nIf you organised your files well, it could be effortless and quick to use all the functions from readr directly. Here are two examples of how you can use readr to import your data. Make sure you have the package loaded.\n\n# Import data from '.csv'\nread_csv(\"00_raw_data/gender_age.csv\")\n\n# Import data from any text file by defining the separator\nread_delim(\"00_raw_data/gender_age.txt\", delim = \"|\")\n\nYou might be wondering whether you can use read_delim() to import .csv files too. The answer is ‘Yes, you can!’. In contrast to read_delim(), read_csv() sets the delimiter to , by default. This is mainly for convenience because .csv files are one of the most popular file formats used to store data.\nYou might also be wondering what a ‘delimiter’ is. When you record data in a plain-text file, it is easy to see where a new observation starts and ends because it is defined by a row in your file. However, we also need to tell our software where a new column starts, i.e. where a cell begins and ends. Consider the following example. We have a file that holds our data which looks like this:\nidagegender\n124male\n256female\n333male\nThe first row we probably can still decipher as id, age, gender. However, the next row makes it difficult to understand which value represents the id of a participant and which value reflects the age of that participant. Like us, computer software would find it hard too to decide on this ambiguous content. Thus, we need to use delimiters to make it very clear which value belongs to which column. For example, in a .csv file, the data would be separated by a ,.\nid,age,gender\n1,24,male\n2,56,female\n3,33,male\nConsidering our example from above, we could also use | as a delimiter, but this is very unusual.\nid|age|gender\n1|24|male\n2|56|female\n3|33|male\nThere is a lot more to readr than could be covered in this book. If you want to know more about this R package, I highly recommend looking at the readr webpage.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-inspecting-raw-data",
    "href": "07_data_wrangling.html#sec-inspecting-raw-data",
    "title": "7  Data Wrangling",
    "section": "7.2 Inspecting your data",
    "text": "7.2 Inspecting your data\nFor the rest of this chapter, we will use the wvs dataset from the r4np package. However, we do not know much about this dataset, and therefore we cannot ask any research questions worth investigating. Therefore, we need to look at what it contains. The first method of inspecting a dataset is to type the name of the object, i.e. wvs.\n\n# Ensure you loaded the 'r4np' package first\nlibrary(r4np)\n\n# Show the data in the console\nwvs\n\n# A tibble: 69,578 × 7\n   `Participant ID` `Country name` Gender   Age relationship_status       \n              &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                     \n 1         20070001 Andorra             1    60 married                   \n 2         20070002 Andorra             0    47 living together as married\n 3         20070003 Andorra             0    48 separated                 \n 4         20070004 Andorra             1    62 living together as married\n 5         20070005 Andorra             0    49 living together as married\n 6         20070006 Andorra             1    51 married                   \n 7         20070007 Andorra             1    33 married                   \n 8         20070008 Andorra             0    55 widowed                   \n 9         20070009 Andorra             1    40 single                    \n10         20070010 Andorra             1    38 living together as married\n# ℹ 69,568 more rows\n# ℹ 2 more variables: Freedom.of.Choice &lt;dbl&gt;, `Satisfaction-with-life` &lt;dbl&gt;\n\n\nThe result is a series of rows and columns. The first information we receive is: A tibble: 69,578 x 7. This indicates that our dataset has 69,578 observations (i.e. rows) and 9 columns (i.e. variables). This rectangular format is the one we encounter most frequently in Social Sciences (and probably beyond). If you ever worked in Microsoft Excel, this format will look familiar. However, rectangular data is not necessarily tidy data. Wickham (2014) (p. 4) defines tidy data as follows:\n\n\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\nEven though it might be nice to look at a tibble in the console, it is not particularly useful. Depending on your monitor size, you might only see a small number of columns, and therefore we do not get to see a complete list of all variables. In short, we hardly ever will find much use in inspecting data this way. Luckily other functions can help us.\nIf you want to see each variable covered in the dataset and their data types, you can use the function glimpse() from the dplyr package which is loaded as part of the tidyverse package.\n\nglimpse(wvs)\n\nRows: 69,578\nColumns: 7\n$ `Participant ID`         &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070…\n$ `Country name`           &lt;chr&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"…\n$ Gender                   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, …\n$ Age                      &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 3…\n$ relationship_status      &lt;chr&gt; \"married\", \"living together as married\", \"sep…\n$ Freedom.of.Choice        &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7…\n$ `Satisfaction-with-life` &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7,…\n\n\nThe output of glimpse shows us the name of each column/variable after the $, for example, `Participant ID`. The $ is used to lookup certain variables in our dataset. For example, if we want to inspect the column relationship_status only, we could write the following:\n\nwvs$relationship_status\n\n    [1] \"married\"                    \"living together as married\"\n    [3] \"separated\"                  \"living together as married\"\n    [5] \"living together as married\" \"married\"                   \n    [7] \"married\"                    \"widowed\"                   \n....\n\n\nWhen using glimpse() we find the recognised data type for each column, i.e. each variable, in &lt;...&gt;, for example &lt;chr&gt;. We will return to data types in Chapter @ref(change-data-types). Lastly, we get examples of the values included in each variable. This output is much more helpful.\nI use glimpse() very frequently for different purposes, for example:\n\nto understand what variables are included in a dataset,\nto check the correctness of data types,\nto inspect variable names for typos or unconventional names,\nto look up variable names.\n\nThere is one more way to inspect your data and receive more information about it by using a specialised R package. The skimr package is excellent in ‘skimming’ your dataset. It provides not only information about variable names and data types but also provides some descriptive statistics. If you installed the r4np package and called the function install_r4np(), you will have skimr installed already.\n\nskimr::skim(wvs)\n\nThe output in the Console should look like this:\n\nAs you can tell, there is a lot more information in this output. Many descriptive statistics that could be useful are already displayed. skim() provides a summary of the dataset and then automatically sorts the variables by data type. Depending on the data type, you also receive different descriptive statistics. As a bonus, the function also provides a histogram for numeric variables. However, there is one main problem: Some of the numeric variables are not numeric: Participant ID and Gender. Thus, we will have to correct the data types in a moment.\nInspecting your data in this way can be helpful to get a better understanding of what your data includes and spot problems with it. In addition, if you receive data from someone else, these methods are an excellent way to familiarise yourself with the dataset relatively quickly. Since I prepared this particular dataset for this book, I also made sure to provide documentation for it. You can access it by using ?wvs in the Console. This will open the documentation in the Help pane. Such documentation is available for every dataset we use in this book.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-colnames-cleaning",
    "href": "07_data_wrangling.html#sec-colnames-cleaning",
    "title": "7  Data Wrangling",
    "section": "7.3 Cleaning your column names: Call the janitor",
    "text": "7.3 Cleaning your column names: Call the janitor\nIf you have eagle eyes, you might have noticed that most of the variable names in wvs are not consistent or easy to read/use.\n# Whitespace and inconsistent capitalisation\nParticipant ID        \nCountry name          \nGender                \nAge                   \n\n# Difficult to read\nFreedom.of.Choice     \nSatisfaction-with-life\nFrom Chapter @ref(coding-etiquette), you will remember that being consistent in writing your code and naming your objects is essential. The same applies, of course, to variable names. R will not break using the existing names, but it will save you a lot of frustration if we take a minute to clean the names and make them more consistent. Since we will use column names in almost every line of code we produce, it is best to standardise them and make them easier to read and write.\nYou are probably thinking: “This is easy. I just open the dataset in Excel and change all the column names.” Indeed, it would be a viable and easy option, but it is not very efficient, especially with larger datasets with many more variables. Instead, we can make use of the janitor package. By definition, janitor is a package that helps to clean up whatever needs cleaning. In our case, we want to tidy our column names. We can use the function clean_names() to achieve this. We store the result in a new object called wvs to keep those changes. The object will also show up in our Environment pane.\n\nwvs_clean &lt;- janitor::clean_names(wvs)\n\nglimpse(wvs_clean)\n\nRows: 69,578\nColumns: 7\n$ participant_id         &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 2007000…\n$ country_name           &lt;chr&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"An…\n$ gender                 &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,…\n$ age                    &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39,…\n$ relationship_status    &lt;chr&gt; \"married\", \"living together as married\", \"separ…\n$ freedom_of_choice      &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, …\n$ satisfaction_with_life &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 1…\n\n\nNow that janitor has done its magic, we suddenly have easy to read variable names that are consistent with the ‘Tidyverse style guide’ (Wickham 2021).\nIf, for whatever reason, the variable names are still not looking the way you want, you can use the function rename() from the dplyr package to manually assign new variable names.\n\nwvs_clean &lt;- \n  wvs_clean |&gt;\n  rename(satisfaction = satisfaction_with_life,\n         country = country_name)\n\nglimpse(wvs_clean)\n\nRows: 69,578\nColumns: 7\n$ participant_id      &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070005, …\n$ country             &lt;chr&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"Andor…\n$ gender              &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,…\n$ age                 &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44…\n$ relationship_status &lt;chr&gt; \"married\", \"living together as married\", \"separate…\n$ freedom_of_choice   &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,…\n$ satisfaction        &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, …\n\n\nYou are probably wondering what |&gt; stands for. This symbol is called a ‘piping operator’ , and it allows us to chain multiple functions together by considering the output of the previous function. So, do not confuse &lt;- with |&gt;. Each operator serves a different purpose. The |&gt; has become synonymous with the tidyverse approach to R programming and is the chosen approach for this book. Many functions from the tidyverse are designed to be chained together.\nIf we wanted to spell out what we just did, we could say:\n\nwvs &lt;-: We assigned whatever happened to the right of the assignment operator to the object wvs.\nwvs |&gt;: We defined the dataset we want to use with the functions defined after the |&gt;.\nrename(satisfaction = satisfcation_with_life): We define a new name satisfaction for the column satisfaction_with_life. Notice that the order is new_name = old_name. Here we also use =. A rare occasion where it makes sense to do so.\n\nJust for clarification, the following two lines of code accomplish the same task. The only difference is that with |&gt; we could chain another function right after it. So, you could say, it is a matter of taste which approach you prefer. However, in later chapters, it will become apparent why using |&gt; is very advantageous.\n\n# Renaming a column using '|&gt;'\nwvs_clean |&gt; rename(satisfaction_new = satisfaction)\n\n# Renaming a column without '|&gt;'\nrename(wvs_clean, satisfaction_new = satisfaction)\n\nSince you will be using the pipe operator very frequently, it is a good idea to remember the keyboard shortcut for it: Ctrl+Shift+M for PC and Cmd+Shift+M for Mac.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-change-data-types",
    "href": "07_data_wrangling.html#sec-change-data-types",
    "title": "7  Data Wrangling",
    "section": "7.4 Data types: What are they and how can you change them",
    "text": "7.4 Data types: What are they and how can you change them\nWhen we inspected our data, I mentioned that some variables do not have the correct data type. You might be familiar with different data types by classifying them as:\n\nNominal data, which is categorical data of no particular order,\nOrdinal data, which is categorical data with a defined order, and\nQuantitative data, which is data that usually is represented by numeric values.\n\nIn R we have a slightly different distinction:\n\ncharacter / &lt;chr&gt;: Textual data, for example the text of a tweet.\nfactor / &lt;fct&gt;: Categorical data with a finite number of categories with no particular order.\nordered / &lt;ord&gt;: Categorical data with a finite number of categories with a particular order.\ndouble / &lt;dbl&gt;: Numerical data with decimal places.\ninteger / &lt;int&gt;: Numerical data with whole numbers only (i.e. no decimals).\nlogical / &lt;lgl&gt;: Logical data, which only consists of values TRUE and FALSE.\ndate / date: Data which consists of dates, e.g. 2021-08-05.\ndate-time / dttm: Data which consists of dates and times, e.g. 2021-08-05 16:29:25 BST.\n\nFor a complete list of data types, I recommend looking at ‘Column Data Types’ (Müller and Wickham 2021).\nR has a fine-grained categorisation of data types. The most important distinction, though, lies between &lt;chr&gt;, &lt;fct&gt;/&lt;ord&gt; and &lt;dbl&gt; for most datasets in the Social Sciences. Still, it is good to know what the abbreviations in your tibble mean and how they might affect your analysis.\nNow that we have a solid understanding of different data types, we can look at our dataset and see whether readr classified our variables correctly.\n\nglimpse(wvs_clean)\n\nRows: 69,578\nColumns: 7\n$ participant_id      &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070005, …\n$ country             &lt;chr&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"Andor…\n$ gender              &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,…\n$ age                 &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44…\n$ relationship_status &lt;chr&gt; \"married\", \"living together as married\", \"separate…\n$ freedom_of_choice   &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,…\n$ satisfaction        &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, …\n\n\nreadr did a great job in identifying all the numeric variables. However, by default, readr imports all variables that include text as &lt;chr&gt;. It appears as if this is not entirely correct for our dataset. The variables country, gender and relationship_status specify a finite number of categories. Therefore they should be classified as a factor. The variable participant_id is represented by numbers, but its meaning is also rather categorical. We would not use the ID numbers of participants to perform additions or multiplications. This would make no sense. Therefore, it might be wise to turn them into a factor, even though we likely will not use it in our analysis and would make no difference. However, I am a stickler for those kinds of things.\nTo perform the conversion, we need to use two new functions from dplyr:\n\nmutate(): Changes, i.e. ‘mutates’, a variable.\nas_factor(): Converts data from one type into a factor.\n\nIf we want to convert all variables in one go, we can put them into the same function, separated by a ,.\n\nwvs_clean &lt;-\n  wvs_clean |&gt;\n  mutate(country = as_factor(country),\n         gender = as_factor(gender),\n         relationship_status = as_factor(relationship_status),\n         participant_id = as_factor(participant_id)\n         )\n\nglimpse(wvs_clean)\n\nRows: 69,578\nColumns: 7\n$ participant_id      &lt;fct&gt; 20070001, 20070002, 20070003, 20070004, 20070005, …\n$ country             &lt;fct&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"Andor…\n$ gender              &lt;fct&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,…\n$ age                 &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44…\n$ relationship_status &lt;fct&gt; married, living together as married, separated, li…\n$ freedom_of_choice   &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,…\n$ satisfaction        &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, …\n\n\nThe output in the console shows that we successfully performed the transformation and our data types are as we intended them to be. Mission accomplished.\nIf you need to convert all &lt;chr&gt; columns you can use mutate_if(is.character, as_factor) instead. This function will look at each column and if it is a character type variable, it will convert it into a factor. However, use this function only if you are certain that all character columns need converting. If this is the case, it can be a huge time-saver. Here is the corresponding code snippet:\n\nwvs_clean |&gt;\n  mutate_if(is.character, as_factor)\n\nLastly, there might be occasions where you want to convert data into other formats than from &lt;chr&gt; to &lt;fct&gt;. Similar to as_factor() there are more functions available to transform your data into the correct format of your choice:\n\nas.integer(): Converts values into integers, i.e. without decimal places. Be aware that this function does not round values as you might expect. Instead, it removes any decimals as if they never existed.\nas.numeric(): Converts values into numbers with decimals if they are present.\nas.character(): Converts values to string values, even if they are numbers. If you use readr to import your data, you will not often use this function because readr imports unknown data types as &lt;chr&gt; by default.\n\nAll these function can be used in the same as we did before. Here are three examples to illustrated their use.\n\n# Conversion to character values\nconverted_data &lt;-\n  wvs_clean |&gt;\n  mutate(participant_id = as.character(participant_id),\n         age = as.character(age))\n\nglimpse(converted_data)\n\nRows: 69,578\nColumns: 7\n$ participant_id      &lt;chr&gt; \"20070001\", \"20070002\", \"20070003\", \"20070004\", \"2…\n$ country             &lt;fct&gt; \"Andorra\", \"Andorra\", \"Andorra\", \"Andorra\", \"Andor…\n$ gender              &lt;fct&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,…\n$ age                 &lt;chr&gt; \"60\", \"47\", \"48\", \"62\", \"49\", \"51\", \"33\", \"55\", \"4…\n$ relationship_status &lt;fct&gt; married, living together as married, separated, li…\n$ freedom_of_choice   &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,…\n$ satisfaction        &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, …\n\n\n\n# Conversion to numeric values\nconverted_data |&gt;\n  mutate(age = as.numeric(age))\n\n# A tibble: 69,578 × 7\n   participant_id country gender   age relationship_status     freedom_of_choice\n   &lt;chr&gt;          &lt;fct&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                               &lt;dbl&gt;\n 1 20070001       Andorra 1         60 married                                10\n 2 20070002       Andorra 0         47 living together as mar…                 9\n 3 20070003       Andorra 0         48 separated                               9\n 4 20070004       Andorra 1         62 living together as mar…                 9\n 5 20070005       Andorra 0         49 living together as mar…                 8\n 6 20070006       Andorra 1         51 married                                10\n 7 20070007       Andorra 1         33 married                                10\n 8 20070008       Andorra 0         55 widowed                                 8\n 9 20070009       Andorra 1         40 single                                  8\n10 20070010       Andorra 1         38 living together as mar…                10\n# ℹ 69,568 more rows\n# ℹ 1 more variable: satisfaction &lt;dbl&gt;\n\n\n\n# Convert numeric values to integers\nnumeric_values &lt;- tibble(values = c(1.3, 2.6, 3.8))\n\nnumeric_values |&gt;\n  mutate(integers = as.integer(values))\n\n# A tibble: 3 × 2\n  values integers\n   &lt;dbl&gt;    &lt;int&gt;\n1    1.3        1\n2    2.6        2\n3    3.8        3",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-handling-factors",
    "href": "07_data_wrangling.html#sec-handling-factors",
    "title": "7  Data Wrangling",
    "section": "7.5 Handling factors",
    "text": "7.5 Handling factors\nFactors are an essential way to classify observations in our data in different ways. In terms of data wrangling, there are usually at least two steps we take to prepare them for analysis:\n\nRecoding factors, and\nReordering factor levels.\n\n\n7.5.1 Recoding factors\nAnother common problem we have to tackle when working with data is their representation in the dataset. For example, gender could be measured as male and female1 or as 0 and 1. R does not mind which way you represent your data, but some other software does. Therefore, when we import data from somewhere else, the values of a variable might not look the way we want. The practicality of having your data represented accurately as what they are, becomes apparent when you intend to create tables and plots.\nFor example, we might be interested in knowing how many participants in wvs_clean were male and female. The function count() from dplyr does precisely that. We can sort the results, i.e. n, in descending order by including sort = TRUE. By default, count() arranges our variable gender in alphabetical order.\n\nwvs_clean |&gt; count(gender, sort = TRUE)\n\n# A tibble: 3 × 2\n  gender     n\n  &lt;fct&gt;  &lt;int&gt;\n1 1      36478\n2 0      33049\n3 &lt;NA&gt;      51\n\n\nNow we know how many people were male and female and how many did not disclose their gender. Or do we? The issue here is that you would have to know what 0 and 1 stand for. Surely you would have a coding manual that gives you the answer, but it seems a bit of a complication. For gender, this might still be easy to remember, but can you recall the ID numbers for 48 countries?\nIt certainly would be easier to replace the 0s and 1s with their corresponding labels. This can be achieved with a simple function called fct_recode() from forcats. However, since we ‘mutate’ a variable into something else, we also have to use the mutate() function.\n\nwvs_clean &lt;-\n  wvs_clean |&gt;\n  mutate(gender = fct_recode(gender,\n                             \"male\" = \"0\",\n                             \"female\" = \"1\"))\n\nIf you have been following along very carefully, you might spot one oddity in this code: \"0\" and \"1\". You likely recall that in Chapter @ref(r-basics-the-very-fundamentals), I mentioned that we use \"\" for character values but not for numbers. So what happens if we run the code and remove \"\".\n\nwvs_clean |&gt;\n  mutate(gender = fct_recode(gender,\n                             \"male\" = 0,\n                             \"female\" = 1))\n\nError in `mutate()`:\nℹ In argument: `gender = fct_recode(gender, male = 0, female = 1)`.\nCaused by error in `fct_recode()`:\n! Each element of `...` must be a named string.\nℹ Problems with 2 arguments: male and female\n\n\nThe error message is easy to understand: fct_recode() only expects strings as input and not numbers. R recognises 0 and 1 as numbers, but fct_recode() only converts a factor value into another factor value and since we ‘mutated’ gender from a numeric variable to a factor variable, all values of gender are no longer numbers. To refer to a factor level (i.e. one of the categories in our factor), we have to use \"\". In other words, data types matter and are often a source of problems with your code. Thus, always pay close attention to them. For now, it is important to remember that factor levels are always string values, i.e. text.\nNow that we have change the factor levels to our liking, we can rerun our analysis and generate a frequency table for gender. This time we receive a much more readable output.\n\nwvs_clean |&gt; count(gender)\n\n# A tibble: 3 × 2\n  gender     n\n  &lt;fct&gt;  &lt;int&gt;\n1 male   33049\n2 female 36478\n3 &lt;NA&gt;      51\n\n\nAnother benefit of going through the trouble of recoding your factors is the readability of your plots. For example, we could quickly generate a bar plot based on the above table and have appropriate labels instead of 0 and 1.\n\nwvs_clean |&gt;\n  count(gender) |&gt;\n  ggplot(aes(gender, n)) +\n  geom_col()\n\n\n\n\nA bar plot of the factor levels male and female\n\n\n\n\nPlots are an excellent way to explore your data and understand relationships between variables. We will learn more about this when we start to perform analytical steps on our data (see Chapter @ref(descriptive-statistics) and beyond).\nAnother use case for recoding factors could be for purely cosmetic reasons. For example, when looking through our dataset, we might notice that some country names are very long and do not look great in data visualisations or tables. Thus, we could consider shortening them which is a slightly more advanced process.\nFirst, we need to find out which country names are particularly long. There are 48 countries in this dataset, so it could take some time to look through them all. Instead, we could use the function filter() from dplyr to pick only countries with a long name. However, this poses another problem: How can we tell the filter function to pick only country names with a certain length? Ideally, we would want a function that does the counting for us. As you probably anticipated, there is a package called stringr, which also belongs to the tidyverse, and has a function that counts the number of characters that represent a value in our dataset: str_length(). This function takes any character variable and returns the length of it. This also works with factors because this function can ‘coerce’ it into a character, i.e. it just ignores that it is a factor and looks at it as if it was a regular character variable. Good news for us, because now we can put the puzzle pieces together.\n\nwvs_clean |&gt;\n  filter(str_length(country) &gt;= 15) |&gt;\n  count(country)\n\n# A tibble: 3 × 2\n  country                             n\n  &lt;fct&gt;                           &lt;int&gt;\n1 Bolivia, Plurinational State of  2067\n2 Iran, Islamic Republic of        1499\n3 Korea, Republic of               1245\n\n\nI use the value 15 arbitrarily after some trial and error. You can change the value and see which other countries would show up with a lower threshold. However, this number seems to do the trick and returns three countries that seem to have longer names. All we have to do is replace these categories with new ones the same way we recoded gender.\n\nwvs_clean &lt;-\n  wvs_clean |&gt;\n  mutate(country = fct_recode(country,\n                              \"Bolivia\" = \"Bolivia, Plurinational State of\",\n                              \"Iran\" = \"Iran, Islamic Republic of\",\n                              \"Korea\" = \"Korea, Republic of\"))\n\nNow we have country names that nicely fit into tables and plots we want to create later and do not have to worry about it later.\n\n\n7.5.2 Reordering factor levels\nBesides changing factor levels, it is also common that we want to reconsider the arrangement of these levels. To inspect the order of factor levels, we can use the function fct_unique() from the forcats package and provide a factor variable, e.g. gender, in the wvs_clean dataset.\n\nfct_unique(wvs_clean$gender)\n\n[1] male   female &lt;NA&gt;  \nLevels: male female\n\n\nA reason for changing the order of factor levels could be to arrange how factor levels are displayed in plots. For example, Figure @ref(fig:gender-bar-plot) shows the bar of male participants first, but instead, we might want to show female participants first. The forcats package offers several options to rearrange factor levels based on different conditions. Depending on the circumstances, one or the other function might be more efficient. Three of these functions I tend to use fairly frequently:\n\nfct_relevel(): To reorder factor levels by hand.\nfct_reorder(): To reorder factor levels based on another variable of interest.\nfct_rev(): To reverse the order of factor levels.\n\nEach function requires slightly different arguments. If we wanted to change the order of our factor levels, we could use all three of them to achieve the same result (in our case):\n\n# Reorder by hand\nwvs_clean |&gt;\n  count(gender) |&gt;\n  ggplot(aes(x = fct_relevel(gender, \"female\", \"male\"),\n             y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n# Reorder by another variable, e.g. the frequency 'n'\nwvs_clean |&gt;\n  count(gender) |&gt;\n  ggplot(aes(x = fct_reorder(gender, n, .desc = TRUE),\n             y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n# Reverse the order of factor levels\nwvs_clean |&gt;\n  count(gender) |&gt;\n  ggplot(aes(x = fct_rev(gender),\n             y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nThere are many ways to rearrange factors, and it very much depends on the data you have and what you wish to achieve. For most of my plots, I tend to revert to a basic R function, i.e. reorder(), which is much shorter to type and does essentially the same as fct_reorder(). There is no right or wrong in this regard, just personal taste.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-handling-dates-times-and-durations",
    "href": "07_data_wrangling.html#sec-handling-dates-times-and-durations",
    "title": "7  Data Wrangling",
    "section": "7.6 Handling dates, times and durations",
    "text": "7.6 Handling dates, times and durations\nThere are many reasons why we encounter dates, times and durations in our research. The most apparent reason for handling dates and times is longitudinal studies that focus on change over time. Therefore, at least one of our variables would have to carry a timestamp. When it comes to data cleaning and wrangling, there are usually two tasks we need to perform:\n\nConvert data into a suitable format for further analysis, and\nPerform computations with dates and times, e.g. computing the duration between two time points.\n\nThe following sections will provide some guidance on solving commonly occurring issues and how to solve them.\n\n7.6.1 Converting dates and times for analysis\nNo matter which data collection tool you use, you likely encounter differences in how various software report dates and times back to you. Not only is there a great variety in formats in which dates and times are reported, but there are also other issues that one has to take care of, e.g. time zone differences. Here are some examples of how dates could be reported in your data:\n26 November 2022\n26 Nov 2022\nNovember, 26 2022\nNov, 26 2022\n26/11/2022\n11/26/2022\n2022-11-26\n20221126\nIt would be nice if there was a standardised method of recording dates and times and, in fact, there is one: ISO 8601. But unfortunately, not every software, let alone scholar, necessarily adheres to this standard. Luckily there is a dedicated R package that solely deals with these issues called lubridate, which is also part of the tidyverse. It helps reformat data into a coherent format and takes care of other essential aspects, such as ‘time zones, leap days, daylight savings times, and other time related quirks’ (Grolemund and Wickham 2011). Here, we will primarily focus on the most common issues, but I highly recommend looking at the ‘lubridate’ website to see what else the package has to offer.\nLet’s consider the dates (i.e. the data type date) shown above and try to convert them all into the same format. To achieve this we can use three different functions, i.e. dmy(), mdy() and ymd(). It is probably very obvious what these letters stand for:\n\nd for day,\nm for month, and\ny for year.\n\nAll we have to do is identify the order in which the dates are presented in your dataset, and the lubridate package will do its magic to standardise the format.\n\nlibrary(lubridate)\n\n# All of the below will yield the same output\ndmy(\"26 November 2022\")\n\n[1] \"2022-11-26\"\n\ndmy(\"26 Nov 2022\")\n\n[1] \"2022-11-26\"\n\nmdy(\"November, 26 2022\")\n\n[1] \"2022-11-26\"\n\nmdy(\"Nov, 26 2022\")\n\n[1] \"2022-11-26\"\n\ndmy(\"26/11/2022\")\n\n[1] \"2022-11-26\"\n\nmdy(\"11/26/2022\")\n\n[1] \"2022-11-26\"\n\nymd(\"2022-11-26\")\n\n[1] \"2022-11-26\"\n\nymd(\"20221126\")\n\n[1] \"2022-11-26\"\n\n\nBesides dates, we sometimes also have to deal with times. For example, when did someone start a survey and finish it. If you frequently work with software like Qualtrics, SurveyMonkey, etc., you are often provided with such information. From a diagnostic perspective, understanding how long it took someone to complete a questionnaire can provide valuable insights into survey optimisation and design or show whether certain participants properly engaged with your data collection tool. In R, we can use either time or dttm (date-time) as a data type to capture hours, minutes and seconds. The lubridate package handles time similarly to date with easily recognisable abbreviations:\n\nh for hour,\nm for minutes, and\ns for seconds.\n\nHere is a simple example that demonstrates how conveniently lubridate manages to handle even the most outrages ways of specifying times:\n\ndinner_time &lt;- tibble(time = c(\"17:34:21\",\n                               \"17:::34:21\",\n                               \"17 34 21\",\n                               \"17 : 34 : 21\")\n                      )\n\ndinner_time |&gt; mutate(time_new = hms(time))\n\n# A tibble: 4 × 2\n  time         time_new   \n  &lt;chr&gt;        &lt;Period&gt;   \n1 17:34:21     17H 34M 21S\n2 17:::34:21   17H 34M 21S\n3 17 34 21     17H 34M 21S\n4 17 : 34 : 21 17H 34M 21S\n\n\nLastly, dttm data types combine both: date and time. To illustrate how we can work with this format we can look at some actual data from a questionnaire. For example, the `quest_time` dataset has information about questionnaire completion dates and times..\n\nquest_time\n\n# A tibble: 84 × 3\n      id start            end             \n   &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;           \n 1     1 12/05/2016 20:46 12/05/2016 20:49\n 2     2 19/05/2016 22:08 19/05/2016 22:25\n 3     3 20/05/2016 07:05 20/05/2016 07:14\n 4     4 13/05/2016 08:59 13/05/2016 09:05\n 5     5 13/05/2016 11:25 13/05/2016 11:29\n 6     6 13/05/2016 11:31 13/05/2016 11:37\n 7     7 20/05/2016 11:29 20/05/2016 11:45\n 8     8 20/05/2016 14:07 20/05/2016 14:21\n 9     9 20/05/2016 14:55 20/05/2016 15:01\n10    10 20/05/2016 16:12 20/05/2016 16:25\n# ℹ 74 more rows\n\n\nWhile it all looks neat and tidy, keeping dates and times as a chr is not ideal, especially if we want to use it to conduct further analysis. Therefore, it would be important to transform these variables into the appropriate dttm format. We can use the function dmy_hm() similar to previous functions we used.\n\nquest_time_clean &lt;-\n  quest_time |&gt;\n  mutate(start = dmy_hm(start),\n         end = dmy_hm(end)\n         )\n\nquest_time_clean\n\n# A tibble: 84 × 3\n      id start               end                \n   &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1     1 2016-05-12 20:46:00 2016-05-12 20:49:00\n 2     2 2016-05-19 22:08:00 2016-05-19 22:25:00\n 3     3 2016-05-20 07:05:00 2016-05-20 07:14:00\n 4     4 2016-05-13 08:59:00 2016-05-13 09:05:00\n 5     5 2016-05-13 11:25:00 2016-05-13 11:29:00\n 6     6 2016-05-13 11:31:00 2016-05-13 11:37:00\n 7     7 2016-05-20 11:29:00 2016-05-20 11:45:00\n 8     8 2016-05-20 14:07:00 2016-05-20 14:21:00\n 9     9 2016-05-20 14:55:00 2016-05-20 15:01:00\n10    10 2016-05-20 16:12:00 2016-05-20 16:25:00\n# ℹ 74 more rows\n\n\nWhile the difference between this format and the previous one is relatively marginal and unimpressive, the main benefit of this conversation becomes apparent when we perform operations on these variables, e.g. computing the time it took participants to complete the questionnaire.\n\n\n7.6.2 Computing durations with dates, times and date-time variables\nOnce we have ‘corrected’ our variables to show as dates, times and date-times, we can perform further analysis and ask important questions about durations, periods and intervals of observations.\nLet’s consider the quest_time dataset which we just prepared. As mentioned earlier, it is often essential to understand how long it took participants to complete a given questionnaire. Therefore we would want to compute the duration based on the start and end time.\n\ndf_duration &lt;-\n  quest_time_clean |&gt;\n  mutate(duration = end - start)\n\ndf_duration\n\n# A tibble: 84 × 4\n      id start               end                 duration\n   &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;  \n 1     1 2016-05-12 20:46:00 2016-05-12 20:49:00  3 mins \n 2     2 2016-05-19 22:08:00 2016-05-19 22:25:00 17 mins \n 3     3 2016-05-20 07:05:00 2016-05-20 07:14:00  9 mins \n 4     4 2016-05-13 08:59:00 2016-05-13 09:05:00  6 mins \n 5     5 2016-05-13 11:25:00 2016-05-13 11:29:00  4 mins \n 6     6 2016-05-13 11:31:00 2016-05-13 11:37:00  6 mins \n 7     7 2016-05-20 11:29:00 2016-05-20 11:45:00 16 mins \n 8     8 2016-05-20 14:07:00 2016-05-20 14:21:00 14 mins \n 9     9 2016-05-20 14:55:00 2016-05-20 15:01:00  6 mins \n10    10 2016-05-20 16:12:00 2016-05-20 16:25:00 13 mins \n# ℹ 74 more rows\n\n\nFrom these results, it seems that some participants have either super-humanly fast reading skills or did not engage with the questionnaire. To get a better overview, we can plot duration to see the distribution better.\n\ndf_duration |&gt;\n  ggplot(aes(x = as.numeric(duration),\n             y = 0)) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nFor now, you should not worry about what this code means but rather consider the interpretation of the plot, i.e. that most participants (represented by a black dot) needed less than 20 minutes to complete the questionnaire. Depending on the length of your data collection tool, this could either be good or bad news. Usually, the shorter the questionnaire can be, the higher the response rate, unless you have incentives for participants to complete a longer survey.\nin Chapter @ref(descriptive-statistics), we will cover data visualisations like this boxplot in greater detail. If you cannot wait any longer and want to create your own visualisations, you are welcome to skip ahead. However, several data visualisations are awaiting you in the next chapter as well.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-dealing-with-missing-data",
    "href": "07_data_wrangling.html#sec-dealing-with-missing-data",
    "title": "7  Data Wrangling",
    "section": "7.7 Dealing with missing data",
    "text": "7.7 Dealing with missing data\nThere is hardly any Social Sciences project where researchers do not have to deal with missing data. Participants are sometimes unwilling to complete a questionnaire or miss the second round of data collection entirely, e.g. in longitudinal studies. It is not the purpose of this chapter to delve into all aspects of analysing missing data but provide a solid starting point. There are mainly three steps involved in dealing with missing data:\n\nMapping missing data,\nIdentifying patterns of missing data, and\nReplacing or removing missing data\n\n\n7.7.1 Mapping missing data\nEvery study that intends to be rigorous will have to identify how much data is missing because it affects the interpretability of our available data. In R, this can be achieved in multiple ways, but using a specialised package like naniar does help us to do this very quickly and systematically. First, we have to load the naniar package, and then we use the function vis_miss() to visualise how much and where exactly data is missing.\n\nlibrary(naniar)\n\nvis_miss(wvs_clean)\n\n\n\n\nMapping missing data with naniar\n\n\n\n\nnaniar plays along nicely with the tidyverse approach of programming. As such, it would also be possible to write wvs |&gt; vis_miss().\nAs we can see, 99,7% of our dataset is complete, and we are only missing 0.3%. The dark lines (actually blocks) refer to missing data points. On the x-axis, we can see all our variables, and on the y-axis, we see our observations. This is the same layout as our rectangular dataset: Rows are observations, and columns are variables. Overall, this dataset appears relatively complete (luckily). In addition, we can see the percentage of missing data per variable. freedom_of_choice is the variable with the most missing data, i.e. 0.79%. Still, the amount of missing data is not very large.\nWhen working with larger datasets, it might also be helpful to rank variables by their degree of missing data to see where the most significant problems lie.\n\ngg_miss_var(wvs_clean)\n\n\n\n\nMissing data per variable\n\n\n\n\nAs we already know from the previous data visualisation, freedom_of_choice has the most missing data points, while participant_id, and country_name have no missing values. If you prefer to see the actual numbers instead, we can use a series of functions that start with miss_ (for a complete list of all functions, see the reference page of naniar). For example, to retrieve the numeric values which are reflected in the plot above, we can write the following:\n\n# Summarise the missingness in each variable\nmiss_var_summary(wvs_clean)\n\n# A tibble: 7 × 3\n  variable            n_miss pct_miss\n  &lt;chr&gt;                &lt;int&gt;    &lt;num&gt;\n1 freedom_of_choice      547   0.786 \n2 relationship_status    335   0.481 \n3 age                    318   0.457 \n4 satisfaction           239   0.343 \n5 gender                  51   0.0733\n6 participant_id           0   0     \n7 country                  0   0     \n\n\nI tend to prefer data visualisations over numerical results for mapping missing data, especially in larger datasets with many variables. This also has the benefit that patterns of missing data can be more easily identified as well.\n\n\n7.7.2 Identifying patterns of missing data\nIf you find that your data ‘suffers’ from missing data, it is essential to answer another question: Is data missing systematically? This is quite an important diagnostic step since systematically missing data would imply that if we remove these observations from our dataset, we likely produce wrong results. The following section elaborate on this aspect of missing data. In general, we can distinguish missing data based on how it is missing, i.e.\n\nmissing completely at random (MCAR),\nmissing at random (MAR), and\nmissing not at random (MNAR). (Rubin 1976)\n\n\n7.7.2.1 Missing completely at random (MCAR)\nMissing completely at random (MCAR) means that neither observed nor missing data can systematically explain why data is missing. It is a pure coincidence how data is missing, and there is no underlying pattern.\nThe naniar package comes with the very popular Little’s MCAR test (Little 1988), which provides insights into whether our data is missing completely at random. Thus, we can call the function mcar_test() and inspect the result.\n\nwvs_clean |&gt;\n  # Remove variables which do not reflect a response\n  select(-participant_id) |&gt;\n  mcar_test()\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      411.    67       0               19\n\n\nWhen you run such a test, you have to ensure that variables that are not part of the data collection process are removed. In our case, the participant_id is generated by the researcher and does not represent an actual response by the participants. As such, we need to remove it using select() before we can run the test. A - inverts the meaning of select(). While select(participant_id) would do what it says, i.e. include it as the only variable in the test, select(-participant_id) results in selecting everything but this variable in our test. You will find it is sometimes easier to remove a variable with select() rather than listing all the variables you want to keep.\nSince the p.value of the test is so small that it got rounded down to 0, i.e. \\(p&lt;0.0001\\), we have to assume that our data is not missing completely at random. If we found that \\(p&gt;0.05\\), we would have confirmation that data are missing completely at random. We will cover p-values and significance tests in more detail in Chapter @ref(significance).\n\n\n7.7.2.2 Missing at random (MAR)\nMissing at random (MAR) refers to a situation where the observed data can explain missing data, but not any unobserved data. Dong and Peng (2013) (p. 2) provide a good example when this is the case:\n\nLet’s suppose that students who scored low on the pre-test are more likely to drop out of the course, hence, their scores on the post-test are missing. If we assume that the probability of missing the post-test depends only on scores on the pre-test, then the missing mechanism on the post-test is MAR. In other words, for students who have the same pre-test score, the probability of (them) missing the post-test is random.\n\nThus, the main difference between MCAR and MAR data lies in the fact that we can observe some patterns of missing data if data is MAR. These patterns are only based on data we have, i.e. observed data. We also assume that no unobserved variables can explain these or other patterns.\nAccordingly, we first look into variables with no missing data and see whether they can explain our missing data in other variables. For example, we could investigate whether missing data in freedom_of_choice is attributed to specific countries. \n\nwvs_clean |&gt;\n  group_by(country) |&gt;\n  filter(is.na(freedom_of_choice)) |&gt;\n  count(sort = TRUE)\n\n# A tibble: 32 × 2\n# Groups:   country [32]\n   country         n\n   &lt;fct&gt;       &lt;int&gt;\n 1 Japan          47\n 2 Brazil         44\n 3 New Zealand    44\n 4 Russia         43\n 5 Bolivia        40\n 6 Romania        29\n 7 Kazakhstan     27\n 8 Turkey         24\n 9 Egypt          23\n10 Serbia         20\n# ℹ 22 more rows\n\n\nIt seems four countries have exceptionally high numbers of missing data for freedom_of_choice: Japan, Brazil, New Zealand, Russia and Bolivia. Why this is the case lies beyond this dataset and is something only the researchers themselves could explain. Collecting data in different countries is particularly challenging, and one is quickly faced with different unfavourable conditions. Furthermore, the missing data is not completely random because we have some first evidence that the location of data collection might have affected its completeness.\nAnother way of understanding patterns of missing data can be achieved by looking at relationships between missing values, for example, the co-occurrence of missing values across different variables. This can be achieved by using upset plots. An upset plot consists of three parts: Set size, intersection size and a Venn diagram which defines the intersections.\n\ngg_miss_upset(wvs_clean)\n\n\n\n\n\n\n\n\nThe most frequent combination of missing data in our dataset occurs when only freedom_of_choice is missing (the first column), but nothing else. Similar results can be found for relationships_status and age. The first combination of missing data is defined by two variables: satisfaction and freedom_of_choice. In total, 107 participants had satisfaction and freedom_of_choice missing but nothing else.\nThe ‘set size’ shown in the upset plot refers to the number of missing values for each variable in the diagram. This corresponds to what we have found when looking at Figure @ref(fig:missing-data-per-variable).\nOur analysis also suggests that values are not completely randomly missing but that we have data to help explain why they are missing.\n\n\n7.7.2.3 Missing not at random (MNAR)\nLastly, missing not at random (MNAR) implies that data is missing systematically and that other variables or reasons exist that explain why data is missing. Still, they are not fully known to us. In questionnaire-based research, an easily overlooked reason that can explain missing data is the ‘page-drop-off’ phenomenon. In such cases, participants stop completing a questionnaire once they advance to another page. Figure @ref(fig:mnar-example) shows this very clearly for a large scale project where an online questionnaire was used. After almost every page break in the questionnaire, some participants decided to discontinue. Finding these types of patterns is difficult when only working with numeric values. Thus, it is always advisable to visualise your data as well. Such missing data is linked to the design of the data collection tool.\n\n\n\n\n\nMNAR pattern in a dataset due to ‘page-drop-offs’\n\n\n\n\nDefining whether a dataset is MNAR or not is mainly achieved by ruling out MCAR and MAR assumptions. It is not possible to test whether missing data is MNAR, unless we have more information about the underlying population available (Ginkel et al. 2020).\nWe have sufficient evidence that our data is MAR as was shown above, because we managed to identify some relationships between unobserved and observed data. In practice, it is very rare to find datasets that are truly MCAR (Buuren 2018). Therefore we might consider ‘imputation’ as a possible strategy to solve our missing data problem. More about imputation in the next Chapter.\nIf you are looking for more inspiration of how you could visualise and identify patterns of missingness in your data, you might find the ‘Gallery’ of the naniar website particularly useful.\n\n\n\n7.7.3 Replacing or removing missing data\nOnce you determined which pattern of missing data applies to your dataset, it is time to evaluate how we want to deal with those missing values. Generally, you can either keep the missing values as they are, replace them or remove them entirely.\nJakobsen et al. (2017) provide a rule of thumb of 5% where researchers can consider missing data as negligible, i.e. we can ignore the missing values because they won’t affect our analysis in a significant way. They also argue that if the proportion of missing data exceeds 40%, we should also only work with our observed data. However, with such a large amount of missing data, it is questionable whether we can rely on our analysis as much as we want to. If the missing data lies somewhere in-between this range, we need to consider the missing data pattern at hand.\nIf data is MCAR, we could remove missing data. This process is called ‘listwise deletion’, i.e. you remove all data from a participant with missing data. As just mentioned, removing missing values is only suitable if you have relatively few missing values in your dataset (Schafer 1999), as is the case with the wvs dataset. There are additional problems with deleting observations listwise, many of which are summarised by Buuren (2018) in his introduction. Usually, we try to avoid removing data as much as possible. If you wanted to perform listwise deletion, it can be done with a single function call: na.omit() from the built-in stats package. Alternatively, you could also use drop_na() from the tidyr package, which is automatically loaded when used library(tidyverse). Here is an example of how we can apply these two functions to achieve the same effect.\n\n# No more missing data in this plot\nwvs_clean |&gt; na.omit() |&gt; vis_miss()\n\n\n\n\n\n\n\n\n\n# Here is the alternative method with drop_na()\nwvs_clean |&gt; drop_na() |&gt; vis_miss()\n\n\n\n\n\n\n\n\n\n# How many observations are left after we removed all missing data?\nwvs_clean |&gt; na.omit() |&gt; count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 68312\n\n\nIf you wanted to remove the missing data without the plot, you could use wvs_no_na &lt;- na.omit(wvs) or wvs_no_na &lt;- drop_na(). However, I always recommend ‘saving’ the result in a new object to ensure we keep the original data available. This can be helpful when trying to compare how this decision affects my analysis, i.e. I can run the analysis with and without missing data removed. For the wvs dataset, using na.omit() does not seem to be the best option.\nBased on our analysis of the wvs dataset (by no means complete!), we could assume that data is MAR. In such cases, it is possible to ‘impute’ the missing values, i.e. replacing the missing data with computed scores. This is possible because we can model the missing data based on variables we have. We cannot model missing data based on variables we have not measured in our study for obvious reasons.\nYou might be thinking: “Why would it make data better if we ‘make up’ numbers? Is this not cheating?” Imputation of missing values is a science in itself. There is plenty to read about the process of handling missing data, which would reach far beyond the scope of this book. However, the seminal work of Buuren (2018), Dong and Peng (2013) and Jakobsen et al. (2017) are excellent starting points. In short: Simply removing missing data can lead to biases in your data, e.g. if we removed missing data in our dataset, we would mainly exclude participants from Japan, Brazil, New Zealand, Russia and Bolivia (as we found out earlier). While imputation is not perfect, it can produce more reliable results than not using imputation at all, assuming our data meets the requirements for such imputation.\nEven though our dataset has only a minimal amount of missing data (relative to the entire size), we can still use imputation. There are many different ways to approach this task, one of which is ‘multiple imputation’. As highlighted by Ginkel et al. (2020), multiple imputation has not yet reached the same popularity as listwise deletion, despite its benefits. The main reason for this lies in the complexity of using this technique. There are many more approaches to imputation, and going through them all in detail would be impossible and distract from the book’s main purpose. Still, I would like to share some interesting packages with you that use different imputation methods:\n\nmice (Uses multivariate imputation via chained equations)\nAmelia (Uses a bootstrapped-based algorithm)\nmissForest (Uses a random forest algorithm)\nHmisc (Uses additive regression, bootstrapping, and predictive mean matching)\nmi (Uses posterior predictive distribution, predictive mean matching, mean-imputation, median-imputation, or conditional mean-imputation)\n\nBesides multiple imputation, there is also the option for single imputation. The simputation package offers a great variety of imputation functions, one of which also fits our data quite well impute_knn(). This function makes use of a clustering technique called ‘K-nearest neighbour’. In this case, the function will look for observations closest to the one with missing data and take the value of that observation. In other words, it looks for participants who answered the questionnaire in a very similar way. The great convenience of this approach is that it can handle all kinds of data types simultaneously, which is not true for all imputation techniques.\nIf we apply this function to our dataset, we have to write the following:\n\nwvs_nona &lt;-\n  wvs_clean |&gt;\n  select(-participant_id) |&gt;\n  \n  # Transform our tibble into a data frame\n  as.data.frame() |&gt;\n  simputation::impute_knn(. ~ .)\n\n\n# Be aware that our new dataframe has different datatypes\nglimpse(wvs_nona)\n\nRows: 69,578\nColumns: 6\n$ country             &lt;fct&gt; Andorra, Andorra, Andorra, Andorra, Andorra, Andor…\n$ gender              &lt;fct&gt; female, male, male, female, male, female, female, …\n$ age                 &lt;chr&gt; \"60\", \"47\", \"48\", \"62\", \"49\", \"51\", \"33\", \"55\", \"4…\n$ relationship_status &lt;fct&gt; married, living together as married, separated, li…\n$ freedom_of_choice   &lt;chr&gt; \"10\", \"9\", \"9\", \"9\", \"8\", \"10\", \"10\", \"8\", \"8\", \"1…\n$ satisfaction        &lt;chr&gt; \"10\", \"9\", \"9\", \"8\", \"7\", \"10\", \"5\", \"8\", \"8\", \"10…\n\n\n\n# Let's fix this\nwvs_nona &lt;-\n  wvs_nona |&gt;\n  mutate(age = as.numeric(age),\n         freedom_of_choice = as.numeric(freedom_of_choice),\n         satisfaction = as.numeric(satisfaction))\n\nThe function impute_knn(. ~ .) might look like a combination of text with an emoji (. ~ .). This imputation function requires us to specify a model to impute the data. Since we want to impute all missing values in the dataset and use all variables available, we put . on both sides of the equation, separated by a ~. The . reflects that we do not specify a specific variable but instead tell the function to use all variables that are not mentioned. In our case, we did not mention any variables at all, and therefore it chooses all of them. For example, if we wanted to impute only freedom_of_choice, we would have to put impute_knn(freedom_of_choice ~ .). We will elaborate more on how to specify models when we cover regression models (see Chapter @ref(regression)).\nAs you will have noticed, we also had to convert our tibble into a data frame using as.data.frame(). As mentioned in Chapter @ref(functions), some functions require a specific data type or format. The simputation package works with dplyr, but it prefers data frames. Based on my experiences, the wrong data type or format is one of the most frequent causes of errors that novice R programmers report. So, keep an eye on it and read the documentation carefully. Also, once you inspect the new data frame, you will notice that some of our double variables have now turned into character values. Therefore, I strongly recommend checking your newly created data frames to avoid any surprises further down the line of your analysis. R requires us to be very precise in our work, which I think is rather an advantage than an annoyance2.\nBe aware that imputation of any kind can take a long time. For example, my MacBook Pro took about 4.22 seconds to complete impute_knn() with the wvs dataset. If we used multiple imputation, this would have taken considerably longer, i.e. several minutes and more.\nWe now should have a dataset that is free of any missing values. To ensure this is the case we can create the missing data matrix that we made at the beginning of this chapter (see Figure @ref(fig:mapping-missing-data)).\n\nvis_miss(wvs_nona)\n\n\n\n\n\n\n\n\nAs the plot demonstrates, we managed to obtain a dataset which is now free of missing values and we can perform our next steps of data wrangling or analysis.\n\n\n7.7.4 Two more methods of replacing missing data you hardly ever need\nThere are two more options for replacing missing data3, but these I would hardly ever recommend. Still, this section would not be complete without mentioning them, and they have their use-cases.\nThe first method involves the function replace_na(). It allows you to replace any missing data with a value of your choice. For example, let’s consider the following dataset which contains feedback from students regarding one of their lectures.\n\n# The dataset which contains the feedback\nstudent_feedback &lt;- tibble(name = c(\"Thomas\", NA, \"Helen\", NA),\n                           feedback = as.numeric(c(6, 5, NA, 4)),\n                           overall = as_factor(c(\"excellent\", \"very good\", NA, NA))\n                           )\n\nstudent_feedback\n\n# A tibble: 4 × 3\n  name   feedback overall  \n  &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;    \n1 Thomas        6 excellent\n2 &lt;NA&gt;          5 very good\n3 Helen        NA &lt;NA&gt;     \n4 &lt;NA&gt;          4 &lt;NA&gt;     \n\n\nWe have different types of data to deal with in student_feedback, and there are several missing data points, i.e. NA in each column. We can use the replace_na() function to decide how to replace the missing data. For feedback, we might want to insert a number, for example, the average of all available ratings ((6 + 5+ 4) / 3 = 5). However, we might wish to add some string values for name and overall. We have to keep in mind that replace_na() requires a chr variable as input, which is not the case for overall, because it is a factor. Consequently, we need to go back to Chapter @ref(change-data-types) and change the data type before replacing NAs. Here is how we can achieve all this in a few lines of code:\n\nstudent_feedback |&gt;\n  mutate(name = replace_na(name, \"Unkown\"),\n         feedback = replace_na(feedback, 5),\n         overall = replace_na(as.character(overall), \"unrated\"),\n         overall = as_factor(overall)\n         )\n\n# A tibble: 4 × 3\n  name   feedback overall  \n  &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;    \n1 Thomas        6 excellent\n2 Unkown        5 very good\n3 Helen         5 unrated  \n4 Unkown        4 unrated  \n\n\nWe successfully replaced all the missing values with values of our choice. Only for overall we have to perform two steps. If you feel adventurous, you could also warp the as_factor() function around the replace_na() one. This would result in using only one line of code for this variable.\n\nstudent_feedback |&gt;\n  mutate(name = replace_na(name, \"Unkown\"),\n         feedback = replace_na(feedback, 0),\n         overall = as_factor(replace_na(as.character(overall), \"unrated\"))\n         )\n\n# A tibble: 4 × 3\n  name   feedback overall  \n  &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;    \n1 Thomas        6 excellent\n2 Unkown        5 very good\n3 Helen         0 unrated  \n4 Unkown        4 unrated  \n\n\nThis approach to handling missing data can be beneficial when addressing these values explicitly, e.g. in a plot. While one could keep the label of NA, it sometimes makes sense to use a different label that is more meaningful to your intended audience. Still, I would abstain from replacing NAs with an arbitrary value unless there is a good reason for it.\nThe second method of replacing missing values is less arbitrary and requires the function fill(). It replaces missing values with the previous or next non-missing value. The official help page for this function offers some excellent examples of when one would need it, i.e. when specific values are only recorded once for a set of data points. For example, we might want to know how many students are taking certain lectures, but the module ID is only recorded for the first student on each module.\n\n# The dataset which contains student enrollment numbers\nstudent_enrollment &lt;- tibble(module = c(\"Management\", NA, NA,\n                                        NA, \"Marketing\", NA),\n                             name = c(\"Alberto\", \"Frances\", \"Martin\",\n                                    \"Alex\", \"Yuhui\", \"Himari\")\n                             )\nstudent_enrollment\n\n# A tibble: 6 × 2\n  module     name   \n  &lt;chr&gt;      &lt;chr&gt;  \n1 Management Alberto\n2 &lt;NA&gt;       Frances\n3 &lt;NA&gt;       Martin \n4 &lt;NA&gt;       Alex   \n5 Marketing  Yuhui  \n6 &lt;NA&gt;       Himari \n\n\nHere it is unnecessary to guess which values we have to insert because we know that Alberto, Frances, Martin, and Alex are taking the Management lecture. At the same time, Yuhui and Himari opted for Marketing. The fill() function can help us complete the missing values accordingly. By default, it completes values top-down, i.e. it takes the last value that was not missing and copies it down until we reach a non-missing value.\n\nstudent_enrollment |&gt; fill(module)\n\n# A tibble: 6 × 2\n  module     name   \n  &lt;chr&gt;      &lt;chr&gt;  \n1 Management Alberto\n2 Management Frances\n3 Management Martin \n4 Management Alex   \n5 Marketing  Yuhui  \n6 Marketing  Himari \n\n\nIf you have to reverse the direction of how R should fill in the blank cells in your dataset, you can use fill(.direction = \"up\"). In our example, this would make not much sense, but for the sake of demonstration, here is what would happen:\n\nstudent_enrollment |&gt; fill(module, .direction = \"up\")\n\n# A tibble: 6 × 2\n  module     name   \n  &lt;chr&gt;      &lt;chr&gt;  \n1 Management Alberto\n2 Marketing  Frances\n3 Marketing  Martin \n4 Marketing  Alex   \n5 Marketing  Yuhui  \n6 &lt;NA&gt;       Himari \n\n\nThis method can be helpful if the system you use to collect or produce your data is only recording changes in values, i.e. it does not record all values for all observations in your dataset.\n\n\n7.7.5 Main takeaways regarding dealing with missing data\nHandling missing data is hardly ever a simple process. Do not feel discouraged if you get lost in the myriad of options. While there is some guidance on how and when to use specific strategies to deal with missing values in your dataset, the most crucial point to remember is: Be transparent about what you did. As long as you can explain why you did something and how you did it, everyone can follow your thought process and help improve your analysis. However, ignoring the fact that data is missing and not acknowledging it is more than just unwise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-latent-constructs",
    "href": "07_data_wrangling.html#sec-latent-constructs",
    "title": "7  Data Wrangling",
    "section": "7.8 Latent constructs and their reliability",
    "text": "7.8 Latent constructs and their reliability\nSocial Scientists commonly face the challenge that we want to measure something that cannot be measured directly. For example, ‘happiness’ is a feeling that does not naturally occur as a number we can observe and measure. The opposite is true for ‘temperature’ which is naturally measured in numbers. At the same time, ‘happiness’ is much more complex of a variable than ‘temperature’, because ‘happiness’ can unfold in various ways and be caused by different triggers (e.g. a joke, an unexpected present, tasty food, etc.). To account for this, we often work with ‘latent variables’. These are defined as variables that are not directly measured but are inferred from other variables. In practice, we often use multiple questions in a questionnaire to measure one latent variable, usually by computing the mean of those questions.\n\n7.8.1 Computing latent variables\nThe gep dataset from the r4np package includes data about students’ social integration experience (si) and communication skills development (cs). Data were obtained using the Global Education Profiler (GEP). I extracted three different questions (also called ‘items’) from the questionnaire, which measure si, and three items that measure cs. This dataset only consists of a randomly chosen set of responses, i.e. 300 out of over 12,000.\n\nglimpse(gep)\n\nRows: 300\nColumns: 12\n$ age                           &lt;dbl&gt; 22, 26, 21, 23, 25, 27, 24, 23, 21, 24, …\n$ gender                        &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", …\n$ level_of_study                &lt;chr&gt; \"UG\", \"PGT\", \"UG\", \"UG\", \"PGT\", \"PGT\", \"…\n$ si_socialise_with_people_exp  &lt;dbl&gt; 6, 3, 4, 3, 4, 2, 5, 1, 6, 6, 3, 3, 4, 5…\n$ si_supportive_friends_exp     &lt;dbl&gt; 4, 4, 5, 4, 4, 2, 5, 1, 6, 6, 3, 3, 4, 6…\n$ si_time_socialising_exp       &lt;dbl&gt; 5, 2, 4, 4, 3, 2, 6, 3, 6, 6, 3, 2, 4, 6…\n$ cs_explain_ideas_imp          &lt;dbl&gt; 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6…\n$ cs_find_clarification_imp     &lt;dbl&gt; 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5…\n$ cs_learn_different_styles_imp &lt;dbl&gt; 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5…\n$ cs_explain_ideas_exp          &lt;dbl&gt; 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6…\n$ cs_find_clarification_exp     &lt;dbl&gt; 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5…\n$ cs_learn_different_styles_exp &lt;dbl&gt; 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5…\n\n\nFor example, if we wanted to know how each student scored with regards to social integration (si), we have to\n\ncompute the mean (mean()) of all related items (i.e. all variables starting with si_),\nfor each row (rowwise()) because each row presents one participant.\n\nThe same is true for communication skills (cs_imp and cs_exp). We can compute all three variables in one go. For each variable, we compute mean() and use c() to list all the variables that we want to include in the mean:\n\n# Compute the scores for the latent variable 'si' and 'cs'\ngep &lt;-\n  gep |&gt;\n  rowwise() |&gt;\n  mutate(si = mean(c(si_socialise_with_people_exp,\n                     si_supportive_friends_exp,\n                     si_time_socialising_exp\n                     )\n                   ),\n         cs_imp = mean(c(cs_explain_ideas_imp,\n                         cs_find_clarification_imp,\n                         cs_learn_different_styles_imp\n                         )\n                       ),\n         cs_exp = mean(c(cs_explain_ideas_exp,\n                         cs_find_clarification_exp,\n                         cs_learn_different_styles_exp\n                         )\n                       )\n         )\n\nglimpse(gep)\n\nRows: 300\nColumns: 15\nRowwise: \n$ age                           &lt;dbl&gt; 22, 26, 21, 23, 25, 27, 24, 23, 21, 24, …\n$ gender                        &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", …\n$ level_of_study                &lt;chr&gt; \"UG\", \"PGT\", \"UG\", \"UG\", \"PGT\", \"PGT\", \"…\n$ si_socialise_with_people_exp  &lt;dbl&gt; 6, 3, 4, 3, 4, 2, 5, 1, 6, 6, 3, 3, 4, 5…\n$ si_supportive_friends_exp     &lt;dbl&gt; 4, 4, 5, 4, 4, 2, 5, 1, 6, 6, 3, 3, 4, 6…\n$ si_time_socialising_exp       &lt;dbl&gt; 5, 2, 4, 4, 3, 2, 6, 3, 6, 6, 3, 2, 4, 6…\n$ cs_explain_ideas_imp          &lt;dbl&gt; 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6…\n$ cs_find_clarification_imp     &lt;dbl&gt; 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5…\n$ cs_learn_different_styles_imp &lt;dbl&gt; 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5…\n$ cs_explain_ideas_exp          &lt;dbl&gt; 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6…\n$ cs_find_clarification_exp     &lt;dbl&gt; 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5…\n$ cs_learn_different_styles_exp &lt;dbl&gt; 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5…\n$ si                            &lt;dbl&gt; 5.000000, 3.000000, 4.333333, 3.666667, …\n$ cs_imp                        &lt;dbl&gt; 5.333333, 5.000000, 5.333333, 5.000000, …\n$ cs_exp                        &lt;dbl&gt; 6.000000, 4.666667, 3.666667, 5.000000, …\n\n\nCompared to dealing with missing data, this is a fairly straightforward task. However, there is a caveat. Before we can compute the mean across all these variables, we need to know and understand whether all these scores reliably contribute to one single latent variable. If not, we would be in trouble and make a significant mistake. This is commonly known as internal consistency and is a measure of reliability, i.e. how much we can rely on the fact that multiple questions in a questionnaire measure the same underlying, i.e. latent, construct.\n\n\n7.8.2 Checking internal consistency of items measuring a latent variable\nBy far, the most common approach to assessing internal consistency of latent variables is Cronbach’s \\(\\alpha\\). This indicator looks at how strongly a set of items (i.e. questions in your questionnaire) are related to each other. For example, the stronger the relationship of all items starting with si_ to each other, the more likely we achieve a higher Cronbach’s \\(\\alpha\\). The psych package has a suitable function to compute it for us.\nInstead of listing all the items by hand, I use the function starts_with() to pick only the variables whose names start with si_. It certainly pays off to think about your variable names more thoroughly in advance to benefit from such shortcuts (see also Chapter @ref(colnames-cleaning)).\n\ngep |&gt;\n  select(starts_with(\"si_\")) |&gt;\n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = select(gep, starts_with(\"si_\")))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.85      0.85     0.8      0.66 5.8 0.015  3.5 1.3     0.65\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.82  0.85  0.88\nDuhachek  0.82  0.85  0.88\n\n Reliability if an item is dropped:\n                             raw_alpha std.alpha G6(smc) average_r S/N alpha se\nsi_socialise_with_people_exp      0.82      0.82    0.70      0.70 4.6    0.021\nsi_supportive_friends_exp         0.77      0.77    0.63      0.63 3.4    0.027\nsi_time_socialising_exp           0.79      0.79    0.65      0.65 3.7    0.025\n                             var.r med.r\nsi_socialise_with_people_exp    NA  0.70\nsi_supportive_friends_exp       NA  0.63\nsi_time_socialising_exp         NA  0.65\n\n Item statistics \n                               n raw.r std.r r.cor r.drop mean  sd\nsi_socialise_with_people_exp 300  0.86  0.86  0.75   0.69  3.7 1.4\nsi_supportive_friends_exp    300  0.90  0.89  0.81   0.75  3.5 1.6\nsi_time_socialising_exp      300  0.88  0.88  0.79   0.73  3.2 1.5\n\nNon missing response frequency for each item\n                                1    2    3    4    5    6 miss\nsi_socialise_with_people_exp 0.06 0.17 0.22 0.24 0.19 0.12    0\nsi_supportive_friends_exp    0.13 0.18 0.20 0.18 0.18 0.13    0\nsi_time_socialising_exp      0.15 0.21 0.22 0.20 0.12 0.10    0\n\n\nThe function alpha() returns a lot of information. The most important part, though, is shown at the very beginning:\n\n\n raw_alpha std.alpha G6(smc) average_r  S/N  ase mean   sd median_r\n      0.85      0.85     0.8      0.66 5.76 0.01 3.46 1.33     0.65\n\n\nIn most publications, researchers would primarily report the raw_alpha value. This is fine, but it is not a bad idea to include at least std.alpha and G6(smc) as well.\nIn terms of interpretation, Cronbrach’s \\(\\alpha\\) scores can range from 0 to 1. The closer the score to 1, the higher we would judge its reliability. Nunally (1967) originally provided the following classification for Cronbach’s \\(\\alpha\\):\n\nbetween 0.6 and 0.5 can be sufficient during the early stages of development,\n0.8 or higher is sufficient for most basic research,\n0.9 or higher is suitable for applied research, where the questionnaires are used to make critical decisions, e.g. clinical studies, university admission tests, etc., with a ‘desired standard’ of 0.95.\n\nHowever, a few years later, Nunally (1978) revisited his original categorisation and considered 0.7 or higher as a suitable benchmark in more exploratory-type research. This gave grounds for researchers to pick and choose the ‘right’ threshold for them (Henson 2001). Consequently, depending on your research field, the expected reliability score might lean more towards 0.7 or 0.8. Still, the higher the score, the more reliable you latent variable is.\nOur dataset shows that si scores a solid \\(\\alpha = 0.85\\), which is excellent. We should repeat this step for cs_imp and cs_exp as well. However, we have to adjust select(), because we only want variables included that start with cs_ and end with the facet of the respective variable, i.e. _imp or _exp. Otherwise we compute the Cronbach’s \\(\\alpha\\) for a combined latent construct.\n\n# Select variables which start with 'cs_'\ngep |&gt;\n  select(starts_with(\"cs_\")) |&gt;\n  glimpse()\n\nRows: 300\nColumns: 8\nRowwise: \n$ cs_explain_ideas_imp          &lt;dbl&gt; 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6…\n$ cs_find_clarification_imp     &lt;dbl&gt; 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5…\n$ cs_learn_different_styles_imp &lt;dbl&gt; 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5…\n$ cs_explain_ideas_exp          &lt;dbl&gt; 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6…\n$ cs_find_clarification_exp     &lt;dbl&gt; 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5…\n$ cs_learn_different_styles_exp &lt;dbl&gt; 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5…\n$ cs_imp                        &lt;dbl&gt; 5.333333, 5.000000, 5.333333, 5.000000, …\n$ cs_exp                        &lt;dbl&gt; 6.000000, 4.666667, 3.666667, 5.000000, …\n\n\nTherefore it is necessary to define two conditions to select the correct items in our dataset using ends_with() and combine these two with the logical operator & (see also Table @ref(tab:logical-operators-r)).\n\n# Do the above but include only variables that also end with '_imp'\ngep |&gt;\n  select(starts_with(\"cs_\") & ends_with(\"_imp\")) |&gt;\n  glimpse()\n\nRows: 300\nColumns: 4\nRowwise: \n$ cs_explain_ideas_imp          &lt;dbl&gt; 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6…\n$ cs_find_clarification_imp     &lt;dbl&gt; 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5…\n$ cs_learn_different_styles_imp &lt;dbl&gt; 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5…\n$ cs_imp                        &lt;dbl&gt; 5.333333, 5.000000, 5.333333, 5.000000, …\n\n\nWe also created a variable that starts with cs_ and ends with _imp, i.e. the latent variable we computed, which also has to be removed.\n\n# Remove 'cs_imp', because it is the computed latent variable\ngep |&gt;\n  select(starts_with(\"cs_\") & ends_with(\"_imp\"), -cs_imp) |&gt;\n  glimpse()\n\nRows: 300\nColumns: 3\nRowwise: \n$ cs_explain_ideas_imp          &lt;dbl&gt; 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6…\n$ cs_find_clarification_imp     &lt;dbl&gt; 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5…\n$ cs_learn_different_styles_imp &lt;dbl&gt; 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5…\n\n\nThus, we end up computing the Cronbach’s \\(\\alpha\\) for both variables in the following way:\n\ngep |&gt;\n  select(starts_with(\"cs_\") & ends_with(\"_imp\"), -cs_imp) |&gt;\n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = select(gep, starts_with(\"cs_\") & ends_with(\"_imp\"), \n    -cs_imp))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.86      0.87    0.82      0.68 6.5 0.014    5 0.98     0.65\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.83  0.86  0.89\nDuhachek  0.84  0.86  0.89\n\n Reliability if an item is dropped:\n                              raw_alpha std.alpha G6(smc) average_r S/N\ncs_explain_ideas_imp               0.78      0.78    0.65      0.65 3.6\ncs_find_clarification_imp          0.76      0.76    0.62      0.62 3.2\ncs_learn_different_styles_imp      0.88      0.88    0.79      0.79 7.4\n                              alpha se var.r med.r\ncs_explain_ideas_imp             0.025    NA  0.65\ncs_find_clarification_imp        0.028    NA  0.62\ncs_learn_different_styles_imp    0.014    NA  0.79\n\n Item statistics \n                                n raw.r std.r r.cor r.drop mean  sd\ncs_explain_ideas_imp          300  0.90  0.90  0.85   0.77  5.2 1.0\ncs_find_clarification_imp     300  0.91  0.91  0.87   0.79  5.1 1.1\ncs_learn_different_styles_imp 300  0.86  0.85  0.71   0.67  4.8 1.2\n\nNon missing response frequency for each item\n                                 1    2    3    4    5    6 miss\ncs_explain_ideas_imp          0.01 0.01 0.06 0.14 0.27 0.51    0\ncs_find_clarification_imp     0.01 0.02 0.05 0.14 0.30 0.47    0\ncs_learn_different_styles_imp 0.01 0.03 0.09 0.19 0.31 0.36    0\n\n\n\ngep |&gt;\n  select(starts_with(\"cs_\") & ends_with(\"_exp\"), -cs_exp) |&gt;\n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = select(gep, starts_with(\"cs_\") & ends_with(\"_exp\"), \n    -cs_exp))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.81      0.82    0.76       0.6 4.4 0.019  4.2 1.1     0.58\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.77  0.81  0.85\nDuhachek  0.78  0.81  0.85\n\n Reliability if an item is dropped:\n                              raw_alpha std.alpha G6(smc) average_r S/N\ncs_explain_ideas_exp               0.69      0.69    0.53      0.53 2.3\ncs_find_clarification_exp          0.73      0.74    0.58      0.58 2.8\ncs_learn_different_styles_exp      0.81      0.81    0.68      0.68 4.2\n                              alpha se var.r med.r\ncs_explain_ideas_exp             0.035    NA  0.53\ncs_find_clarification_exp        0.031    NA  0.58\ncs_learn_different_styles_exp    0.022    NA  0.68\n\n Item statistics \n                                n raw.r std.r r.cor r.drop mean  sd\ncs_explain_ideas_exp          300  0.88  0.88  0.80   0.71  4.2 1.3\ncs_find_clarification_exp     300  0.85  0.86  0.76   0.68  4.5 1.2\ncs_learn_different_styles_exp 300  0.84  0.82  0.67   0.61  4.0 1.4\n\nNon missing response frequency for each item\n                                 1    2    3    4    5    6 miss\ncs_explain_ideas_exp          0.04 0.09 0.11 0.33 0.26 0.17    0\ncs_find_clarification_exp     0.02 0.05 0.13 0.27 0.32 0.21    0\ncs_learn_different_styles_exp 0.06 0.09 0.21 0.24 0.22 0.17    0\n\n\nSimilarly, to si, cs_imp and cs_exp show very good internal consistency scores: \\(\\alpha_{cs\\_imp} = 0.86\\) and \\(\\alpha_{cs\\_exp} = 0.81\\). Based on these results we could be confident to use our latent variables for further analysis.\n\n\n7.8.3 Confirmatory factor analysis\nHowever, while Cronbach’s \\(\\alpha\\) is very popular due to its simplicity, there is plenty of criticism. Therefore, it is often not enough to report the Cronbach’s \\(\\alpha\\), but undertake additional steps. Depending on the stage of development of your measurement instrument (e.g. your questionnaire), you likely have to perform one of the following before computing the \\(\\alpha\\) scores:\n\nExploratory factor analysis (EFA): Generally used to identify latent variables in a set of questionnaire items. Often these latent variables are not yet known. This process is often referred to as dimension reduction.\nConfirmatory factor analysis (CFA): This approach is used to confirm whether a set of items truly reflect a latent variable which we defined ex-ante.\n\nDuring the development of a new questionnaire, researchers often perform EFA on a pilot dataset to see which factors would emerge without prior assumptions. Once this process has been completed, another round of data would be collected to perform a CFA. This CFA ideally confirms the results from the pilot study. It is also not unusual to split a larger dataset into two parts and then perform EFA and CFA on separate sub-samples. However, this latter approach has the disadvantage that researchers would not have the opportunity to improve the questionnaire between EFA and CFA.\nSince the gep data is based on an established measurement tool, we perform a CFA. To perform a CFA, we use the popular lavaan (Latent Variable Analysis) package. The steps of running a CFA in R include:\n\nDefine which variables are supposed to measure a specific latent variable (i.e. creating a model).\nRun the CFA to see whether our model fits the data we collected.\nInterpret the results based on various indicators.\n\nThe code chunk below breaks down these steps in the context of the lavaan package.\n\nlibrary(lavaan)\n\n#1: Define the model which explains how items relate to latent variables\nmodel &lt;- '\nsocial_integration =~\nsi_socialise_with_people_exp +\nsi_supportive_friends_exp +\nsi_time_socialising_exp\n\ncomm_skills_imp =~\ncs_explain_ideas_imp +\ncs_find_clarification_imp +\ncs_learn_different_styles_imp\n\ncomm_skills_exp =~\ncs_explain_ideas_exp +\ncs_find_clarification_exp +\ncs_learn_different_styles_exp\n'\n\n#2: Run the CFA to see how well this model fits our data\nfit &lt;- cfa(model, data = gep)\n\n#3a: Extract the performance indicators\nfit_indices &lt;- fitmeasures(fit)\n\n#3b: We tidy the results with enframe() and\n#    pick only those indices we are most interested in\nfit_indices |&gt;\n  enframe() |&gt;\n  filter(name == \"cfi\" |\n         name == \"srmr\" |\n         name == \"rmsea\") |&gt;\n  mutate(value = round(value, 3))   # Round to 3 decimal places\n\n# A tibble: 3 × 2\n  name  value     \n  &lt;chr&gt; &lt;lvn.vctr&gt;\n1 cfi   0.967     \n2 rmsea 0.081     \n3 srmr  0.037     \n\n\nThe enframe() function is useful to turn a named vector (i.e. a named sequence of values) into a tibble, which we can further manipulate using the functions we already know. However, it is not required to take this step to inspect the results. This can be done out of pure convenience. If we want to know where name and value came from we have to inspect the output from enframe(fit_indices).\n\nenframe(fit_indices)\n\n# A tibble: 46 × 2\n   name            value       \n   &lt;chr&gt;           &lt;lvn.vctr&gt;  \n 1 npar            2.100000e+01\n 2 fmin            1.177732e-01\n 3 chisq           7.066390e+01\n 4 df              2.400000e+01\n 5 pvalue          1.733810e-06\n 6 baseline.chisq  1.429729e+03\n 7 baseline.df     3.600000e+01\n 8 baseline.pvalue 0.000000e+00\n 9 cfi             9.665187e-01\n10 tli             9.497780e-01\n# ℹ 36 more rows\n\n\nThese column names were generated when we called the function enframe(). I often find myself working through chains of analytical steps iteratively to see what the intermediary steps produce. This also makes it easier to spot any mistakes early on. Therefore, I recommend slowly building up your dplyr chains of function calls, especially when you just started learning R and the tidyverse approach of data analysis.\nThe results of our CFA appear fairly promising:\n\nThe cfi (Comparative Fit Index) lies above 0.95,\nThe rmsea (Root Mean Square Error of Approximation) appears slightly higher than desirable, which usually is lower than 0.6, and\nThe srmr (Standardised Root Mean Square Residual) lies well below 0.08 (West et al. 2012; Hu and Bentler 1999).\n\nOverall, the model seems to suggest a good fit with our data. Combined with the computed Cronbach’s \\(\\alpha\\), we can be reasonably confident in our latent variables and perform further analytical steps.\n\n\n7.8.4 Reversing items with opposite meaning\nQuestionnaires like the gep consist of items that are all phrased in the ‘same direction’. What I mean by this is that all questions are asked so that a high score would carry the same meaning. However, this is not always true. Consider the following three questions as an example:\n\nQ1: ‘I enjoy talking to people I don’t know.’\nQ2: ‘I prefer to stay at home.’\nQ3: ‘Making new friends is exciting.’\n\nQ1 and Q3 both refer to aspects of socialising with or getting to know others. Q2, however, seems to ask for the opposite, i.e. not engaging with others and staying at home. Intuitively, we would assume that a participant who scores high on Q1 and Q2 would score low on Q3. Q3 is a so-called reversed item, i.e. the meaning of the question is reversed compared to other questions in this set. This strategy is commonly used to test for respondents’ engagement with a questionnaire and break a possible response pattern. For example, if a participant scores 5 out of 6 on Q1, Q2 and Q3, we have to assume that this person might not have paid much attention to the statements because someone who scores high on Q1 and Q3 would likely not score high on Q2. However, their usefulness is debated across disciplines. When designing your data collection tool, items must be easy to understand, short, not double-barreled and reflect a facet of a latent variable appropriately. These aspects are far more important than reversing the meaning of items.\nSuppose we work with a dataset containing reversed items. In that case, we have to recode the answers for each participant to reflect the same meaning as the other questions before computing the Cronbach’s \\(\\alpha\\) or performing an EFA or CFA.\nLet’s assume we ask some of our friends to rate Q1, Q2 and Q3 on a scale from 1-6. We might obtain the following results:\n\ndf_social &lt;- tibble(name = c(\"Agatha Harkness\", \"Daren Cross\", \"Hela\",\n                             \"Aldrich Killian\", \"Malekith\", \"Ayesha\"),\n                    q1 = c(6, 2, 4, 5, 5, 2),\n                    q2 = c(2, 5, 1, 2, 3, 6),\n                    q3 = c(5, 1, 5, 4, 5, 1))\n\ndf_social\n\n# A tibble: 6 × 4\n  name               q1    q2    q3\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Agatha Harkness     6     2     5\n2 Daren Cross         2     5     1\n3 Hela                4     1     5\n4 Aldrich Killian     5     2     4\n5 Malekith            5     3     5\n6 Ayesha              2     6     1\n\n\nWhile, for example, Agatha Harkness and Aldrich Killian are both very social, Ayesha seems to be rather the opposite. For all of our respondents, we can notice a pattern that if the value of q1 is high, usually q3 is high too, but q2 is low. We would be very disappointed and probably puzzled if we now computed the Cronbach’s \\(\\alpha\\) of our latent variable social.\n\n# Cronbach's alpha without reverse coding q2\ndf_social |&gt;\n  select(-name) |&gt;\n  psych::alpha()\n\nWarning in psych::alpha(select(df_social, -name)): Some items were negatively correlated with the first principal component and probably \nshould be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\nSome items ( q2 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\n\nReliability analysis   \nCall: psych::alpha(x = select(df_social, -name))\n\n  raw_alpha std.alpha G6(smc) average_r   S/N ase mean   sd median_r\n      -2.2      -1.7     0.7     -0.27 -0.64 1.7  3.6 0.69     -0.8\n\n    95% confidence boundaries \n          lower alpha upper\nFeldt    -12.47 -2.18  0.52\nDuhachek  -5.49 -2.18  1.13\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r   S/N alpha se var.r med.r\nq1    -21.00    -21.04   -0.91     -0.91 -0.95   17.947    NA -0.91\nq2      0.94      0.95    0.91      0.91 19.70    0.042    NA  0.91\nq3     -7.61     -8.03   -0.80     -0.80 -0.89    6.823    NA -0.80\n\n Item statistics \n   n raw.r std.r r.cor r.drop mean  sd\nq1 6  0.93  0.94  0.95   0.29  4.0 1.7\nq2 6 -0.58 -0.61 -0.89  -0.88  3.2 1.9\nq3 6  0.83  0.84  0.93  -0.22  3.5 2.0\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nq1 0.00 0.33 0.00 0.17 0.33 0.17    0\nq2 0.17 0.33 0.17 0.00 0.17 0.17    0\nq3 0.33 0.00 0.00 0.17 0.50 0.00    0\n\n\nThe function alpha() is clever enough to make us aware that something went wrong. It even tells us that Some items ( q2 ) were negatively correlated with the total scale and probably should be reversed. Also, the Cronbach’s \\(\\alpha\\) is now negative, which is another indicator that reversed items might be present in the dataset. To remedy this problem, we have to recode q2 to reflect the opposite meaning. Inverting the coding usually implies we have to mirror the scores, i.e. the largest value become the smallest one and vice versa. Since we use a 6-point Likert scale, we have to recode values in the following way:\n\n6 -&gt; 1\n5 -&gt; 2\n4 -&gt; 3\n3 -&gt; 4\n2 -&gt; 5\n1 -&gt; 6\n\nAs is so often the case in R, there is more than just one way of achieving this:\n\nCompute a new variable with mutate() only,\nApply recode() to the variable in question, or\nUse the psych package.\n\nI will introduce you to all three methods, because each comes with their own benefits and drawbacks. Thus, for me, it is very context-specific.\n\n7.8.4.1 Reversing items with mutate()\nLet us begin with the method that is my favourite because it is very quick to execute and only requires one function. With mutate(), we can calculate the reversed score for an item by applying the following formula:\\[score_{reversed} = score_{max} + score_{min} - score_{obs}\\]\nIn other words, the reversed score \\(score_{reversed}\\) of an item equals the theoretical maximum score on a given scale \\(score_{max}\\) plus the theoretical minimum score on this scale \\(score_{min}\\), and then we subtract the observed score \\(score_{obs}\\), i.e. the actual response by our participant. For example, if someone in our study scored a 5 on q2, we could compute the reversed score as follows: 6 + 1 - 5 = 2 because our Likert scale ranges from 1 to 6. In R, we can apply this idea to achieve this for multiple observations at once.\n\ndf_social_r &lt;-\n  df_social |&gt;\n  mutate(q2_r = 7 - q2)\n\ndf_social_r |&gt;\n  select(q2, q2_r)\n\n# A tibble: 6 × 2\n     q2  q2_r\n  &lt;dbl&gt; &lt;dbl&gt;\n1     2     5\n2     5     2\n3     1     6\n4     2     5\n5     3     4\n6     6     1\n\n\nThis technique is simple, flexible and easy to read and understand. We can change this formula to fit any scale length and make adjustments accordingly. I strongly recommend starting with this approach before opting for one of the other two.\n\n\n7.8.4.2 Reversing items with recode()\nAnother option to reverse items is recode(). Conceptually, recode() functions similarly to fct_recode(), only that it can handle numbers and factors. Therefore, we can apply a similar strategy as we did before with factors (see Chapter @ref(recoding-factors)).\n\ndf_social_r &lt;-\n  df_social |&gt;\n  mutate(q2_r = recode(q2,\n                       \"6\" = \"1\",\n                       \"5\" = \"2\",\n                       \"4\" = \"3\",\n                       \"3\" = \"4\",\n                       \"2\" = \"5\",\n                       \"1\" = \"6\")\n         )\n\ndf_social_r\n\n# A tibble: 6 × 5\n  name               q1    q2    q3 q2_r \n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 Agatha Harkness     6     2     5 5    \n2 Daren Cross         2     5     1 2    \n3 Hela                4     1     5 6    \n4 Aldrich Killian     5     2     4 5    \n5 Malekith            5     3     5 4    \n6 Ayesha              2     6     1 1    \n\n\nWhile this might seem convenient, it requires more coding and is, therefore, more prone to errors. If we had to do this for multiple items, this technique is not ideal. However, it is a great option whenever you need to change values to something entirely customised, e.g. every 6 needs to be come a 10 and every 3 needs to become a 6. However, such cases are rather rare in my experience.\nBe aware, that after using recode(), your variable becomes a chr, and you likely have to convert it back to a numeric data type. We can adjust the above code to achieve recoding and converting in one step.\n\ndf_social_r &lt;-\n  df_social |&gt;\n  mutate(q2_r = as.numeric(recode(q2,\n                                  \"6\" = \"1\",\n                                  \"5\" = \"2\",\n                                  \"4\" = \"3\",\n                                  \"3\" = \"4\",\n                                  \"2\" = \"5\",\n                                  \"1\" = \"6\"))\n  )\n\ndf_social_r\n\n# A tibble: 6 × 5\n  name               q1    q2    q3  q2_r\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Agatha Harkness     6     2     5     5\n2 Daren Cross         2     5     1     2\n3 Hela                4     1     5     6\n4 Aldrich Killian     5     2     4     5\n5 Malekith            5     3     5     4\n6 Ayesha              2     6     1     1\n\n\n\n\n7.8.4.3 Reversing items with the psych package\nThe last option I would like to share with you is with the psych package. This option can be helpful when reversing multiple items at once without having to create additional variables. The function reverse.code() takes a data set with items and requires the specification of a coding key, i.e. keys. This coding key indicates whether an item in this dataset should be reversed or not by using 1 for items that should not be reversed and -1 for items you wish to reverse.\n\nitems &lt;- df_social |&gt; select(-name)\n\ndf_social_r &lt;- psych::reverse.code(keys = c(1, -1, 1),\n                      items = items,\n                      mini = 1,\n                      maxi = 6)\n\ndf_social_r\n\n     q1 q2- q3\n[1,]  6   5  5\n[2,]  2   2  1\n[3,]  4   6  5\n[4,]  5   5  4\n[5,]  5   4  5\n[6,]  2   1  1\n\n\n\n\n7.8.4.4 A final remark about reversing item scores\nNo matter which method we used, we successfully converted the scores from q2 and stored them in a new variable q2_r or, in the case of the psych package, as q2-. We can now perform the internal consistency analysis again.\n\n# Cronbach's alpha after reverse coding q2\ndf_social_r |&gt;\n  select(-name, -q2) |&gt;\n  psych::alpha()\n\n\nReliability analysis   \nCall: psych::alpha(x = select(df_social_r, -name, -q2))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.95      0.95    0.95      0.87  21 0.033  3.8 1.8     0.91\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.80  0.95  0.99\nDuhachek  0.89  0.95  1.02\n\n Reliability if an item is dropped:\n     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nq1        0.95      0.95    0.91      0.91  21    0.037    NA  0.91\nq3        0.88      0.89    0.80      0.80   8    0.092    NA  0.80\nq2_r      0.94      0.95    0.91      0.91  20    0.042    NA  0.91\n\n Item statistics \n     n raw.r std.r r.cor r.drop mean  sd\nq1   6  0.94  0.94  0.91   0.87  4.0 1.7\nq3   6  0.98  0.98  0.98   0.96  3.5 2.0\nq2_r 6  0.95  0.95  0.91   0.88  3.8 1.9\n\nNon missing response frequency for each item\n        1    2    4    5    6 miss\nq1   0.00 0.33 0.17 0.33 0.17    0\nq3   0.33 0.00 0.17 0.50 0.00    0\nq2_r 0.17 0.17 0.17 0.33 0.17    0\n\n\nNow everything seems to make sense, and the internal consistency is excellent. We can assume that all three questions reflect one latent construct. From here, we can return to computing our latent variables (see Chapter @ref(computing-latent-variables)) and continue with our analysis.\nWhen working with datasets, especially those you have not generated yourself, it is always important to check whether any items are meant to be reverse coded. Missing this aspect will likely throw many surprising results and can completely ruin your analysis. I vividly remember my PhD supervisor’s story about a viva where a student forgot about reversing their scores. The panellists noticed that the correlations presented were counter-intuitive, and only then it was revealed that the entire analysis had to be redone. Tiny mistakes can often have an enormous ripple effect. Make sure you have ‘reversed items’ on your checklist when performing data cleaning and wrangling.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#sec-conclusion-data-wrangling",
    "href": "07_data_wrangling.html#sec-conclusion-data-wrangling",
    "title": "7  Data Wrangling",
    "section": "7.9 Once you finished with data wrangling",
    "text": "7.9 Once you finished with data wrangling\nOnce you finished your data cleaning, I recommend writing (i.e. exporting) your cleaned data out of R into your ‘01_tidy_data’ folder. You can then use this dataset to continue with your analysis. This way, you do not have to run all your data wrangling and cleaning code every time you open your R project. To write your tidy dataset onto your hard drive of choice, we can use readr in the same way as we did at the beginning to import data. However, instead of read_csv() we have to use another function that writes the file into a folder. The function write_csv() first takes the name of the object we want to save and then the folder and file name we want to save it to. We only made changes to the wvs dataset and after cleaning and dealing with missing data, we saved it in the object wvs_nona. So we should save it to our hard drive and name it, for example, wvs_nona.csv, because it is not only clean, but also has ‘no NA’, i.e. no missing data.\n\nwrite_csv(wvs_clean, \"01_tidy_data/wvs_nona.csv\")\n\nThis chapter has been a reasonably long one. Nonetheless, it only covered the basics of what can and should be done when preparing data for analysis. These steps should not be rushed or skipped. It is essential to have the data cleaned appropriately. This process helps you familiarise yourself with the dataset in great depth, and it makes you aware of limitations or even problems of your data. In the spirit of Open Science, using R also helps to document the steps you undertook to get from a raw dataset to a tidy one. This is also beneficial if you intend to publish your work later. Reproducibility has become an essential aspect of transparent and impactful research and should be a guiding principle of any empirical research.\nNow that we have learned the basics of data wrangling, we can finally start our data analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#exercises",
    "href": "07_data_wrangling.html#exercises",
    "title": "7  Data Wrangling",
    "section": "7.10 Exercises",
    "text": "7.10 Exercises\nIf you would like to test your knowledge or simply practice what we covered in this chapter you can copy and paste the following code if you have the r4np installed already.\n\n# Option 1:\nlearnr::run_tutorial(\"ex_data_wrangling\", package = \"r4np\")\n\nIf you have not yet installed the r4np package, you will have to do this first using by using this code chunk:\n\n# Option 2:\ndevtools::install_github(\"ddauber/r4np\")\nlearnr::run_tutorial(\"ex_data_wrangling\", package = \"r4np\")\n\n\n\n\n\nBuuren, Stef van. 2018. Flexible Imputation of Missing Data. CRC press.\n\n\nDong, Yiran, and Chao-Ying Joanne Peng. 2013. “Principled Missing Data Methods for Researchers.” SpringerPlus 2 (1): 1–17.\n\n\nGinkel, Joost R van, Marielle Linting, Ralph CA Rippe, and Anja van der Voort. 2020. “Rebutting Existing Misconceptions about Multiple Imputation as a Method for Handling Missing Data.” Journal of Personality Assessment 102 (3): 297–308.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nHenson, Robin K. 2001. “Understanding Internal Consistency Reliability Estimates: A Conceptual Primer on Coefficient Alpha.” Measurement and Evaluation in Counseling and Development 34 (3): 177–89.\n\n\nHu, Li-tze, and Peter M Bentler. 1999. “Cutoff Criteria for Fit Indexes in Covariance Structure Analysis: Conventional Criteria Versus New Alternatives.” Structural Equation Modeling: A Multidisciplinary Journal 6 (1): 1–55.\n\n\nJakobsen, Janus Christian, Christian Gluud, Jørn Wetterslev, and Per Winkel. 2017. “When and How Should Multiple Imputation Be Used for Handling Missing Data in Randomised Clinical Trials–a Practical Guide with Flowcharts.” BMC Medical Research Methodology 17 (1): 1–10.\n\n\nLittle, Roderick J. A. 1988. “A Test of Missing Completely at Random for Multivariate Data with Missing Values.” Journal of the American Statistical Association 83 (404): 1198–1202. https://doi.org/10.1080/01621459.1988.10478722.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. “Column Data Types.” https://tibble.tidyverse.org/articles/types.html.\n\n\nNunally, J. C. 1967. Psychometric Theory. New york: Mc Graw-Hill.\n\n\n———. 1978. Psychometric Theory (2nd Edition). New york: Mc Graw-Hill.\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92.\n\n\nSchafer, Joseph L. 1999. “Multiple Imputation: A Primer.” Statistical Methods in Medical Research 8 (1): 3–15.\n\n\nStobierski, Tim. 2021. “Data Wrangling: What It Is and Why It’s Important.” Harvard Business School Online. https://online.hbs.edu/blog/post/data-wrangling.\n\n\nWest, Stephen G, Aaron B Taylor, Wei Wu, et al. 2012. “Model Fit and Model Selection in Structural Equation Modeling.” Handbook of Structural Equation Modeling 1: 209–31.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23.\n\n\n———. 2021. The Tidyverse Style Guide. https://style.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07_data_wrangling.html#footnotes",
    "href": "07_data_wrangling.html#footnotes",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "A sophisticated research project would likely not rely on a dichotomous approach to gender, but appreciate the diversity in this regard.↩︎\nStill, I understand that it often feels annoying.↩︎\nTechnically, there are many more options, but most of them work similar to the demonstrated techniques and are equally less needed in most scenarios.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html",
    "href": "08_descriptive_statistics.html",
    "title": "8  Descriptive Statistics",
    "section": "",
    "text": "8.1 Plotting in R with ggplot2\nPlotting can appear intimidating at first but is very easy and quick once you understand the basics. The ggplot2 package is a very popular package to generate plots in R, and many other packages are built upon it. This makes it a very flexible tool to create almost any data visualisation you could imagine. If you want to see what is possible with ggplot2, you might want to consider looking at #tidytuesday on Twitter, where novices and veterans share their data visualisations every week.\nTo generate any plot, we need to define three components at least:\nAdmittedly, this is a harsh oversimplification, but it will serve as a helpful guide to get us started. The function ggplot() is the one responsible for creating any type of data visualisation. The generic structure of a ggplot() looks like this:\nIn other words, we first need to provide the dataset data, and then the aesthetics (aes()). Think of aes() as the place where we define our variables (i.e. x and y). For example, we might be interested to know which movie genre is the most popular among the top 250 IMDb movies. The dataset imdb_top_250 from the r4np package allows us to find an answer to this question. Therefore we define the components of the plot as follows:\nggplot(imdb_top_250, aes(x = genre_01))\nRunning this line of code will produce an empty plot. We only get labels for our x-axis since we defined it already. However, we have yet to tell ggplot() how we want to represent the data on this canvas. Your choice for how you want to plot your data is usually informed by the type of data you use and the statistics you want to represent. For example, plotting the mean of a factor is not meaningful, e.g. computing the mean of movie genres. On the other hand, we can count how often specific genres appear in our dataset. One way of representing a factor’s count (or frequency) is to use a bar plot. To add an element to ggplot(), i.e. bars, we use + and append the function geom_bar(), which draws bars. The + operator works similar to |&gt; and allows to chain multiple functions one after the other as part of a ggplot().\nggplot(imdb_top_250, aes(x = genre_01)) +\n  geom_bar()\nWith only two lines of coding, we created a great looking plot. We can see that Drama is by far the most popular genre, followed by Action and Crime. Thus, we successfully found an answer to our question. Still, there are more improvements necessary to use it in a publication.\nWe can use + to add other elements to our plot, such as a title and proper axes labels. Here are some common functions to further customise our plot:\nggplot(imdb_top_250, aes(x = genre_01)) +\n  geom_bar() +\n  ggtitle(\"Most popular movie genres\") +  # Add a title\n  xlab(\"movie genre\") +                   # Rename x-axis\n  ylab(\"frequency\")                       # Rename y-axis\nWhen working with factors, the category names can be rather long. In this plot, we have lots of categories, and the labels Adventure, Animation, and Biography are a bit too close to each other for my taste. This might be an excellent opportunity to use coord_flip(), which rotates the entire plot by 90 degrees, i.e. turning the x-axis into the y-axis and vice versa. This makes the labels much easier to read.\nggplot(imdb_top_250, aes(x = genre_01)) +\n  geom_bar() +\n  ggtitle(\"Most popular movie genres\") +\n  xlab(\"movie genre\") +\n  ylab(\"frequency\") +\n  coord_flip()\nOur plot is almost perfect, but we should take one more step to make reading and understanding this plot even easier. At the moment, the bars are ordered alphabetically by movie genre. Unfortunately, this is hardly ever a useful way to order your data. Instead, we might want to sort the data by frequency, showing the most popular genre at the top. To achieve this, we could either sort the movies by hand (see Chapter @ref(reordering-factor-levels)) or slightly amend what we have coded so far.\nThe problem you encounter when rearranging a geom_bar() with only one variable is that we do not have an explicit value to indicate how we want to sort the bars. Our current code is based on the fact that ggplot does the counting for us. So, instead, we need to do two things:\n# Step 1: The frequency table only\nimdb_top_250 |&gt;\n  count(genre_01)\n\n# A tibble: 11 × 2\n   genre_01      n\n   &lt;fct&gt;     &lt;int&gt;\n 1 Action       40\n 2 Adventure    20\n 3 Animation    24\n 4 Biography    22\n 5 Comedy       23\n 6 Crime        38\n 7 Drama        72\n 8 Film-Noir     1\n 9 Horror        3\n10 Mystery       4\n11 Western       3\n# Step 2: Plotting a barplot based on the frequency table\nimdb_top_250 |&gt;\n  count(genre_01) |&gt;\n  ggplot(aes(x = genre_01, y = n)) +\n\n  # Use geom_col() instead of geom_bar()\n  geom_col() +\n  \n  # Add titles for plot\n  ggtitle(\"Most popular movie genres\") +\n  xlab(\"movie genre\") +\n  ylab(\"frequency\") +\n  \n  # Rotate plot by 180 degrees\n  coord_flip()\n#Step 3: reorder() genre_01 by frequency, i.e. by 'n'\nimdb_top_250 |&gt;\n  count(genre_01) |&gt;\n  ggplot(aes(x = reorder(genre_01, n), y = n)) +  # Use 'reorder()'\n  geom_col() +\n  ggtitle(\"Most popular movie genres\") +\n  xlab(\"movie genre\") +\n  ylab(\"frequency\") +\n  coord_flip()\nStep 3 is the only code you need to create the desired plot. The other two steps only demonstrate how one can slowly build this plot, step-by-step. You might have noticed that I used dplyr to chain all these functions together (i.e. |&gt;), and therefore, it was not necessary to specify the dataset in ggplot().\nThere are also two new functions we had to use: geom_col() and reorder(). The function geom_col() performs the same step as geom_bar() which can be confusing. The easiest way to remember how to use them is: If you use a frequency table to create your barplot, use geom_col(), if not, use geom_bar(). The function geom_col() requires that we specify an a-axis and a y-axis (our frequency scores), while geom_bar() works with one variable.\nIn many cases, when creating plots, you have to perform two steps:\nNow you have learned the fundamentals of plotting in R. We will create a lot more plots throughout the next chapters. They all share the same basic structure, but we will use different geom_s to describe our data. By the end of this book, you will have accrued enough experience in plotting in R that it will feel like second nature. If you want to deepen your knowledge of ggplot, you should take a look at the book ‘ggplot: Elegant Graphics for Data Analysis’ or ‘R Graphics Cookbook’ which moves beyond ggplot2. Apart from that, you can also find fantastic packages which extend the range of ‘geoms’ you can use in the ‘ggplot2 extensions gallery’.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html#sec-plotting-in-r-with-ggplot2",
    "href": "08_descriptive_statistics.html#sec-plotting-in-r-with-ggplot2",
    "title": "8  Descriptive Statistics",
    "section": "",
    "text": "a dataset,\nvariables we want to plot, and\na function to indicate how we want to plot them, e.g. as lines, bars, points, etc.\n\n\n\nggplot(data, aes(x = variable_01, y = variable_02))\n\n\n\nour data is imdb_top_250, and\nour variable of interest is genre_01.\n\n\n\n\n\n\n\n\n\n\n\n\ncreate a table with all genres and their frequency, and\nuse this table to plot the genres by the frequency we computed\n\n\n\n\n\n\n\n\ngenerate the statistics you want to plot, e.g. a frequency table, and\nplot the data via ggplot()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html#sec-frequency",
    "href": "08_descriptive_statistics.html#sec-frequency",
    "title": "8  Descriptive Statistics",
    "section": "8.2 Frequencies and relative frequencies: Counting categorical variables",
    "text": "8.2 Frequencies and relative frequencies: Counting categorical variables\nOne of the most fundamental descriptive statistics includes frequencies, i.e. counting the number of occurrences of a factor level, string or numeric value. For example, we might be interested to know which director produced most movies listed in the IMDb Top 250. Therefore multiple movies might belong to the same value in our factor director. We already used the required function to compute such a frequency when we looked at movie genres (see Chapter @ref(plotting-in-r-with-ggplot2)), i.e. count().\n\nimdb_top_250 |&gt; count(director)\n\n# A tibble: 155 × 2\n   director                  n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 Aamir Khan                1\n 2 Adam Elliot               1\n 3 Akira Kurosawa            6\n 4 Alejandro G. Inarritu     1\n 5 Alfred Hitchcock          6\n 6 Andrei Tarkovsky          2\n 7 Andrew Stanton            2\n 8 Anthony Russo             2\n 9 Anurag Kashyap            1\n10 Asghar Farhadi            1\n# ℹ 145 more rows\n\n\nThe result indicates that 155 directors produced 250 movies. Thus, some movies have to be from the same director. As the frequency table shows, Akira Kurosawa directed 6 films, and Alfred Hitchcock also produced 6 movies. To keep the table more manageable, we can filter() our dataset and include only directors who made at least 6 movies because we know there are at least two already.\n\nimdb_top_250 |&gt;\n  count(director) |&gt;\n  filter(n &gt;= 6)\n\n# A tibble: 6 × 2\n  director              n\n  &lt;fct&gt;             &lt;int&gt;\n1 Akira Kurosawa        6\n2 Alfred Hitchcock      6\n3 Christopher Nolan     7\n4 Martin Scorsese       7\n5 Stanley Kubrick       7\n6 Steven Spielberg      6\n\n\nThere are in total 6 directors who produced several excellent movies, and if you are a bit of a movie fan, all these names will sound familiar.\nBesides absolute frequencies, i.e. the actual count of occurrences of a value, we sometimes wish to compute relative frequencies to make it easier to compare categories more easily. Relative frequencies are more commonly known as percentages which indicate the ratio or proportion of an occurrence relative to the total number of observations. Consider the following example, which reflects a study where participants were asked to rate a new movie after watching it.\n\nglimpse(movie_ratings)\n\nRows: 26\nColumns: 3\n$ participant &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\"…\n$ gender      &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"male\", \"female\", …\n$ rating      &lt;int&gt; 5, 8, 4, 8, 3, 4, 10, 5, 2, 8, 4, 3, 7, 9, 3, 6, 4, 8, 10,…\n\n\nIt might be interesting to know whether our movie ratings were mainly from male or female viewers. Thus, we can compute the absolute and relative frequencies at once by creating a new variable with mutate() and the formula to calculate percentages, i.e. n / sum(n).\n\nmovie_ratings |&gt;\n  count(gender) |&gt;\n  \n  # add the relative distributions, i.e. percentages\n  mutate(perc = n / sum(n))\n\n# A tibble: 2 × 3\n  gender     n  perc\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n1 female    22 0.846\n2 male       4 0.154\n\n\nIt seems we have a very uneven distribution of male and female participants. Thus, our analysis is affected by having more female participants. Consequently, any further computation we perform with this dataset would reflect the opinion of female viewers more than those of male ones. This is important to know because it limits the representativeness of our results. The insights gained from primarily female participants in this study will not be helpful when trying to understand male audiences. Therefore, understanding who the participants are in your study is essential before undertaking any further analysis. Sometimes, it might even imply that we have to change the focus of our study entirely.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html#sec-central-tendency",
    "href": "08_descriptive_statistics.html#sec-central-tendency",
    "title": "8  Descriptive Statistics",
    "section": "8.3 Central tendency measures: Mean, Median, Mode",
    "text": "8.3 Central tendency measures: Mean, Median, Mode\nThe mean, median and mode (the 3 Ms) are all measures of central tendency, i.e. they provide insights into how our data is distributed. Measures of central tendency help to provide insights into the most frequent/typical scores we can observe in the data for a given variable. All the 3Ms summarise our data/variable by a single score which can be helpful but sometimes also terribly misleading.\n\n8.3.1 Mean\nThe mean is likely the most known descriptive statistics and, at the same time, a very powerful and influential one. For example, the average ratings of restaurants on Google might influence our decision on where to eat out. Similarly, we might consider the average rating of movies to decide which one to watch in the cinema with friends. Thus, what we casually refer to as the ‘average’ is equivalent to the ‘mean’.\nIn R, it is simple to compute the mean using the function mean(). We used this function in Chapter @ref(functions) and Chapter @ref(latent-constructs) already. However, we have not looked at how we can plot the mean.\nAssume we are curious to know how successful movies are in each of the genres. The mean could be a good starting point to answer this question because it provides the ‘average’ success of movies in a particular genre. The simplest approach to investigating this is using a bar plot, like in Chapter @ref(plotting-in-r-with-ggplot2). So, we first create a tibble that contains the means of gross_in_m (i.e. the financial success fo a movie) for each genre in genre_01. Then we use this table to plot a bar plot with geom_col().\n\nimdb_top_250 |&gt;\n  \n  # Group data by genre\n  group_by(genre_01) |&gt;\n  \n  # Compute the mean for each group (remove NAs via na.rm = TRUE)\n  summarise(mean_earnings_in_m = mean(gross_in_m, na.rm = TRUE)) |&gt;\n  \n  # Create the plot\n  ggplot(aes(x = reorder(genre_01, mean_earnings_in_m), y = mean_earnings_in_m)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nYou will have noticed that we use the function summarise() instead of mutate(). The summarise() function is a special version of mutate(). While mutate() returns a value for each row, summarise condenses our dataset, e.g. turning each row of observations into scores for each genre. Here is an example of a simple dataset which illustrates the difference between these two functions.\n\n# Create a simple dataset from scratch with tibble()\n(data &lt;- tibble(number = c(1, 2, 3, 4, 5),\n               group = factor(c(\"A\", \"B\", \"A\", \"B\", \"A\"))))\n\n# A tibble: 5 × 2\n  number group\n   &lt;dbl&gt; &lt;fct&gt;\n1      1 A    \n2      2 B    \n3      3 A    \n4      4 B    \n5      5 A    \n\n\n\n# Return the group value for each observation/row\ndata |&gt;\n  group_by(group) |&gt;\n  mutate(sum = sum(number))\n\n# A tibble: 5 × 3\n# Groups:   group [2]\n  number group   sum\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1      1 A         9\n2      2 B         6\n3      3 A         9\n4      4 B         6\n5      5 A         9\n\n\n\n# Returns the group value once for each group\ndata |&gt;\n  group_by(group) |&gt;\n  summarise(sum = sum(number))\n\n# A tibble: 2 × 2\n  group   sum\n  &lt;fct&gt; &lt;dbl&gt;\n1 A         9\n2 B         6\n\n\nConsidering the results from our plot, it appears as if Action and Animation are far ahead of the rest. On average, both make around 200 million per movie. Adventure ranks third with only approximately 70 million. We can retrieve the exact earnings by removing the plot from the above code.\n\nimdb_top_250 |&gt;\n  group_by(genre_01) |&gt;\n  summarise(mean_earnings_in_m = mean(gross_in_m, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_earnings_in_m))\n\n# A tibble: 11 × 2\n   genre_01  mean_earnings_in_m\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 Action                203.  \n 2 Animation             189.  \n 3 Adventure              73.9 \n 4 Biography              57.9 \n 5 Drama                  52.9 \n 6 Crime                  49.6 \n 7 Mystery                48.4 \n 8 Horror                 41.6 \n 9 Comedy                 33.2 \n10 Western                 8.81\n11 Film-Noir               0.45\n\n\nIn the last line, I used a new function called arrange(). It allows us to sort rows in our dataset by a specified variable (i.e. a column). By default, arrange() sorts values in ascending order, putting the top genre last. Therefore, we have to use another function to change the order to descending with desc() to see the top-genre listed first. The function arrange() works similarly to reorder() but is used for sorting variables in data frames and less so for plots.\nBased on this result, we might believe that Action and Animation movies are the most successful genres. However, we have not taken into account how many movies there are in each genre. Consider the following example:\n\n# Assume there are 2 Action movies in the top 250\n# both of which earn 203 million\n2 * 203\n\n[1] 406\n\n\n\n# Assume there are 10 Drama movies in the top 25\n# each of which earns 53 million\n10 * 53\n\n[1] 530\n\n\nThus, the ‘mean’ alone might not be a good indicator. It can tell us which genre is most successful based on a single movie, but we also should consider how many movies there are in each genre.\nLet’s add the number of movies (n) in each genre to our table. We can achieve this by using the function n(). Of course, we could also use the function count() first and then summarise() the earnings. There is hardly ever one single way to achieve the same result. This time it is truly a matter of personal taste.\n\nimdb_top_250 |&gt;\n  group_by(genre_01) |&gt;\n  summarise(mean_earnings_in_m = mean(gross_in_m, na.rm = TRUE),\n            n = n()) |&gt;\n  arrange(desc(mean_earnings_in_m))\n\n# A tibble: 11 × 3\n   genre_01  mean_earnings_in_m     n\n   &lt;fct&gt;                  &lt;dbl&gt; &lt;int&gt;\n 1 Action                203.      40\n 2 Animation             189.      24\n 3 Adventure              73.9     20\n 4 Biography              57.9     22\n 5 Drama                  52.9     72\n 6 Crime                  49.6     38\n 7 Mystery                48.4      4\n 8 Horror                 41.6      3\n 9 Comedy                 33.2     23\n10 Western                 8.81     3\n11 Film-Noir               0.45     1\n\n\nOur interpretation might slightly change based on these findings. There are considerably more movies in the genre Drama than in Action or Animation. We already plotted the frequency of movies per genre in Chapter @ref(plotting-in-r-with-ggplot2). Accordingly, we would have to think that Drama turns out to be the most successful genre if we consider the number of movies listed in the IMDb top 250.\nAs a final step, we can plot the sum of all earnings per genre as yet another indicator for the ‘most successful genre’.\n\nimdb_top_250 |&gt;\n  filter(!is.na(gross_in_m)) |&gt;\n  group_by(genre_01) |&gt;\n  summarise(sum_gross_in_m = sum(gross_in_m)) |&gt;\n  ggplot(aes(x = genre_01, y = sum_gross_in_m)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThese results confirm that the Action genre made the most money out of all genres covered by the top 250 IMDb movies. I am sure you are curious to know which action movie contributed the most to this result. We can achieve this easily by drawing on functions we already know.\n\nimdb_top_250 |&gt;\n  select(title, genre_01, gross_in_m) |&gt;\n  filter(genre_01 == \"Action\") |&gt;\n  slice_max(order_by = gross_in_m,\n            n = 5)\n\n# A tibble: 5 × 3\n  title                  genre_01 gross_in_m\n  &lt;chr&gt;                  &lt;fct&gt;         &lt;dbl&gt;\n1 Avengers: Endgame      Action         858.\n2 Avengers: Infinity War Action         679.\n3 The Dark Knight        Action         535.\n4 The Dark Knight Rises  Action         448.\n5 Jurassic Park          Action         402.\n\n\nAvengers: Endgame and Avengers: Infinity war rank the highest out of all Action movies. Two incredible movies if you are into Marvel comics.\nIn the last line, I sneaked in another new function from dplyr called slice_max(). This function allows us to pick the top 5 movies in our data. So, if you have many rows in your data frame (remember there are 40 action movies), you might want to be ‘picky’ and report only the top 3, 4 or 5. As you can see, slice_max() requires at least to arguments: order_by which defines the variable your dataset should be sorted by, and n which defines how many rows should be selected, e.g. n = 3 for the top 3 movies or n = 10 for the Top 10 movies. If you want to pick the lowest observations in your dataframe, you can use slice_min(). There are several other ‘slicing’ functions that can be useful and can be found on the corresponding dplyr website.\nIn conclusion, the mean helps understand how each movie, on average, performed across different genres. However, the mean alone provides a somewhat incomplete picture. Thus, we need to always look at means in the context of other information to gain a more comprehensive insight into our data.\n\n\n8.3.2 Median\nThe ‘median’ is the little, for some, lesser-known and used brother of the ‘mean’. However, it can be a powerful indicator for central tendency because it is not so much affected by outliers. With outliers, I mean observations that lie way beyond or below the average observation in our dataset. Let’s inspect the Action genre more closely and see how each movie in this category performed relative to each other. We first filter() our dataset to only show movies in the genre Action and also remove responses that have no value for gross_in_m using the opposite (i.e. !) of the function is.na().\n\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" & !is.na(gross_in_m)) |&gt;\n  ggplot(aes(x = reorder(title, gross_in_m),\n             y = gross_in_m)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nAvengers: Endgame and Avengers: Infinity War are far ahead of any other movie. We can compute the mean with and without these two movies by adding the titles of the movies as a filter criterion.\n\n# Mean earnings for Action genre with Avengers movies\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\") |&gt;\n  summarise(mean = mean(gross_in_m, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1  203.\n\n\n\n# Mean earnings for Action genre without Avengers movies\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" &\n           title != \"Avengers: Endgame\" &\n           title != \"Avengers: Infinity War\") |&gt;\n  summarise(mean = mean(gross_in_m, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1  170.\n\n\nThe result is striking. Without these two movies, the Action genre would have fewer earnings per movie than the Animation genre. However, if we computed the median instead, we would not notice such a massive difference in results. This is because the median sorts a dataset, e.g. by gross_in_m and then picks the value that would cut the data into two equally large halves. It does not matter which value is the highest or lowest in our dataset. What matters is the value that is ranked right in the middle of all values. The median splits your dataset into two equally large datasets.\n\n# Median earnings for Action genre with Avengers movies\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\") |&gt;\n  summarise(median = median(gross_in_m, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1   180.\n\n\n\n# Median earnings for Action genre with Avengers movies\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" &\n           title != \"Avengers: Endgame\" &\n           title != \"Avengers: Infinity War\") |&gt;\n  summarise(median = median(gross_in_m, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1   163.\n\n\nBoth medians are much closer to each other, showing how much better suited the median is in our case. In general, when we report means, it is advisable to report the median as well. If a mean and median differ substantially, it could imply that your data ‘suffers’ from outliers. So we have to detect them and think about whether we should remove them for further analysis (see Chapter @ref(dealing-with-outliers)).\nWe can visualise medians using boxplots. Boxplots are a very effective tool to show how your data is distributed in one single data visualisation, and it offers more than just the mean. It also shows the spread of your data (see Chapter @ref(spread-of-data)).\n\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" & !is.na(gross_in_m)) |&gt;\n  ggplot(aes(gross_in_m)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTo interpret this boxplot consider Figure @ref(fig:anatomy-of-a-boxplot). Every boxplot consists of a ‘box’ and two whiskers (i.e. the lines leading away from the box). The distribution of data can be assessed by looking at different ranges:\n\nMINIMUM to Q1 represents the bottom 25% of our observations,\nQ1 to Q3, also known as the interquartile range (IQR) defines the middle 50% of our observations, and\nQ3 to MAXIMUM, contains the top 25% of all our data.\n\nThus, with the help of a boxplot, we can assess whether our data is evenly distributed across these ranges or not. If observations fall outside a certain range (i.e. 1.5 IQR) they are classified as outliers. We cover outliers in great detail in Chapter @ref(dealing-with-outliers).\n\n\n\n\n\n\nFigure 8.1: Anatomy of a Boxplot\n\n\n\nConsidering our boxplot, it shows that we have one observation that is an outlier, which is likely Avengers: Endgame, but what happened to the other Avengers movie? Is it not an outlier as well? It seems we need some more information. We can overlay another geom_ on top to visualise where precisely each movie lies on this boxplot. We can represent each movie as a point by using geom_point(). This function requires us to define the values for the x and y-axis. Here it makes sense to set y = 0, which aligns all the dots in the middle of the boxplot.\n\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" & !is.na(gross_in_m)) |&gt;\n  ggplot(aes(gross_in_m)) +\n  geom_boxplot() +\n  geom_point(aes(y = 0, col = \"red\"),\n             show.legend = FALSE)\n\n\n\n\n\n\n\n\nTo make the dots stand out more, I changed the colour to \"red\". By adding the col attributed (color and colour also work), ggplot2 would automatically generate a legend. Since we do not need it, we can specify it directly in the geom_point() function. If you prefer the legend, remove show.legend = FALSE.\nThe two dots on the right are the two Avengers movies. This plot also nicely demonstrates why boxplots are so popular and helpful: They provide so many insights, not only into the central tendency of a variable, but also highlight outliers and, more generally, give a sense of the spread of our data (more about this in Chapter @ref(spread-of-data)).\nThe median is an important descriptive and diagnostic statistic and should be included in most empirical quantitative studies.\n\n\n8.3.3 Mode\nFinally, the ‘mode’ indicates which value is the most frequently occurring value for a specific variable. For example, we might be interested in knowing which IMDb rating was most frequently awarded to the top 250 movies.\nWhen trying to compute the mode in R, we quickly run into a problem because there is no function available to do this straight away unless you search for a package that does it for you. However, before you start searching, let’s reflect on what the mode does and why we can find the mode without additional packages or functions. The mode is based on the frequency of the occurrence of a value. Thus, the most frequently occurring value would be the one that is listed at the top of a frequency table. We have already created several frequency tables in this book, and we can create another one to find the answer to our question.\n\nimdb_top_250 |&gt; count(imdb_rating)\n\n# A tibble: 12 × 2\n   imdb_rating     n\n         &lt;dbl&gt; &lt;int&gt;\n 1         8.1    80\n 2         8.2    45\n 3         8.3    42\n 4         8.4    29\n 5         8.5    22\n 6         8.6    14\n 7         8.7     5\n 8         8.8     5\n 9         8.9     3\n10         9       3\n11         9.2     1\n12         9.3     1\n\n\nWe can also easily visualise this frequency table in the same way as before. To make the plot a bit more ‘fancy’, we can add labels to the bar which reflect the frequency of the rating. We need to add the attribute label and also add a geom_text() layer to display them. Because the numbers would overlap with the bars, I ‘nudge’ the labels up by 4 units on the y-axis. Finally, because we treat imdb_rating as categories, we should visualise them as_factor()s.\n\nimdb_top_250 |&gt;\n  count(imdb_rating) |&gt;\n  ggplot(aes(x = as_factor(imdb_rating),\n             y = n,\n             label = n)) +\n  geom_col() +\n  geom_text(nudge_y = 4)\n\n\n\n\n\n\n\n\nThe frequency table and the plot reveal that the mode for imdb_rating is 8.1. In addition, we also get to know how often this rating was applied, i.e. 80 times. This provides much more information than receiving a single score and helps better interpret the importance of the mode as an indicator for a central tendency. Consequently, there is generally no need to compute the mode if you can have a frequency table instead. Still, if you are keen to have a function that computes the mode, you will have to write your own function, e.g. as shown in this post on stackeroverflow.com or search for a package that coded one already.\nAs a final remark, it is also possible that you can find two or more modes in your data. For example, if the rating 8.1 appears 80 times and the rating 9.0 appears 80 times, both would be considered a mode.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html#sec-spread-of-data",
    "href": "08_descriptive_statistics.html#sec-spread-of-data",
    "title": "8  Descriptive Statistics",
    "section": "8.4 Indicators and visualisations to examine the spread of data",
    "text": "8.4 Indicators and visualisations to examine the spread of data\nUnderstanding how your data is spread out is essential to get a better sense of what your data is composed of. We already touched upon the notion of spread in Chapter @ref(median) through plotting a boxplot. Furthermore, the spread of data provides insights into how homogeneous or heterogeneous our participants’ responses are. The following will cover some essential techniques to investigate the spread of your data and investigate whether our variables are normally distributed, which is often a vital assumption for specific analytical methods (see Chapter @ref(sources-of-bias). In addition, we will aim to identify outliers that could be detrimental to subsequent analysis and significantly affect our modelling and testing in later stages.\n\n8.4.1 Boxplot: So much information in just one box\nThe boxplot is a staple in visualising descriptive statistics. It offers so much information in just a single plot that it might not take much to convince you that it has become a very popular way to show the spread of data.\nFor example, we might be interested to know how long most movies run. Our gut feeling might tell us that most movies are probably around two hours long. One approach to finding out is a boxplot, which we used before.\n\n# Text for annotations\nlongest_movie &lt;-\n  imdb_top_250 |&gt;\n  filter(runtime_min == max(runtime_min)) |&gt;\n  select(title)\n\nshortest_movie &lt;-\n  imdb_top_250 |&gt;\n  filter(runtime_min == min(runtime_min)) |&gt;\n  select(title)\n\n# Create the plot\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_boxplot() +\n  annotate(\"text\",\n           label = glue::glue(\"{longest_movie}\n                              ({max(imdb_top_250$runtime_min)} min)\"),\n           x = 310,\n           y = 0.05,\n           size = 2.5) +\n  annotate(\"text\",\n           label = glue::glue(\"{shortest_movie}\n                              ({min(imdb_top_250$runtime_min)} min)\"),\n           x = 45,\n           y = 0.05,\n           size = 2.5)\n\n\n\n\nA boxplot\n\n\n\n\nThe results indicate that most movies are between 100 to 150 minutes long. Our intuition was correct. We find that one movie is even over 300 minutes long, i.e. over 5 hours: Gangs of Wasseypur. In contrast, the shortest movie only lasts 45 minutes and is called Sherlock Jr.. I added annotations using annotate() to highlight these two movies in the plot. A very useful package for annotations is glue, which allows combining text with data to label your plots. So, instead of looking up the longest and shortest movie, I used the filter() and select() functions to find them automatically. This has the great advantage that if I wanted to update my data, the name of the longest movie might change. However, I do not have to look it up again by hand.\nAs we can see from our visualisation, both movies would also count as outliers in our dataset (see Chapter @ref(outliers-iqr) for more information on the computation of such outliers).\n\n\n8.4.2 Histogram: Do not mistake it as a bar plot\nAnother frequently used approach to show the spread (or distribution) of data is the histogram. The histogram easily gets confused with a bar plot. However, you would be very mistaken to assume that they are the same. Some key differences between these two types of plots is summarised in Table @ref(tab:histogram-vs-bar-plot).\n\n(#tab:histogram-vs-bar-plot) Histogram vs bar plot\n\n\n\n\n\n\nHistogram\nBar plot\n\n\n\n\nUsed to show the distribution of non-categorical data\nUsed for showing the frequency of categorical data, i.e. factors\n\n\nEach bar (also called ‘bin’) represents a group of observations.\nEach bar represents one category (or level) in our factor.\n\n\nThe order of the bars is important and cannot/should not be changed.\nThe order of bars is arbitrary and can be reordered if meaningful.\n\n\n\nLet’s overlap a bar plot with a histogram for the same variable to make this difference even more apparent.\n\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram() +\n  geom_bar(aes(fill = \"red\"), show.legend = FALSE)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere are a couple of important observations to be made:\n\nThe bar plot has much shorter bars because each bar represents the frequency of a single unique score. Thus, the runtime of 151 is represented as a bar, as is the runtime of 152. Only identical observations are grouped together. As such, the bar plot is based on a frequency table, similar to what we computed before.\nIn contrast, the histogram’s ‘bars’ are higher because they group together individual observations based on a specified range, e.g. one bar might represent movies that have a runtime between 150-170 minutes. These ranges are called bins.\n\nWe can control the number of bars in our histogram using the bins attribute. ggplot even reminds us in a warning that we should adjust it to represent more details in the plot. Let’s experiment with this setting to see how it would look like with different numbers of bins.\n\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram(bins = 5) +\n  geom_bar(aes(fill = \"red\"), show.legend = FALSE)\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram(bins = 20) +\n  geom_bar(aes(fill = \"red\"), show.legend = FALSE)\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram(bins = 60) +\n  geom_bar(aes(fill = \"red\"), show.legend = FALSE)\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram(bins = 300) +\n  geom_bar(aes(fill = \"red\"), show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\nAs becomes evident, if we select a large enough number of bins, we can achieve the same result as a bar plot. This is the closest a bar plot can become to a histogram, i.e. if you define the number of bins so that each observation is captured by one bin. While theoretically possible, practically, this rarely makes much sense.\nWe use histograms to judge whether our data is normally distributed. A normal distribution is often a requirement for assessing whether we can run certain types of analyses or not (for more details, see Chapter @ref(normality)). In short: It is imperative to know about it in advance. The shape of a normal distribution looks like a bell (see Figure @ref(fig:normal-distribution)). If our data is equal to a normal distribution, we find that\n\nthe mean and the median are the same value and we can conclude that\nthe mean is a good representation for our data/variable.\n\n\n\n\n\n\nA normal distribution\n\n\n\n\nLet’s see whether our data is normally distributed using the histogram we already plotted and overlay a normal distribution. The coding for the normal distribution is a little more advanced. Do not worry if you cannot fully decipher its meaning just yet. To draw such a reference plot, we need to:\n\ncompute the mean() of our variable,\ncalculate the standard deviation sd() of our variable (see Chapter @ref(standard-deviation)),\nuse the function geom_func() to plot it and,\ndefine the function fun as dnorm, which stands for ‘normal distribution’.\n\nIt is more important to understand what the aim of this task is, rather than fully comprehending the computational side in R: we try to compare our distribution with a normal one.\n\nimdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_histogram(aes(y = ..density..), bins = 30) +\n\n  # The following part creates a normal curve based on\n  # the mean and standard deviation of our data\n\n  geom_function(fun = dnorm,\n                n = 103,\n                args = list(mean = mean(imdb_top_250$runtime_min),\n                            sd = sd(imdb_top_250$runtime_min)),\n                col = \"red\")\n\n\n\n\n\n\n\n\nHowever, there are two problems if we use histograms in combination with a normal distribution reference plot: First, the y-axis needs to be transformed to fit the normal distribution (which is a density plot and not a histogram). Second, the shape of our histogram is affected by the number of bins we have chosen, which is an arbitrary choice we make. Besides, the lines of code might be tough to understand because we have to ‘hack’ the visualisation to make it work, i.e. using aes(y = ..density..). There is, however, a better way to do this: Density plots.\n\n\n8.4.3 Density plots: Your smooth histograms\nDensity plots are a special form of the histogram. It uses ‘kernel smoothing’, which turns our blocks into a smoother shape. Better than trying to explain what it does, it might help to see it. We use the same data but replace geom_histogram() with geom_density(). I also saved the plot with the normal distribution in a separate object, so we can easily reuse throughout this chapter.\n\n# Ingredients for our normality reference plot\nmean_ref &lt;- mean(imdb_top_250$runtime_min)\nsd_ref &lt;- sd(imdb_top_250$runtime_min)\n\n# Create a plot with our reference normal distribution\nn_plot &lt;-\n  imdb_top_250 |&gt;\n  ggplot(aes(runtime_min)) +\n  geom_function(fun = dnorm,\n                n = 103,\n                args = list(mean = mean_ref,\n                            sd = sd_ref),\n                col = \"red\")\n\n# Add our density plot\nn_plot +\n  geom_density()\n\n\n\n\nDensity plot vs normal distribution\n\n\n\n\nWe can also define how big the bins are for density plots, which make the plot more or less smooth. After all, the density plot is a histogram, but the transitions from one bin to the next are ‘smoothed’. Here is an example of how different bw settings affect the plot.\n\n# bw = 18\nn_plot +\n  geom_density(bw = 18) +\n  ggtitle(\"bw = 18\")\n\n\n\n\n\n\n\n\n\n# bw = 3\nn_plot +\n  geom_density(bw = 3) +\n  ggtitle(\"bw = 18\")\n\n\n\n\n\n\n\n\nThe benefits of the density plot in this situation are obvious: It is much easier to see whether our data is normally distributed or not when compared to a reference plot. However, we still might struggle to determine normality just by these plots because it depends on how high or low we set bw.\n\n\n8.4.4 Violin plot: Your smooth boxplot\nIf a density plot is the sibling of a histogram, the violin plot would be the sibling of a boxplot, but the twin of a density plot. Confused? If so, then let’s use the function geom_volin() to create one.\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = runtime_min, y = 0)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nLooking at our plot, it becomes evident where the violin plot got its name from, i.e. its shape. The reason why it is also a twin of the density plot becomes clear when we only plot half of the violin with geom_violinghalf() from the see package.\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = 0, y = runtime_min)) +\n  see::geom_violinhalf() +\n  coord_flip()\n\n\n\n\n\n\n\n\nIt looks exactly like the density plot we plotted earlier (see Figure @ref(fig:density-vs-normal)). The interpretation largely remains the same to a density plot as well. The relationship between the boxplot and the violin plot lies in the symmetry of the violin plot.\nAt this point, it is fair to say that we enter ‘fashion’ territory. It is really up to your taste which visualisation you prefer because they are largely similar but offer nuances that some data sets might require. Each visualisation, though, comes with caveats, which results in the emergence of new plot types. For example, ‘sina’ plots which address the following problem: A boxplot by itself will never be able to show bimodal distributions like a density plot. On the other hand, a density plot can show observations that do not exist, because they are smoothed. The package ggforce enables us to draw a ‘sina’ plot which combines a violin plot with a dot plot. Here is an example of overlaying a sina plot (black dots) and a violin plot (faded blue violin plot).\n\nimdb_top_250 |&gt;\n  ggplot(aes(y = runtime_min, x = 0)) +\n  geom_violin(alpha = 0.5, col = \"#395F80\", fill = \"#C0E2FF\") +\n  ggforce::geom_sina() +\n  coord_flip()\n\n\n\n\n\n\n\n\nLastly, I cannot finish this chapter without sharing with you one of the most popular uses of half-violin plots: The rain cloud plot. It combines a dot plot with a density plot, and each dot represents a movie in our dataset. This creates the appearance of a cloud with raindrops. There are several packages available that can make such a plot. Here I used the see package.\n\nimdb_top_250 |&gt;\n  filter(genre_01 == \"Action\" | genre_01 == \"Drama\") |&gt;\n  ggplot(aes(x = genre_01, y = imdb_rating, fill = genre_01)) +\n  see::geom_violindot(fill_dots = \"blue\", size_dots = 0.2) +\n  see::theme_modern() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n8.4.5 QQ plot: A ‘cute’ plot to check for normality in your data\nThe QQ plot is an alternative to comparing distributions to a normality curve. Instead of a curve, we plot a line that represents our data’s quantiles against the quantiles of a normal distribution. The term ‘quantile’ can be somewhat confusing, especially after learning about the boxplot, which shows quartiles. However, there is a relationship between these terms. Consider the following comparison:\n\n\\(1^{st}\\) Quartile = \\(25^{th}\\) percentile = 0.25 quantile\n\\(2^{nd}\\) Quartile = \\(50^{th}\\) percentile = 0.50 quantile = median\n\\(3^{rd}\\) Quartile = \\(75^{th}\\) percentile = 0.75 quantile\n\nWith these definitions out of the way, let’s plot some quantiles against each other.\n\nimdb_top_250 |&gt;\n  ggplot(aes(sample = runtime_min)) +\n  geom_qq() +\n  geom_qq_line(col = \"red\") +\n  annotate(\"text\",\n           label = \"The 5h-long movie\",\n           x = 2.5,\n           y = 308,\n           size = 3)\n\n\n\n\n\n\n\n\nThe function geom_qq() creates the dots, while geom_qq_line establishes a reference for a normal distribution. The reference line is drawn in such a way that it touches the quartiles of our distribution. Ideally, we would want that all dotes are firmly aligned with each other. Unfortunately, this is not the case in our dataset. At the top and the bottom, we have points that deviate quite far from a normal distribution. Remember the five-hour-long movie? It is very far away from the rest of the other movies in our dataset.\n\n\n8.4.6 Standard deviation: Your average deviation from the mean\nI left the most commonly reported statistics for the spread of data last. The main reason for this is that one might quickly jump ahead to look at the standard deviation without ever considering plotting the distribution of variables in the first place. Similar to the mean and other numeric indicators, they could potentially convey the wrong impression. Nevertheless, the standard deviation is an important measure.\nTo understand what the standard deviation is, we can consider the following visualisation:\n\nruntime_mean &lt;- mean(imdb_top_250$runtime_min)\n\nimdb_top_250 |&gt;\n  select(title, runtime_min) |&gt;\n  ggplot(aes(x = title, y = runtime_min)) +\n  geom_point() +\n  geom_hline(aes(yintercept = runtime_mean, col = \"red\"), show.legend = FALSE) +\n\n  # Making the plot a bit more pretty\n  theme(axis.text.x = element_blank(),         # Removes movie titles\n        panel.grid.major = element_blank(),    # Removes grid lines\n        panel.background = element_blank()     # Turns background white\n        ) +\n  ylab(\"runtime\") +\n  xlab(\"movies\")\n\n\n\n\n\n\n\n\nThe red line (created with geom_hline() represents the mean runtime for all movies in the dataset, which is 129 minutes. We notice that the points are falling above and below the mean, but not directly on it. In other words, there are not many movies that are about 129 minutes long. We make this visualisation even more meaningful if we sorted the movies by their runtime. We can also change the shape of the dots (see also Chapter @ref(correlations)) by using the attribute shape. This helps to plot many dots without having them overlap.\n\nimdb_top_250 |&gt;\n  select(title, runtime_min) |&gt;\n  ggplot(aes(x = reorder(title, runtime_min), y = runtime_min)) +\n  geom_point(shape = 124) +\n  geom_hline(aes(yintercept = runtime_mean, col = \"red\"), show.legend = FALSE) +\n\n  theme(axis.text.x = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.background = element_blank()\n        ) +\n  ylab(\"runtime\") +\n  xlab(\"movies\")\n\n\n\n\nDeviation of runtime_min from the mean run time of movies.\n\n\n\n\nAs we can see, only a tiny fraction of movies are close to the mean. If we now consider the distance of each observation from the red line, we know how much each of them, i.e. each movie, deviates from the mean. The standard deviation tells us how much the runtime deviates on average. To be more specific, to compute the standard deviation by hand, you would:\n\ncompute the difference between each observed value of runtime_min and the mean of runtime_min in your dataset, which is also called ‘deviance’, i.e. \\(deviance = runtime\\_min - mean_{all\\ movies}\\)\nsquare the deviance to turn all scores into positive ones, i.e. \\(deviance_{squared} = (deviance)^2\\),\nthen we take the sum of all deviations (also known as ‘sum of squared errors’) and divide it by the number of movies minus 1, which results in the ‘variance’, i.e. \\(variance = \\frac{\\sum(deviations_{squared})}{250-1}\\). The variance reflects the average dispersion of our data.\nLastly, we take the square root of this score to obtain the standard deviation, i.e. \\(sd = \\sqrt{variance}\\).\n\nWhile the deviation and variance are interesting to look at, the standard deviation has the advantage that it provides us with the average deviation (also called ‘error’) based on the units of measurement of our variable. Thus, it is much easier to interpret its size.\nWe could compute this by hand if we wanted, but it is much simpler to use the function sd() to achieve the same.\n\nsd(imdb_top_250$runtime_min)\n\n[1] 32.63701\n\n\nThe result shows that movies tend to be about 32 minutes longer or shorter than the average movie. This seems quite long. However, we must be aware that this score is also influenced by the outliers we detected before. As such, if standard deviations in your data appear quite large, it can be due to outliers, and you should investigate further. Plotting your data will undoubtedly help to diagnose any outliers.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "08_descriptive_statistics.html#sec-packages-for-descriptive-statistics",
    "href": "08_descriptive_statistics.html#sec-packages-for-descriptive-statistics",
    "title": "8  Descriptive Statistics",
    "section": "8.5 Packages to compute descriptive statistics",
    "text": "8.5 Packages to compute descriptive statistics\nThere is no one right way of how you can approach the computation of descriptive statistics. There are only differences concerning convenience and whether you have intentions to use the output from descriptive statistics to plot them or use them in other ways. Often, we only want to take a quick look at our data to understand it better. If this is the case, I would like to introduce you to two packages that are essential in computing descriptive statistics and constitute an excellent starting point to understanding your data:\n\npsych\nskimr (see also Chapter @ref(inspecting-raw-data))\n\n\n8.5.1 The psych package for descriptive statistics\nAs its name indicates, the psych package is strongly influenced by how research is conducted in the field of Psychology. The package is useful in many respects, and we already used it in Chapter @ref(latent-constructs) to compute Cronbach’s \\(\\alpha\\).\nAnother useful function is describe(). We can use this function to compute summary statistics for a variable. For example, we might wish to inspect the descriptive statistics for runtime_min.\n\npsych::describe(imdb_top_250$runtime_min)\n\n   vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 250 129.37 32.64    126  126.45 28.17  45 321   276 1.36     4.66 2.06\n\n\nIf we want to see descriptive statistics per group, we use describeBy(), which takes a factor as a second argument. So, for example, we could compute the descriptive statistics for runtime_min separately for each genre_01.\n\ndescriptives &lt;- psych::describeBy(imdb_top_250$runtime_min,\n                  imdb_top_250$genre_01)\n\n# The output for the first three genres\ndescriptives[1:3]\n\n$Action\n   vars  n   mean   sd median trimmed   mad min max range skew kurtosis  se\nX1    1 40 141.98 42.4  136.5  138.75 24.46  45 321   276  1.6     6.21 6.7\n\n$Adventure\n   vars  n   mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 20 141.85 39.09    142  138.56 39.29  89 228   139 0.46    -0.62 8.74\n\n$Animation\n   vars  n   mean    sd median trimmed  mad min max range skew kurtosis se\nX1    1 24 102.96 14.67     99     102 12.6  81 134    53 0.57    -0.85  3\n\n\nThe advantage of using psych is the convenience of retrieving several descriptive statistics with just one function instead of chaining together several functions to achieve the same.\n\n\n8.5.2 The skimr package for descriptive statistics\nWhile the psych package returns descriptive statistics for a single numeric variable, skimr takes this idea further and allows us to generate descriptive statistics for all variables of all types in a data frame with just one function, i.e. skim(). We already covered this package in Chapter @ref(inspecting-raw-data), but here is a brief reminder of its use for descriptive statistics.\n\nskimr::skim(imdb_top_250)\n\n\nIn addition to providing descriptive statistics, skim() sorts the variables by data type and provides different descriptive statistics meaningful to each kind. Apart from that, the column n_missing can help spot missing data. Lastly, we also find a histogram in the last column for numeric data, which I find particularly useful.\nWhen just starting to work on a new dataset, the psych and the skimr package are solid starting points to get an overview of the main characteristics of your data, especially in larger datasets. On your R journey, you will encounter many other packages which provide useful functions like these.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html",
    "href": "09_sources_of_bias.html",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "",
    "text": "9.1 Linearity and additivity\nThe assumption of linearity postulates that the relationship of variables represents a straight line and not a curve or any other shape. Figure @ref(fig:linear-nonlinear-relationships) depicts examples of how two variables could be related to each other. Only the first one demonstrates a linear relationship, and all other plots would represent a violation of linearity.\nExamples of linear and non-linear relationships of two variables\nData visualisations are particularly useful to identify whether variables are related to each other in a linear fashion. The examples above were all created with geom_point(), which creates a dot plot that maps the relationship between two variables. In Chapter @ref(correlations), we will look more closely at the relationship of two variables in the form of correlations, which measure the strength of a linear relationship between variables.\nAdditivity is given when the effects of all independent variables can be added up to obtain the total effect they have on a dependent variable. In other words, the effect that multiple variables have on another variable can be added up to reflect their total effect.\nIf we assume that we have a dependent variable \\(Y\\) which is affected by other (independent) variables \\(X\\), we could summarise additivity and linearity as a formula:\nThe \\(\\beta\\) (beta) stands for the degree of change in a variable \\(X\\) causes in \\(Y\\). Or, in simpler terms, \\(\\beta\\) reflects the impact an independent variable has on the dependent variable. We will return to this equation in Chapter @ref(regression), where we create a linear model via regression.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html#sec-additivity-and-linearity",
    "href": "09_sources_of_bias.html#sec-additivity-and-linearity",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "",
    "text": "\\(Y = \\beta_{1} * X_1 + \\beta_{2} * X_{2} + ... + \\beta_{n} * X_{n}\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html#independence-sec-independence",
    "href": "09_sources_of_bias.html#independence-sec-independence",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "9.2 Independence {sec-independence}",
    "text": "9.2 Independence {sec-independence}\nThe notion of independence is an important one. It assumes that each observation in our dataset is independent of other observations. For example, imagine my wife Fiona and I take part in a study that asks us to rank movies by how much we like them. Each of us has to complete the ranking by ourselves, but since we both sit in the same living room, we start chatting about these movies. By doing so, we influence each other’s rankings and might even agree on the same ranking. Thus, our scores are not independent from each other. On the other hand, suppose we were both sitting in our respective offices and rank these movies. In that case, the rankings could potentially still be very similar, but this time the observations are independent of each other.\nThere is no statistical measurement or plot that can tell us whether observations are independent or not. Ensuring independence is a matter of data collection and not data analysis. Thus, it depends on how you, for example, designed your experiment, or when and where you ask participants to complete a survey, etc. Still, this criterion should not be downplayed as being a ‘soft’ one, just because there is no statistical test, but should remain on your radar throughout the planning and data collection stage of your research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html#sec-normality",
    "href": "09_sources_of_bias.html#sec-normality",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "9.3 Normality",
    "text": "9.3 Normality\nWe touched upon the notion of ‘normality’ and ‘normal distributions’ before in Chapter @ref(spread-of-data) because it refers to the spread of our data. Figure @ref(fig:normal-distribution2) should look familiar by now.\n\n\n\n\n\nA normal distribution\n\n\n\n\nHowever, we have yet to understand why it is essential that our data follows a normal distribution. Most parametric tests are based on means. For example, if we want to compare two groups with each other, we would compute the mean for each of them and then see whether their means differ from each other in a significant way (see Chapter @ref(comparing-groups). Of course, if our data is not very normally distributed, means are a poor reference point for most of the observations in this group. We already know that outliers heavily affect means, but even without outliers, the mean could be a poor choice. Let me provide some visual examples.\n\n\nWarning in geom_text(aes(x = mean(income) - 15, y = 2e-04, label = \"mean\", : All aesthetics have length 1, but the data has 11 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_text(aes(x = median(income) + 15, y = 0.00024, label = \"median\", : All aesthetics have length 1, but the data has 11 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_text(aes(x = mean(income) + 25, y = 2e-04, label = \"mean\", : All aesthetics have length 1, but the data has 13 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_text(aes(x = median(income) - 30, y = 0.00024, label = \"median\", : All aesthetics have length 1, but the data has 13 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_text(aes(x = mean(income) - 15, y = 2e-04, label = \"mean\", : All aesthetics have length 1, but the data has 12 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_text(aes(x = median(income) + 15, y = 0.00024, label = \"median\", : All aesthetics have length 1, but the data has 12 rows.\nℹ Did you mean to use `annotate()`?\n\n\n\n\n\n\n\n\n\nWe notice that neither the median nor the mean by themselves is a reliable indicator for normality. Figure A and Figure C both show that the median and mean are almost identical, but only Figure A follows a normal distribution. The median and mean in Figure C are not reflective of the average observation in this dataset. Most scores lie below and above the mean/median. Therefore, when we analyse the normality of our data, we usually are not interested in the normality of a single variable but the normality of the sampling distribution. However, we cannot directly assess the sampling distribution in most cases. As such, we often revert to testing the normality of our data. There are also instances where we would not expect a normal distribution to exist. Consider the following plots:\n\n\n\n\n\nTwo normal distributions in one dataset.\n\n\n\n\nThe first plot clearly shows that data is not normally distributed. If anything, it looks more like the back of a camel. The technical term for this distribution is called ‘bimodal distribution’. In cases where our data has even more peaks we consider it as a ‘multimodal distribution’. If we identify distributions that look remotely like this, we can assume that there must be another variable that helps explain why there are two peaks in our distribution. The plot below reveals that gender appears to play an important role. Drawing the distribution for each subset of our data reveals that income is now normally distributed for each group and has two different means. Thus, solely focusing on normal distributions for a single variable would not be meaningful if you do not consider the impact of other variables. If we think that gender plays an important role to understand income levels, we would have to expect that the distribution is not normal when looking at our data in its entirety.\nDetermining whether data is normally distributed can be challenging when only inspecting plots, e.g. histograms, density plots or QQ plots. Luckily, there is also a statistical method to test whether our data is normally distributed: The Shapiro-Wilk test. This test compares our distribution with a normal distribution (like in our plot) and tells us whether our distribution is significantly different from it. Thus, if the test is not significant, the distribution of our data is not significantly different from a normal distribution, or in simple terms: It is normally distributed. We can run the test in R as follows for the dataset that underpins Fig A above:\n\nshapiro.test(data$income)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$income\nW = 0.94586, p-value = 0.5916\n\n\nThis result confirms that the data is normally distributed, because it is not significantly different (\\(p &gt; 0.05\\)). The chapter on correlations looks at significance and its meaning more thoroughly (see Chapter @ref(correlations)).\nIn conclusion, the normality of data is an essential pre-test for any of our studies. If we violate the assumption of normality, we will have to fall back to non-parametric tests. However, this rule has two exceptions: The Central Limit Theorem and using ‘robust’ measures of parametric tests.The Central Limit Theorem postulates that as our sample becomes larger, our sampling distribution becomes more and more normal around the mean of the underlying population. For example, Field (2013) (p.54) refers to a sample of 30 as a common rule of thumb. As such, it is possible to assume normality for larger datasets even though our visualisation and the Shapiro-Wilk test tell us otherwise. The second exception is that many parametric tests offer a ‘robust’ alternative, often via bootstrapping. Bootstrapping refers to the process of subsampling your data, for example, 2000 times, and look at the average outcome. An example of bootstrapping is shown in Chapter @ref(bootstrapped-regression).\nAdmittedly, this raises the question: Can I ignore normality and move on with my analysis if my sample is large enough or I use bootstrapping? In short: Yes. This fact probably also explains why we hardly ever find results from normality tests in journal publications since most Social Science research involves more than 30 participants. However, if you find yourself in a situation where the sample size is smaller, all of the above needs to be check and thoroughly considered. However, the sample size also has implications with regards to the power of your test/findings (see Chapter @ref(power-analysis)). Ultimately, the answer to the above questions remains, unsatisfyingly: ‘It depends’.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html#sec-homogeneity-of-variance",
    "href": "09_sources_of_bias.html#sec-homogeneity-of-variance",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "9.4 Homogeneity of variance (homoscedasticity)",
    "text": "9.4 Homogeneity of variance (homoscedasticity)\nThe term ‘variance’ should sound familiar, because we mentioned it in Chapter @ref(standard-deviation) where we looked at the standard deviation derived from the variance.\nHomogeneity of variance implies that the variance of, for example, two subsets of data, is equal or close to being equal. Let’s look at how close the observed values are to the mean for the two groups identified in Figure @ref(fig:two-normalities-groups).\n\ndata4 |&gt;\n  ggplot(aes(x = gender, y = income, col = gender)) +\n  geom_jitter(width = 0.1) +\n  geom_hline(yintercept = group_means$mean[1], color = \"red\") +\n  geom_hline(yintercept = group_means$mean[2], color = \"turquoise\")\n\n\n\n\n\n\n\n\nJudging by eye, we could argue that most values lie around the mean for each respective group. However, some observations are a bit further off. Still, using this visualisation, it is tough to judge whether the spread is about the same. However, boxplots can help with this, or even better, a boxplot reflected by a bar. The package ggdist has an excellent plotting function called stat_interval(), which allows us to show a boxplot in the form of a bar.\n\ndata4 |&gt;\n  ggplot(aes(x = gender, y = income, group = gender)) +\n  ggdist::stat_interval(aes(y = income),\n                        .width = c(0.25, 0.5, 0.75, 1)) +\n\n  # Let's add some nice complementary colours\n  scale_color_manual(values = c(\"#4D87B3\", \"#A1D6FF\", \"#FFDAA1\", \"#B3915F\"))\n\n\n\n\n\n\n\n\nIf we compare the bars, we can tell that the variance in both groups looks very similar, i.e. the length of the bars appear to be about the same height. Furthermore, if we compare the IQR for both groups, we find that they are quite close to each other.\n\ndata4 |&gt;\n  group_by(gender) |&gt;\n  summarise(iqr = IQR(income))\n\n# A tibble: 2 × 2\n  gender   iqr\n  &lt;fct&gt;  &lt;dbl&gt;\n1 male    82.5\n2 female  99  \n\n\nHowever, to test whether the variance between these two groups is truly similar or different, we have to perform a Levene’s test. The Levene’s test follows a similar logic as the Shapiro-Wilk test. If the test is significant, i.e. \\(p &lt; 0.05\\), we have to assume that the variances between these groups are significantly different from each other. However, if the test is not significant, then the variances are similar, and we can proceed with a parametric test - assuming other assumptions are not violated. The interpretation of this test follows the one for the Shapiro-Wilk test. To compute the Levene’s test we can use the function leveneTest() from the car package. However, instead of using , to separate the two variables, this function expects a formula, which is denotated by a ~. We will cover formulas in the coming chapters.\n\ncar::leveneTest(gep$age ~ gep$gender)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   2  0.8834 0.4144\n      297               \n\n\nThe Levene’s test shows that our variances are similar and not different from each other because \\(p &gt; 0.05\\). This is good news if we wanted to continue and perform a group comparison, like in Chapter @ref(comparing-groups).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "09_sources_of_bias.html#sec-dealing-with-outliers",
    "href": "09_sources_of_bias.html#sec-dealing-with-outliers",
    "title": "9  Sources of bias: Outliers, normality and other ‘conundrums’",
    "section": "9.5 Outliers and how to deal with them",
    "text": "9.5 Outliers and how to deal with them\nIn Chapter @ref(descriptive-statistics), I referred to outliers many times but never eluded to the aspects of handling them. Dealing with outliers is similar to dealing with missing data. However, it is not quite as straightforward as one might think.\nIn a first step, we need to determine which values count as an outlier. Aguinis, Gottfredson, and Joo (2013) reviewed 232 journal articles and found that scholars had defined outliers in 14 different ways, used 39 different techniques to detect them and applied 20 different strategies to handle them. It would be impossible to work through all these options in this book. However, I want to offer two options that have been frequently considered in publications in the field of Social Sciences:\n\nThe standard deviation (SD), and\nThe inter-quartile range (IQR).\n\n\n9.5.1 Detecting outliers using the standard deviation\nA very frequently used approach to detecting outliers is the use of the standard deviation. Usually, scholars use multiples of the standard deviation to determine thresholds. For example, a value that lies 3 standard deviations above or below the mean could be categorised as an outlier. Unfortunately, there is quite some variability regarding how many multiples of the standard deviation counts as an outlier. Some authors might use 3, and others might settle for 2 (see also Leys et al. (2013)). Let’s stick with the definition of 3 standard deviations to get us started. We can revisit our previous plot regarding runtime_min (see Figure @ref(fig:runtime-movies-deviation)) and add lines that show the thresholds above and below the mean. As before, I will create a base plot outlier_plot first so that we do not have to repeat the same code over and over again. We then use outlier_plot and add more layers as we see fit.\n\n# Compute the mean and the thresholds\nruntime_mean &lt;- mean(imdb_top_250$runtime_min)\nsd_upper &lt;- runtime_mean + 3 * sd(imdb_top_250$runtime_min)\nsd_lower &lt;- runtime_mean - 3 * sd(imdb_top_250$runtime_min)\n\n# Create the base plot\noutlier_plot &lt;-\n  imdb_top_250 |&gt;\n  select(title, runtime_min) |&gt;\n  ggplot(aes(x = reorder(title, runtime_min),\n             y = runtime_min)) +\n  geom_point(shape = 124) +\n\n  theme(axis.text.x = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.background = element_blank()\n        ) +\n  ylab(\"run time\") +\n  xlab(\"movies\")\n\n# Add the thresholds and mean\noutlier_plot +\n  geom_hline(aes(yintercept = runtime_mean, col = \"red\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = sd_upper, col = \"blue\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = sd_lower, col = \"blue\"),\n             show.legend = FALSE)\n\n\n\n\n\n\n\n\nThe results suggest that only very few outliers would be detected if we chose these thresholds. Especially Sherlock Jr., the shortest movie in our dataset, would not classify as an outlier. How about we choose 2 standard deviations instead?\n\n# Compute the mean and standard deviation\nruntime_mean &lt;- mean(imdb_top_250$runtime_min)\nsd_upper &lt;- runtime_mean + 2 * sd(imdb_top_250$runtime_min)\nsd_lower &lt;- runtime_mean - 2 * sd(imdb_top_250$runtime_min)\n\n# Create our plot\noutlier_plot +\n  geom_hline(aes(yintercept = runtime_mean, col = \"red\"),\n               show.legend = FALSE) +\n  geom_hline(aes(yintercept = sd_upper, col = \"blue\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = sd_lower, col = \"blue\"),\n             show.legend = FALSE)\n\n\n\n\n\n\n\n\nAs we would expect, we identify some more movies as being outliers, but the shortest movie would still not be classified as an outlier. It certainly feels somewhat arbitrary to choose a threshold of our liking. Despite its popularity, there are additional problems with this approach:\n\noutliers affect our mean and standard deviation too,\nsince we use the mean, we assume that our data is normally distributed, and\nin smaller samples, this approach might result in not identifying outliers at all (despite their presence) (Leys et al. 2013) (p. 764).\n\nLeys et al. (2013) propose an alternative approach because medians are much less vulnerable to outliers than the mean. Similarly to the standard deviation, it is possible to calculate thresholds using the median absolute deviation (MAD). Best of all, the function mad() in R does this automatically for us. Leys et al. (2013) suggest using 2.5 times the MAD as a threshold. However, if we want to compare how well this option performs against the standard deviation, we could use 3 as well.\n\n# Compute the median and thresholds\nruntime_median &lt;- median(imdb_top_250$runtime_min)\nmad_upper &lt;- runtime_median + 3 * mad(imdb_top_250$runtime_min)\nmad_lower &lt;- runtime_median - 3 * mad(imdb_top_250$runtime_min)\n\n# Create our plot\noutlier_plot +\n  geom_hline(aes(yintercept = runtime_median, col = \"red\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = mad_upper, col = \"blue\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = mad_lower, col = \"blue\"),\n             show.legend = FALSE)\n\n\n\n\n\n\n\n\nCompared to our previous results, we notice that the median approach was much better in detecting outliers at the upper range of runtim_min. Because the median is not affected so much by the five-hour-long movie, the results have improved. Still, we would not classify the outlier at the bottom for the shortest film in the data. If we chose the criterion of 2.5 * MAD, we would also get this outlier (see Figure @ref(fig:MAD2-outlier-detection)).\n\n\n\n\n\nOutlier detection via MAD using 2.5 * MAD as a threshold\n\n\n\n\nWhich approach to choose very much depends on the nature of your data. For example, one consideration could be that if the median and the mean of a variable are dissimilar, choosing the median might be the better option. However, we should not forget that the outliers we need to identify will affect the mean. Hence, it appears that in many cases, the median could be the better choice. Luckily, there is yet another highly popular method based on two quartiles (rather than one, i.e. the median).\n\n\n9.5.2 Detecting outliers using the interquartile range (IQR)\nAnother approach to classifying outliers is the use of the interquartile range (IQR). The IQR is used in boxplots and creates the dots at its ends to indicate any outliers. This approach is straightforward to implement because the computation of the IQR is simple:\n\n\\(IQR = Q_{3}-Q_{1}\\)\n\nTherefore, we can create new thresholds for the detection of outliers. For the IQR, it is common to use \\(\\pm 1.5 * IQR\\) as the lower and upper thresholds measured from \\(Q_1\\) and \\(Q_3\\) respectively. To compute the quartiles we can use the function quantile(), which returns \\(Minimum, Q_1, Q_2, Q_3, and\\ Q_4\\)\n\n# Compute the quartiles\n(runtime_quantiles &lt;- quantile(imdb_top_250$runtime_min))\n\n    0%    25%    50%    75%   100% \n 45.00 107.00 126.00 145.75 321.00 \n\n# Compute the thresholds\niqr_upper &lt;- runtime_quantiles[4] + 1.5 * IQR(imdb_top_250$runtime_min)\niqr_lower &lt;- runtime_quantiles[2] - 1.5 * IQR(imdb_top_250$runtime_min)\n\n# Create our plot\noutlier_plot +\n  geom_hline(aes(yintercept = runtime_median, col = \"red\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = iqr_upper, col = \"blue\"),\n             show.legend = FALSE) +\n  geom_hline(aes(yintercept = iqr_lower, col = \"blue\"),\n             show.legend = FALSE)\n\n\n\n\n\n\n\n\nAs we can tell, the IQR method detects about the same outliers for our data as the median absolute deviation (MAD) approach. The outliers we find here are the same as shown in Figure @ref(fig:a-boxplot). For our data, I would argue that the IQR and MAD method by Leys et al. (2013) produced the ‘best’ selection of outliers. However, we have to acknowledge that these classifications will always be subjective because how we position the thresholds depends on the researcher’s choice. Still, the computation is standardised, and we can plausibly explain the process of identifying outliers, we just have to make it explicit to the audience of our research.\n\n\n9.5.3 Removing or replacing outliers\nNow that we have identified our outliers, we are confronted with the question of what we should do with them. Like missing data (see Chapter @ref(dealing-with-missing-data)), we can either remove them or replace them with other values. While removal is a relatively simple task, replacing it with other ‘reasonable’ values implies finding techniques to create such values. As you may remember, we were confronted with a similar problem before when we looked into missing data (Chapter @ref(dealing-with-missing-data)). The same techniques, especially multiple imputation (see Cousineau and Chartier 2010), can be used for such scenarios.\nIrrespective of whether we remove or replace outliers, we somehow need to single them out of the crowd. Since the IQR strategy worked well for our data, we can use the thresholds we defined before, i.e. iqr_upper and iqr_lower. Therefore, an observation (i.e. a movie) is considered an outlier if\n\nits value lies above iqr_upper, or\nits value lies below iqr_lower\n\nIt becomes clear that we somehow need to define a condition because if it is an outlier, it should be labelled as one, but if not, then obviously we need to label the observation differently. Ideally, we want a new column in our dataset which indicates whether a movie is an outlier (i.e. outlier == TRUE) or not (outlier == FALSE). R offers a way for us to express such conditions with the function ifelse(). It has the following structure:\n\nifelse(condition, TRUE, FALSE)\n\nLet’s formulate a sentence that describes our scenario as an ifelse() function:\n\nIf a movie’s runtime_min is longer than iqr_upper, or\nif a movie’s runtime_min is lower than iqr_lower,\nclassify this movie as an outlier (i.e. TRUE),\notherwise, classify this movie as not being an outlier (i.e. FALSE).\n\nWe already know from Chapter @ref(basic-computations-in-r) how to use logical and arithmetic operators. All we have to do is put them together in one function call and create a new column with mutate().\n\nimdb_top_250 &lt;-\n  imdb_top_250 |&gt;\n  mutate(outlier = ifelse(runtime_min &gt; iqr_upper |\n                            runtime_min &lt; iqr_lower,\n                          TRUE, FALSE))\n\nSince we have now a classification, we can more thoroughly inspect our outliers and see which movies are the ones that are lying outside our defined norm. We can arrange() them by runtime_min.\n\nimdb_top_250 |&gt;\n  filter(outlier == \"TRUE\") |&gt;\n  select(title, runtime_min, outlier) |&gt;\n  arrange(runtime_min)\n\n# A tibble: 8 × 3\n  title                       runtime_min outlier\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;lgl&gt;  \n1 Sherlock Jr.                         45 TRUE   \n2 Andrei Rublev                       205 TRUE   \n3 Seven Samurai                       207 TRUE   \n4 Ben-Hur                             212 TRUE   \n5 Lawrence of Arabia                  228 TRUE   \n6 Once Upon a Time in America         229 TRUE   \n7 Gone with the Wind                  238 TRUE   \n8 Gangs of Wasseypur                  321 TRUE   \n\n\nThe list of movies contains some of the most iconic Hollywood films ever shown on screen. However, I think we can agree that most of them are truly outside the norm of regular movies, not just in terms of runtime.\nFrom here, it is simple to remove these movies (i.e. keep the movies that are not outliers) or set their values to NA. If we replace() the values with NA, we can continue with one of the techniques demonstrated for missing values in Chapter @ref(dealing-with-missing-data). Both approaches are shown in the following code chunk.\n\n# Keep all observations but outliers\nimdb_top_250 |&gt;\n  filter(outlier == \"FALSE\") |&gt;\n  select(title, outlier)\n\n\n# Replace values with NA\nimdb_top_250 |&gt;\n  mutate(runtime_min = replace(runtime_min, outlier == \"TRUE\", NA)) |&gt;\n  select(title, runtime_min)\n\n# A tibble: 250 × 2\n   title                                         runtime_min\n   &lt;chr&gt;                                               &lt;dbl&gt;\n 1 The Shawshank Redemption                              142\n 2 The Godfather                                         175\n 3 The Dark Knight                                       152\n 4 The Godfather: Part II                                202\n 5 12 Angry Men                                           96\n 6 The Lord of the Rings: The Return of the King         201\n 7 Pulp Fiction                                          154\n 8 Schindler's List                                      195\n 9 Inception                                             148\n10 Fight Club                                            139\n# ℹ 240 more rows\n\n\nThe replace() function is very intuitive to use. It first needs to know where you want to replace a value (runtime_min), then what the condition for replacing is (outlier == \"TRUE\"), and lastly, which value should be put instead of the original one (NA).\n\n\n9.5.4 Concluding remarks about outliers\nAs you hopefully noticed, understanding your data requires some effort, but it is important to know your data well before proceeding to any further analysis. You can experiment with different data visualisations and design them in a way that best reflects the message you want to get across. For example, because we have added a new variable that classifies outliers, we can do more with our data visualisation than before and highlight them more clearly.\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = reorder(genre_01, runtime_min),\n             y = runtime_min,\n             col = outlier,\n             shape = outlier)\n         ) +\n  geom_point(size = 1) +\n  theme(panel.background = element_blank()) +\n  coord_flip()\n\n\n\n\n\n\n\n\nTo round things off, let me share with you one more visualisation, which demonstrates that outliers might also have to be defined based on specific groupings of your data, e.g. movie genres. Thus, it might be essential to consider outliers in light of sub-samples of your data rather than the entire dataset, especially when comparing groups (see Chapter @ref(comparing-groups)).\n\n# The wesanderson package can make your plots look more\n# 'Wes Anderson'\ncolour_pal &lt;- wesanderson::wes_palette(\"Darjeeling1\", 11, type = \"continuous\")\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = reorder(genre_01, runtime_min),\n             y = runtime_min,\n             col = genre_01)\n         ) +\n  geom_boxplot(alpha = 0,\n               show.legend = FALSE) +\n  geom_jitter(width = 0.1,\n              size = 0.5,\n              alpha = 0.5,\n              show.legend = FALSE\n              ) +\n  scale_color_manual(values = colour_pal) +\n  coord_flip() +\n  theme(panel.background = element_blank()) +\n  xlab(\"runtime\") +\n  ylab(\"Genre\") +\n  ggtitle(\"Distribution of movie runtimes by genre\")\n\n\n\n\n\n\n\n\n\n\n\n\nAguinis, Herman, Ryan K Gottfredson, and Harry Joo. 2013. “Best-Practice Recommendations for Defining, Identifying, and Handling Outliers.” Organizational Research Methods 16 (2): 270–301.\n\n\nCousineau, Denis, and Sylvain Chartier. 2010. “Outliers Detection and Treatment: A Review.” International Journal of Psychological Research 3 (1): 58–67.\n\n\nField, Andy. 2013. Discovering Statistics Using IBM SPSS Statistics. Sage Publications.\n\n\nLeys, Christophe, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent Licata. 2013. “Detecting Outliers: Do Not Use Standard Deviation Around the Mean, Use Absolute Deviation Around the Median.” Journal of Experimental Social Psychology 49 (4): 764–66.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sources of bias: Outliers, normality and other 'conundrums'</span>"
    ]
  },
  {
    "objectID": "10_correlations.html",
    "href": "10_correlations.html",
    "title": "10  Correlations",
    "section": "",
    "text": "10.1 Plotting correlations\nSince correlations only show the relationship between two variables, we can easily put one variable on the x-axis and one on the y-axis, creating a so-called ‘scatterplot’. We used two functions to plot scatterplots before, i.e. geom_point() and geom_jitter. Let’s try to answer our first research question, i.e. whether regular movie viewers and critics (people who review movies as a profession) rate the top 250 in the same way. One assumption could be that it does not matter whether you are a regular movie viewer or someone who does it professionally. After all, we are just human beings. A counter thesis could be that critics have a different perspective on movies and might use other evaluation criteria. Either way, we first need to identify the two variables of interest:\nimdb_top_250 |&gt;\n  filter(!is.na(metascore)) |&gt;\n  ggplot(aes(imdb_rating, metascore)) +\n  geom_jitter() +\n  see::theme_modern()\nThe results from our scatterplot are, well, somewhat random. We can see that some movies receive high imdb_ratings as well as high metascores. However, some movies receive high imdb_ratings but low metascores. Overall, the points look like they are randomly scattered all over our canvas. The only visible pattern we can notice is that there are more movies at the lower end of the rating system relative to all the films in the top 250. In fact, there are only two movies that received an IMDb rating of over 9.0. Be aware that geom_jitter() offsets values slightly, so it can appear as if there were more movies over 9.0.\nSince correlations only explain linear relationships, a perfect correlation would be represented by a straight line. Consider the following examples of correlations:\nA correlation can be either positive or negative, and its value (i.e. r in case of the Pearson correlation) can range from -1 to 1:\nIn other words, the further the score is away from 0, the stronger is the relationship between variables. We also have benchmarks that we can use to assess the strength of a relationship, for example, the one by Cohen (1988). The strength of the relationship is also called ‘effect size’. Table @ref(tab:effect-size-cohen) shows the relevant benchmarks. Note that effect sizes are always provided as absolute figures. Therefore, -0.4 would also count as a moderate relationship.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "10_correlations.html#sec-plotting-correlations",
    "href": "10_correlations.html#sec-plotting-correlations",
    "title": "10  Correlations",
    "section": "",
    "text": "imdb_rating is based on IMDb users, and\nmetascore is based on movie critics.\n\n\n\n\n\n\n\n-1 defines a perfectly negative correlation,\n0 defines no correlation (completely random), and\n1 defines a perfectly positive correlation.\n\n\n\n(#tab:effect-size-cohen) Assessing effect size of relationships according to Cohen (1988)\n\n\neffect size\ninterpretation\n\n\n\n\nr &lt; 0.1\nvery small\n\n\n0.1 \\(\\leq\\) r &lt; 0.3\nsmall\n\n\n0.3 \\(\\leq\\) r &lt; 0.5\nmoderate\n\n\nr \\(\\geq\\) 0.5\nlarge",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "10_correlations.html#sec-computing-correlations",
    "href": "10_correlations.html#sec-computing-correlations",
    "title": "10  Correlations",
    "section": "10.2 Computing correlations",
    "text": "10.2 Computing correlations\nIf we compare the plot from our data with the sample plots, we can conclude that the relationship is weak, and therefore the r must be close to zero. We can test this with the Pearson correlation using the function correlation() from the correlation package. By default, it will perform a Pearson correlation, which is only applicable for parametric data. As outlined before, if our data violates the assumptions for parametric tests, we can use Spearman’s correlation instead. For the rest of the chapter, we assume that our data is parametric, but I will demonstrate how to compute both in the following code chunk:\n\nlibrary(correlation)\n\n# Pearson correlation\nimdb_top_250 |&gt;\n  select(imdb_rating, metascore) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |    r |        95% CI | t(214) |     p\n----------------------------------------------------------------\nimdb_rating |  metascore | 0.08 | [-0.06, 0.21] |   1.11 | 0.270\n\np-value adjustment method: Holm (1979)\nObservations: 216\n\n\n\n# Spearman correlation\nimdb_top_250 |&gt;\n  select(imdb_rating, metascore) |&gt;\n  correlation(method = \"spearman\")\n\n# Correlation Matrix (spearman-method)\n\nParameter1  | Parameter2 |  rho |        95% CI |        S |     p\n------------------------------------------------------------------\nimdb_rating |  metascore | 0.06 | [-0.08, 0.19] | 1.58e+06 | 0.406\n\np-value adjustment method: Holm (1979)\nObservations: 216\n\n\nIndeed, our analysis reveals that the effect size is very small (r = 0.08 &lt; 0.1). Therefore, critics appear to rate movies differently than regular movie viewers, irrespective of which method we use. This mistmatch of ratings triggers an interesting follow-up question: Which movie is the most controversial one, i.e. for which movie is the difference between imdb_rating and metascore the biggest?\nWe can answer this question with the tools we already know. We create a new variable to subtract the metascore from the imdb_rating and plot it. We have to make sure both scales are the same length. The variable imdb_rating ranges from 0-10, but the metascore ranges from 0-100. I used the percentile score instead by dividing the scores by 10 and 100, respectively. Since plotting 250 movies would have been too much and would also not help us find the answer to our question, I chose arbitrary values to pick only those movies with the highest differences in scores. Feel free to adjust the filter() to your liking to see more or fewer movies. While this plot requires much more coding than previous ones, it looks nicer and demonstrates how many of the functions we have learned so far can be combined to create unique visualisations of your data.\n\nplot_data &lt;-\n  imdb_top_250 |&gt;\n  mutate(r_diff = imdb_rating / 10 - metascore / 100) |&gt;\n  filter(!is.na(r_diff) & r_diff &gt;= 0.25 | r_diff &lt;= -0.165)\n\nplot_data |&gt;\n  ggplot(aes(x = reorder(title, r_diff), y = r_diff, label = title)) +\n  \n  # Colour bars based on the value of 'r_diff'\n  geom_col(aes(fill = ifelse(r_diff &gt; 0, \"Viewers\", \"Critics\"))) +\n  \n  # Add movie titles\n  geom_text(aes(x = title,\n                y = 0,\n                label = title),\n            size = 2.5,\n\n            # Align the labels of the plot\n            hjust = ifelse(plot_data$r_diff &gt;= 0, \"right\", \"left\"),\n            nudge_y = ifelse(plot_data$r_diff &gt;= 0, -0.008, 0.008)\n            ) +\n\n  coord_flip() +\n\n  # Cleaning up the plot to make it look more readable and colourful\n  scale_fill_manual(values = c(\"#FFCE7D\", \"#7DC0FF\")) +\n  theme_void() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe blue bars reflect movies that were rated much higher by regular viewers than critics. The opposite is true for the yellow bars, i.e. movies that were favoured by critics.\nAnother question we posed at the beginning was: Do higher-ranked movies receive more votes from users than lower-ranked ones? Our intuition might say ‘yes’. If movies are ranked higher, they are likely seen by more people, which makes them more popular. Consequently, if more people have seen a movie, it is more likely they might vote for this movie. For lower ranked movies, the opposite should be true. Let’s create another scatterplot to find out.\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = imdb_rating, y = votes)) +\n  geom_jitter() +\n  \n  # Adjust the labels of the y-axis\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\"))\n\n\n\n\n\n\n\n\nThe last line of code helps to make the labels on the y-axis more readable. The numbers are in the millions, and it helps to have the indicators present using label_number() from the scales package.\nThe scatterplot shows a somewhat positive trend. Often, it can be tough to see the trend. To improve our plot, we can use the function geom_smooth(), which can help us draw a straight line that best fits our data points. We need to set the method for drawing the line to lm, which stands for ‘linear model’. Remember, correlations assume a linear relationship between two variables.\n\nimdb_top_250 |&gt;\n  ggplot(aes(x = imdb_rating, y = votes)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              col = \"red\")\n\n\n\n\n\n\n\n\nAnother problem we face with this plot (and correlations) are extreme values, i.e. outliers. We already know that outliers tend to cause trouble for our analysis (see Chapter @ref(dealing-with-outliers)), and in correlations, they can affect the strength of relationships. However, the differences are very significant if we compute the Pearson correlation with and without outliers. Be aware that since we work with two variables simultaneously, we should consider outliers in both. The function filter() will have to include four conditions.\n\n# The correlation with outliers\nimdb_top_250 |&gt;\n  select(imdb_rating, votes) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |    r |       95% CI | t(248) |         p\n-------------------------------------------------------------------\nimdb_rating |      votes | 0.59 | [0.51, 0.67] |  11.57 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 250\n\n\n\n# Compute quartiles\nvotes_quantiles &lt;- quantile(imdb_top_250$votes)\nrating_quantiles &lt;- quantile(imdb_top_250$imdb_rating)\n\n# Define outliers\nvotes_out_upper &lt;- votes_quantiles[4] + 1.5 * IQR(imdb_top_250$votes)\nvotes_out_lower &lt;- votes_quantiles[2] - 1.5 * IQR(imdb_top_250$votes)\n\nrating_out_upper &lt;- rating_quantiles[4] + 1.5 * IQR(imdb_top_250$imdb_rating)\nrating_out_lower &lt;- rating_quantiles[2] - 1.5 * IQR(imdb_top_250$imdb_rating)\n\n# Remove outliers\nmovies_no_out &lt;-\n  imdb_top_250 |&gt;\n  filter(votes &lt;= votes_out_upper &\n           votes &gt;= votes_out_lower &\n           imdb_rating &lt;= rating_out_upper &\n           imdb_rating &gt;= rating_out_lower)\n\n# The correlation without outliers (based on 1.5 * IQR)\nmovies_no_out |&gt;\n  filter(votes &lt;= votes_out_upper &\n           votes &gt;= votes_out_lower &\n           imdb_rating &lt;= rating_out_upper &\n           imdb_rating &gt;= rating_out_lower) |&gt;\n  select(imdb_rating, votes) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |    r |       95% CI | t(238) |         p\n-------------------------------------------------------------------\nimdb_rating |      votes | 0.43 | [0.32, 0.53] |   7.41 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 240\n\n\nWhile both correlations are highly significant (\\(p &lt; 0.01\\)), the drop in r from 0.59 to 0.43 is substantial. When we plot the data again, we can see that the dots are fairly randomly distributed across the plotting area. Thanks to geom_smooth we get an idea of a slight positive relationship between these two variables.\n\nmovies_no_out |&gt;\n  ggplot(aes(x = imdb_rating, y = votes)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              col = \"red\")\n\n\n\n\n\n\n\n\nIn conclusion, while there is some relationship between the rating and the number of votes, it is by far not as strong as we might have thought, especially after removing outliers, which had a considerable bearing on the effect size.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "10_correlations.html#sec-significance",
    "href": "10_correlations.html#sec-significance",
    "title": "10  Correlations",
    "section": "10.3 Significance: A way to help you judge your findings",
    "text": "10.3 Significance: A way to help you judge your findings\nOne of the most common pitfalls of novice statisticians is the interpretation of what counts as significant and not significant. Most basic tests offer a ‘p value’, which stands for ‘probability value’. I referred to this value multiple times in this book already. The p-value can range from 1 (for 100%) to 0 (for 0%) and implies:\n\n\\(p = 1\\), there is a 100% chance that the result is a pure coincidence\n\\(p = 0\\), there is a 0% chance that the result is a pure coincidence, i.e. we can be certain this is not just luck.\n\nTechnically, we would not find that p is ever truly zero and instead denote very small p-values with p &lt; 0.01 or even p &lt; 0.001.\nThere are also commonly considered thresholds for the p-value:\n\n\\(p &gt; 0.05\\), the result is not significant. There is a chance of +5% that our finding is a pure coincidence.\n\\(p \\leq 0.05\\) , the result is significant.\n\\(p \\leq 0.01\\), the result is highly significant.\n\nWe will cover more about the p-value in Chapter @ref(comparing-groups) and Chapter @ref(regression). For now, it is important to know that a significant correlation is one that we should look at more closely. Usually, correlations that are not significant suffer from small effect sizes. However, different samples can lead to different effect sizes and different significant levels. Consider the following examples:\n\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |        95% CI | t(2) |     p\n-------------------------------------------------------------\nx          |          y | 0.77 | [-0.73, 0.99] | 1.73 | 0.225\n\np-value adjustment method: Holm (1979)\nObservations: 4\n\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(10) |       p\n---------------------------------------------------------------\nx          |          y | 0.77 | [0.36, 0.93] |  3.87 | 0.003**\n\np-value adjustment method: Holm (1979)\nObservations: 12\n\n\nIn both examples \\(r = 0.77\\), but the sample sizes are different (4 vs 12), and the p-values differ. In the first example, \\(p = 0.225\\), which means the relationship is not significant, while in the second example, we find that \\(p &lt; 0.01\\) and is therefore highly significant. As a general rule, the bigger the sample, the more likely we find significant results, even though the effect size is small. Therefore, it is crucial to interpret correlations base on at least three factors:\n\nthe p-value, i.e. significance level,\nthe r-value, i.e. the effect size, and\nthe sample size.\n\nThe interplay of all three can help determine whether a relationship is important. Therefore, when we include correlation tables in publications, we have to provide information about all three indicators.\nIt is common that we do not only compute correlations for two variables at a time. Instead, we can do this for multiple variables simultaneously.\n\nimdb_top_250 |&gt;\n  select(imdb_rating, metascore, year, votes, gross_in_m) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |     r |         95% CI |     t |  df |         p\n---------------------------------------------------------------------------\nimdb_rating |  metascore |  0.08 | [-0.06,  0.21] |  1.11 | 214 | 0.539    \nimdb_rating |       year |  0.03 | [-0.10,  0.15] |  0.42 | 248 | 0.678    \nimdb_rating |      votes |  0.59 | [ 0.51,  0.67] | 11.57 | 248 | &lt; .001***\nimdb_rating | gross_in_m |  0.21 | [ 0.07,  0.33] |  3.07 | 213 | 0.010**  \nmetascore   |       year | -0.41 | [-0.52, -0.30] | -6.63 | 214 | &lt; .001***\nmetascore   |      votes | -0.25 | [-0.37, -0.12] | -3.76 | 214 | 0.001**  \nmetascore   | gross_in_m | -0.13 | [-0.27,  0.01] | -1.83 | 193 | 0.207    \nyear        |      votes |  0.37 | [ 0.26,  0.47] |  6.29 | 248 | &lt; .001***\nyear        | gross_in_m |  0.36 | [ 0.23,  0.47] |  5.58 | 213 | &lt; .001***\nvotes       | gross_in_m |  0.56 | [ 0.46,  0.64] |  9.79 | 213 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 195-250\n\n\nIf you have seen correlation tables before, you might find that correlation() does not produce the classic table by default. If you want it to look like the tables in publications, which are more compact but offer less information, you can use the function summary().\n\nimdb_top_250 |&gt;\n  select(imdb_rating, metascore, year, votes, gross_in_m) |&gt;\n  correlation() |&gt;\n  summary()\n\n# Correlation Matrix (pearson-method)\n\nParameter   | gross_in_m |   votes |     year | metascore\n---------------------------------------------------------\nimdb_rating |     0.21** | 0.59*** |     0.03 |      0.08\nmetascore   |      -0.13 | -0.25** | -0.41*** |          \nyear        |    0.36*** | 0.37*** |          |          \nvotes       |    0.56*** |         |          |          \n\np-value adjustment method: Holm (1979)\n\n\nThis table answers our final question, i.e. do movies with more votes earn more money. It appears as if this is true, because \\(r = 0.56\\) and \\(p &lt; 0.001\\). In the classic correlation table, you often see *. These stand for the different significant levels:\n\n*, i.e. \\(p &lt; 0.05\\)\n**, i.e. \\(p &lt; 0.01\\)\n***, i.e. \\(p &lt; 0.001\\) (you might find some do not use this as a separate level)\n\nIn short, the more * there are attached to each value, the more significant a result. Therefore, we likely find the same relationships in newly collected data if there are many *.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "10_correlations.html#sec-limitations-of-correlations",
    "href": "10_correlations.html#sec-limitations-of-correlations",
    "title": "10  Correlations",
    "section": "10.4 Limitations of correlations",
    "text": "10.4 Limitations of correlations\nCorrelations are helpful, but only to some extend. The three most common limitations you should be aware of are:\n\nCorrelations are not causal relationships\nCorrelations can be spurious\nCorrelations might only appear in sub-samples of your data\n\n\n10.4.1 Correlations are not causal relationships\nCorrelations do not offer insights into causality, i.e. whether a change in one variable causes change in the other variable. Correlations only provide insights into whether these two variables tend to change when one of them changes. Still, sometimes we can infer such causality by the nature of the variables. For example, in countries with heavy rain, more umbrellas are sold. Buying more umbrellas will not cause more rain, but if there is more rain in a country, we rightly assume a higher demand for umbrellas. If we can theorise the relationship between variables, we would rather opt for a regression model instead of a correlation (see Chapter @ref(regression)).\n\n\n10.4.2 Correlations can be spurious\nJust because we find a relationship between two variables does not necessarily mean that they are truly related. Instead, it might be possible that a third variable is the reason for the relationship. We call relationships between variables that are caused by a third variable ‘spurious correlations’. This third variable can either be part of our dataset or even something we have not measured at all. The latter case would make it impossible to investigate the relationship further. However, we can always test whether some of our variables affect the relationship between the two variables of interest. This can be done by using partial correlations. A partial correlation returns the relationship between two variables minus the relationship to a third variable. Figure @ref(fig:illustration-spurious-correlation) depicts this visually. While a and b appear to be correlated, the correlation might only exist because they correlate with x.\n\n\n\n\n\n\nFigure 10.1: Illustration of a spurious correlation\n\n\n\nLet’s consider a practical example. We found that votes and gross_in_m are positively correlated with each other. However, could it be possible that this relationship is affected by the year in which the movies were published? We could assume that later movies received more votes because it has become more of a cultural phenomenon to vote about almost everything online1.\n\n# Correlation between variables\nimdb_top_250 |&gt;\n  select(votes, gross_in_m, year) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI |    t |  df |         p\n----------------------------------------------------------------------\nvotes      | gross_in_m | 0.56 | [0.46, 0.64] | 9.79 | 213 | &lt; .001***\nvotes      |       year | 0.37 | [0.26, 0.47] | 6.29 | 248 | &lt; .001***\ngross_in_m |       year | 0.36 | [0.23, 0.47] | 5.58 | 213 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 215-250\n\n\n\n# Correlation between variables, considering partial correlations\nimdb_top_250 |&gt;\n  select(votes, gross_in_m, year) |&gt;\n  correlation(partial = TRUE)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(213) |         p\n------------------------------------------------------------------\nvotes      | gross_in_m | 0.48 | [0.37, 0.58] |   7.98 | &lt; .001***\nvotes      |       year | 0.28 | [0.16, 0.40] |   4.31 | &lt; .001***\ngross_in_m |       year | 0.16 | [0.03, 0.29] |   2.43 | 0.016*   \n\np-value adjustment method: Holm (1979)\nObservations: 215\n\n\nThe first table reveals a strong relationship between the variables of interest, i.e. between votes and gross_in_m with an effect size of \\(r = 0.56\\). There is also a significant relationship between votes and year, \\(r = 0.37\\) and between gross_in_m and year, \\(r = 0.36\\). Thus, we should control for year to see how it might affect the relationship between votes and gross_in_m. The second table suggests that year does not much affect the relationship between votes and gross_in_m. However, we notice that the relationship between gross_in_m and year substantially goes down to \\(r = 0.16\\). It appears as if the year has not managed to impact the relationship between votes and gross_in_m all that much. Therefore, we can be more confident that this relationship is likely not spurious. However, we can never be fully sure because we might not have all data that could explain this correlation.\n\n\n10.4.3 Simpson’s Paradox: When correlations betray you\nThe final limitation is so important that it even has its own name: the ‘Simpson’s Paradox’. Let’s find out what is so paradoxical about some correlations. For this demonstration, we have to make use of a different dataset: simpson of the r4np package. It contains information about changes in student performance and changes in happiness. The dataset includes responses from three different groups: Teachers, Students and Parents.\nWe would assume that an increase in students’ performance will likely increase the happiness of participants. After all, all three have stakes in students’ performance.\n\nsimpson |&gt;\n  ggplot(aes(x = performance,\n             y = happiness)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              color = \"red\")\n\n\n\n\n\n\n\n\n\nsimpson |&gt;\n  select(performance, happiness) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |     r |         95% CI | t(298) |         p\n----------------------------------------------------------------------\nperformance |  happiness | -0.34 | [-0.44, -0.24] |  -6.32 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 300\n\n\nIt appears we were terribly wrong. It seems as if performance and happiness are moderately negatively correlated with each other. Thus, the more a student improves their performance, the less happy teachers, parents, and students are. I hope you agree that this is quite counter-intuitive. However, what could be the cause for such a finding?\nIf you have the instincts of a true detective, you would think that maybe we should look at each group of participants separately. So, let’s plot the same scatterplot again but colour the responses of each participant group differently and also compute the correlation for each subset of our data.\n\nsimpson |&gt;\n  ggplot(aes(x = performance,\n             y = happiness,\n             group = group,\n             col = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE,\n              col = \"black\") +\n\n  # Choose custom colours for the dots\n  scale_color_manual(values = c(\"#42BDFF\", \"#FF5C67\", \"#B3AF25\"))\n\n\n\n\n\n\n\n\n\nsimpson |&gt;\n  group_by(group) |&gt;\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nGroup   |  Parameter1 | Parameter2 |    r |       95% CI | t(98) |         p\n----------------------------------------------------------------------------\nParent  | performance |  happiness | 0.65 | [0.52, 0.75] |  8.47 | &lt; .001***\nStudent | performance |  happiness | 0.65 | [0.52, 0.75] |  8.47 | &lt; .001***\nTeacher | performance |  happiness | 0.65 | [0.52, 0.75] |  8.47 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 100\n\n\nThe results have magically been inverted. Instead of a negative correlation, we now find a strong positive correlation among all groups. This result seems to make much more sense.\nWhen computing correlations, we need to be aware that subsets of our data might show different directions of correlations. Sometimes insignificant correlations might suddenly become significant. This is what the Simpson’s paradox postulates.\nIf your study relies solely on correlations to detect relationships between variables, which it hopefully does not, it is essential to investigate whether the detected (or undetected) correlations exist. Of course, such an investigation can only be based on the data you obtained. The rest remains pure speculation. Nevertheless, correlations are beneficial to review the bilateral relationship of your variables. It is often used as a pre-test for regressions (see Chapter @ref(regression)) and similarly more advanced computations. As a technique to make inferences, correlations are a good starting point but should be complemented by other steps, if possible.\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences New York. NY: Academic Press.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "10_correlations.html#footnotes",
    "href": "10_correlations.html#footnotes",
    "title": "10  Correlations",
    "section": "",
    "text": "If you want to know what people vote on these days, have a look at www.ranker.com↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "11_power_analysis.html",
    "href": "11_power_analysis.html",
    "title": "11  Power: You either have it or you don’t",
    "section": "",
    "text": "11.1 Ingredients to achieve the power you deserve\nWe already know that sample size matters, for example, to assume normality when there is none in your sample (see Central Limit Theorem in Chapter @ref(normality)). Also, we should not mistakenly assume that a relationship exists where there is none and vice versa. I am referring to so-called Type I and Type II errors (see Table 11.1).\nPower analysis aims to help us avoid type II errors and is defined as the opposite of it, i.e. \\(1 - \\beta\\), i.e. a ‘true positive’. To perform such a power analysis, we need at least three of the following four ingredients:\nAs I mentioned earlier, it makes more sense to use a power analysis to determine the sample size. However, in the unfortunate event that you forgot to do so, at least you can find out whether your results are underpowered by providing the sample size you collected. If you find that your study is underpowered, you are in serious trouble and dire need of more participants.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power: You either have it or you don't</span>"
    ]
  },
  {
    "objectID": "11_power_analysis.html#sec-sec-ingredients-power-analysis",
    "href": "11_power_analysis.html#sec-sec-ingredients-power-analysis",
    "title": "11  Power: You either have it or you don’t",
    "section": "",
    "text": "Table 11.1: Type I and Type II error defined\n\n\n\n\n\n\n\n\n\nError\nMeaning\n\n\n\n\nType I\n\nWe assume a relationship exists between variables where there is none in the population.\nWe also refer to this as ‘false positive’.\nWhile we find a significant relationship between variables in our sample, a larger sample would not find such a relationship.\nIs represented by the Greek letter \\(\\alpha\\) (alpha).\nThe acceptance level of this error is equivalent to the p-value, i.e. significance level.\n\n\n\nType II\n\nWe assume that there is no relationship between variables even though there is one in the population.\nWe also refer to this as ‘false negative’.\nWhile our sample does not reveal a significant relationship between variables, a larger sample would show that the relationship is significant.\nIs represented by the Greek letter \\(\\beta\\) (beta).\n\n\n\n\n\n\n\n\n\nthe power level we want to achieve, i.e. the probability that we find a ‘true positive’,\nthe expected effect size (\\(r\\)) we hope to find,\nthe significance level we set for our test, i.e. how strict we are about deciding when a relationship is significant, and\nthe sample size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power: You either have it or you don't</span>"
    ]
  },
  {
    "objectID": "11_power_analysis.html#sec-computing-power",
    "href": "11_power_analysis.html#sec-computing-power",
    "title": "11  Power: You either have it or you don’t",
    "section": "11.2 Computing power",
    "text": "11.2 Computing power\nSo, how exactly can we compute the right sample size, and how do we find all the numbers for these ingredients without empirical data at our disposal? First, we need a function that can compute it because computing a power analysis manually is quite challenging. The package pwr was developed to do precisely that for different kinds of statistical tests. Table Table 11.2 provides an overview of functions needed to perform power analysis for various tests covered in this book.\n\n\n\nTable 11.2: Power analysis via the package `pwr` for different methods covered in this book\n\n\n\n\n\n\n\n\n\n\nStatistical method\nChapter\nfunction in pwr\n\n\n\n\nCorrelation\nChapter 10\npwr.r.test()\n\n\nT-test\nChapter @ref(comparing-groups)\npwr.t.test()\npwr.t2n.test() (for unequal sample sizes)\n\n\nANOVA\nChapter @ref(comparing-more-than-two-groups)\npwr.anova.test()\n\n\nChi-squared test\nChapter @ref(chi-squared-test)\npwr.chisq.test()\n\n\nLinear regression\nChapter @ref(regression)\npwr.f2.test()\n\n\n\n\n\n\nSince all functions work essentially the same, I will provide an example of last chapter’s correlation analysis. You likely remember that we looked at whether the number of votes a movie receives on IMDb can predict its earnings. Below is the partial correlation that revealed a relationship in our sample based on the top 250 movies. This is just a sample, and it is not representative of all movies listed on IMDb because it only includes the best movies of all time. As of June 2021, IMDb offers information about 8 million movie titles.\n\nimdb_top_250 |&gt;\n  select(votes, gross_in_m, year) |&gt;\n  correlation(partial = TRUE)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(213) |         p\n------------------------------------------------------------------\nvotes      | gross_in_m | 0.48 | [0.37, 0.58] |   7.98 | &lt; .001***\nvotes      |       year | 0.28 | [0.16, 0.40] |   4.31 | &lt; .001***\ngross_in_m |       year | 0.16 | [0.03, 0.29] |   2.43 | 0.016*   \n\np-value adjustment method: Holm (1979)\nObservations: 215\n\n\nLet’s define our ingredients to compute the ideal sample size for our correlation:\n\nPower: Cohen (1988) suggests that an acceptable rate of type II error is \\(\\beta = 0.2\\) (i.e. 20%). Thus, we can define the expected power level as \\(power = 1 - 0.2 = 0.8\\).\nSignificance level (i.e. \\(\\alpha\\)): We can apply the traditional cut-off point of 0.05 (see also Chapter @ref(significance)).\nEffect size: This value is the most difficult to judge but substantially affects your sample size. It is easier to detect big effects than smaller effects. Therefore, the smaller our r, the bigger our sample size has to be. We know from our correlation that the effect size is 0.48, but what can you do to estimate your effect size if you do not have data available. Unfortunately, the only way to acquire an effect size is to either use simulated data or, as most frequently is the case, we need to refer to reference studies that looked at similar phenomena. I would always aim for slightly smaller effect sizes than expected, which means I need a larger sample. If the effect is bigger than expected, we will not find ourselves with an underpowered result. Thus, for our example, let’s assume we expected \\(r = 0.4\\).\n\nAll there is left to do is insert these parameters into our function pwr.r.test().\n\npwr::pwr.r.test(r = 0.4,\n                sig.level = 0.05,\n                power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 45.91614\n              r = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nTo find the effect we are looking for, we only need 46 movies (we always round up!), and our dataset contains 250 observations. Thus, we are considerably overpowered for this type of analysis. What if we change the parameters slightly and expect \\(r = 0.3\\) and increase increase our threshold of accepting a true relationship, i.e. \\(p = 0.01\\)?\n\npwr::pwr.r.test(r = 0.3,\n                sig.level = 0.01,\n                power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 124.405\n              r = 0.3\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\n\nThe results show that if we are stricter with our significance level and look for a smaller effect, we need about three times more movies in our sample, i.e. at least 125.\nGiven that we already know our sample size, we could also use this function to determine the power of our result ex-post. This time we need to provide the sample size n and we will not specify power.\n\npwr::pwr.r.test(n = 215,\n                r = 0.48,\n                sig.level = 0.01)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 215\n              r = 0.48\n      sig.level = 0.01\n          power = 0.9999998\n    alternative = two.sided\n\n\nThe results reveal that our power level is equivalent to cosmic entities, i.e. extremely powerful. In other words, we would not have needed a sample this large to find this genuine relationship.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power: You either have it or you don't</span>"
    ]
  },
  {
    "objectID": "11_power_analysis.html#sec-plotting-power",
    "href": "11_power_analysis.html#sec-plotting-power",
    "title": "11  Power: You either have it or you don’t",
    "section": "11.3 Plotting power",
    "text": "11.3 Plotting power\nLastly, I want to show you how to create a plot that reveals the different sample sizes needed to achieve \\(power = 0.8\\). It visually demonstrates how much more data we need depending on the effect size. For the plot, I set a power level (i.e. 0.8) and significance levels of 0.01 and 0.05. I only varied effect sizes to see the required sample size.\n\n# Create a tibble with effect size r\npower_df &lt;- tibble(r = seq(from = 0.1,\n                           to =  0.9,\n                           by = 0.01))\n\n# We need to compute the sample size for each effect size level\n# power_df &lt;-\n#   power_df |&gt;\n#   rowwise() |&gt;\n#   mutate(n_0_01 =\n#            broom::tidy(pwr::pwr.r.test(r = r,\n#                                        sig.level = 0.01,\n#                                        power = 0.8)) |&gt;\n#            first(),\n#          n_0_05 =\n#            broom::tidy(pwr::pwr.r.test(r = r,\n#                                        sig.level = 0.05,\n#                                        power = 0.8)) |&gt;\n#            first())\n\npower_df &lt;- \n  power_df |&gt;\n  rowwise() |&gt;\n  mutate(n_0_01 =\n           broom::tidy(pwr::pwr.r.test(r = r,\n                            sig.level = 0.01,\n                            power = 0.8)\n           ),\n         n_0_05 =\n           broom::tidy(pwr::pwr.r.test(r = r,\n                            sig.level = 0.05,\n                            power = 0.8)\n           )\n         )\n\nglimpse(power_df)\n\nRows: 81\nColumns: 3\nRowwise: \n$ r      &lt;dbl&gt; 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2…\n$ n_0_01 &lt;tibble[,3]&gt; &lt;tbl_df[26 x 3]&gt;\n$ n_0_05 &lt;tibble[,3]&gt; &lt;tbl_df[26 x 3]&gt;\n\n\n\n# Plot the effect sizes against the sample size\npower_df |&gt;\n  ggplot() +\n\n  # The line for p = 0_01\n  geom_line(aes(x = r,\n                y = n_0_01$n,\n                col = \"p = 0.01\")) +\n\n  # The line for p = 0_05\n  geom_line(aes(x = r,\n                y = n_0_05$n,\n                col = \"p = 0.05\")) +\n\n  # Add nice labels\n  labs(col = \"Sig. levels\",\n       x = \"r\",\n       y = \"n\")\n\n\n\n\n\n\n\n\nThe results reveal that the relationship between n and r is logarithmic and not linear. Thus, the smaller the effect size, the more difficult it is to reach reliable results.\nThe R code to produce this plot features broom::tidy() and first(). The function tidy() from the broom package converts the result from pwr.r.test() from a list to a tibble, and the function first() picks the first entry in this tibble, i.e. our sample sizes. Don’t worry if this is confusing at this stage. We will return to tidy() in later chapters, where its use becomes more apparent.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power: You either have it or you don't</span>"
    ]
  },
  {
    "objectID": "11_power_analysis.html#concluding-remarks-about-power-analysis",
    "href": "11_power_analysis.html#concluding-remarks-about-power-analysis",
    "title": "11  Power: You either have it or you don’t",
    "section": "11.4 Concluding remarks about power analysis",
    "text": "11.4 Concluding remarks about power analysis\nConducting a power analysis is fairly straightforward for the techniques we cover in this book. Therefore there is no excuse to skip this step in your research. The challenging part for conducting a power analysis in advance is the definition of your effect size. If you feel you cannot commit to a single effect size, specify a range, e.g. \\(0.3 &lt; effect size &lt; 0.5\\). This will provide you with a range for your sample size.\nFor statistical tests like group comparisons (Chapter @ref(comparing-groups)) and regressions (Chapter @ref(regression)), the number of groups or variables also plays an important role. Usually, the more groups/variables are included in a test/model, the larger the sample has to be.\nAs mentioned earlier, it is not wrong to collect slightly more data, but you need to be cautious not to waste ‘resources’. In other fields, like medical studies, it is absolutely critical to know how many patients should be given a drug to test its effectiveness. It would be unethical to administer a new drug to 300 people when 50 would be enough. This is especially true if the drug turns out to have an undesirable side effect. In Social Sciences, we are not often confronted with health-threatening experiments or studies. Still, being mindful of how research impacts others is essential. A power analysis can help to determine how you can carry out a reliable study without affecting others unnecessarily.\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences New York. NY: Academic Press.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Power: You either have it or you don't</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html",
    "href": "12_group_comparison.html",
    "title": "12  Comparing groups",
    "section": "",
    "text": "12.1 Comparability: Apples vs Oranges\nBefore we can jump into group comparisons, we need to make ourselves aware of whether our groups can be compared in the first place. ‘Comparability’ should not be confused with ‘are the groups equal’. In many cases, we don’t want groups to be equal in terms of participants, e.g. between-subject studies. On the other hand, we might wish for groups to be perfectly equal when we perform within-subject studies. Thus, asking whether groups are comparable is unrelated to whether the subjects in our study are the same. Instead, we are looking at the characteristics of our groups. Some commonly considered features include:\nWhen we compare groups, we want to minimise the systematic differences that are not the primary focus of our study. Using the right sampling technique can help with this matter. For example, using a random sample and performing a random allocation to groups can help achieve comparable groups and remove systematic differences in a way no other sampling strategy can. However, there is still no guarantee that they will be comparable Berger (2006). Besides, we also face the challenge that in Social Sciences, we do not always have the option of random sampling. For example, International Business studies heavily rely on lists provided by others (e.g. the European Union, Fortune 500, etc.), personal judgement and convenience sampling. Only a small proportion perform probability sampling (Yang, Wang, and Su 2006). In short, there is no reason to worry if your sampling technique is not random. However, it emphasises the need to understand your sample and your groups thoroughly. To inspect characteristics of groups we wish to compare, we can use descriptive statistics as we covered them in Chapter 8). However, this time, we apply these techniques to subsets of our data and not the entire dataset.\nFor example, we might wish to compare female and male Egyptians (see Chapter @ref(two-unpaired-groups)). If we wanted to make sure these two groups can be compared, we might have to check (among other characteristics) whether their age is distributed similarly. We can use the functions we already know to create a plot to investigate this matter and apply the function facet_wrap() at the end.\n# Only select participants from 'Egypt'\ncomp &lt;-\n  wvs_nona |&gt;\n  filter(country == \"Egypt\")\n\n# Plot distribution using facet_wrap()\ncomp |&gt;\n  ggplot(aes(x = age)) +\n  geom_density() +\n  facet_wrap(~ gender)\nThe function facet_wrap(~ gender) allows us to create two plots based on a grouping variable, i.e. gender. We have to use a tilde (~) to indicate the group. This character is related to writing formulas in R, which we will cover in more detail in the following chapters. Being able to place plots next to each other can be very beneficial for comparison. This way, inspecting the normality across multiple groups can be done within seconds. On the other hand, creating separate plots for each group can take a long time, for example, comparing 48 countries.\nAlternatively, we could also create density plots using the ggridges package, which puts all the plots onto the same x-axis, making it even easier to see the differences across groups.\n# Plot distributions\ncomp |&gt;\n  ggplot(aes(x = age,\n             y = gender,\n             fill = gender)) +\n  ggridges::geom_density_ridges(bandwidth = 4)\nIn light of both data visualisations, we can conclude that the distribution of age across both gender groups is fairly similar and likely not different between groups. Of course, we could also statistically explore this using a suitable test before performing the main group comparison. However, we first have to understand how we can perform such tests.\nIn the following chapters, we will primarily rely on the package rstatix, which offers a pipe-friendly approach to using the built-in functions of R to perform our group comparisons. However, you are welcome to try the basic functions, which you can find in Appendix @ref(appendix-comparing-groups).\nlibrary(rstatix)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#sec-comparability-apples-vs-oranges",
    "href": "12_group_comparison.html#sec-comparability-apples-vs-oranges",
    "title": "12  Comparing groups",
    "section": "",
    "text": "Size: Are the groups about equally large?\nTime: Was the data collected around the same time?\nExogenous variables: Is the distribution of characteristics we are not interested in approximately the same across groups?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#sec-comparing-two-groups",
    "href": "12_group_comparison.html#sec-comparing-two-groups",
    "title": "12  Comparing groups",
    "section": "12.2 Comparing two groups",
    "text": "12.2 Comparing two groups\nThe simplest of comparisons is the one where you only have two groups. These groups could either consist of different people (unpaired) or represent two measurements of the same individuals (paired). We will cover scenarios in the following chapter because they require slightly different computational techniques.\n\n12.2.1 Two unpaired groups\nAn unpaired group test assumes that the observations in each group are not related to each other, for example, observations in each group are collected from different individuals.\nOur first comparison will be participants from Egypt, and we want to understand whether male and female citizens in this country perceive their freedom_of_choice differently or equally.\nWe first can compare these two groups using our trusty geom_boxplot() (or any variation) and use different fill colours for each group.\n\n# Compute the mean for and size of each group\ngroup_means &lt;-\n  comp |&gt;\n  group_by(gender) |&gt;\n  summarise(mean = mean(freedom_of_choice),\n            n = n())\n\ngroup_means\n\n# A tibble: 2 × 3\n  gender  mean     n\n  &lt;fct&gt;  &lt;dbl&gt; &lt;int&gt;\n1 female  6.21   579\n2 male    6.82   621\n\n\n\n# Create our data visualisation\ncomp |&gt;\n  ggplot(aes(x = gender, y = freedom_of_choice, fill = gender)) +\n  geom_boxplot() +\n\n  # Add the mean for each group\n  geom_point(data = group_means,\n             aes(x = gender, y = mean),\n             shape = 3,\n             size = 2)\n\n\n\n\n\n\n\n\nWhile the distribution looks similar, we notice that the median and the mean (marked by the cross inside the boxplot) are slightly higher for male participants. Thus, we can suspect some differences between these two groups, but we do not know whether these differences are significant or not. Therefore, to consider the significance (remember Chapter @ref(significance)) and the effect size (see Table @ref(tab:effect-size-cohen)), we have to perform statistical tests.\nTable @ref(tab:comparing-two-groups-unpaired) summarises the different tests and functions to perform the group comparison computationally. It is important to note that the parametric test compares the means of two groups, while the non-parametric test compares medians. All of these tests turn significant if the differences between groups are large enough. Thus, significant results can be read as ‘these groups are significantly different from each other’. Of course, if the test is not significant, the groups are considered to be not different. For parametric tests, i.e. t_test(), it is also essential to indicate whether the variances between these two groups are equal or not. Remember, the equality of variances was one of the assumptions for parametric tests. The Welch t-test can be used if the variances are not equal, but all other criteria for normality are met. By setting var.equal = TRUE, a regular T-Test would be performed. By default, t_test() assumes that variances are not equal. Make sure you test for homogeneity of variance before making your decision (see Chapter @ref(homogeneity-of-variance)).\n\nComparing two unpaired groups (effect size functions from package effectsize, except for wilcoxonR() from rcompanion {(#tbl-comparing-two-groups-unpaired)}\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction\nEffect size\nFunction\n\n\n\n\nParametric\nT-Test\nWelch T-Test\nt_test(var.equal = TRUE)\nt_test(var.equal = FALSE)\nCohen’s d\ncohens_d()\n\n\nNon-parametric\nMann-Whitney U\nwilcox_test(paired = FALSE)\nWilcoxon R\nwilcoxonR()\n\n\n\nWith this information in hand, we can start comparing the female Egyptians with the male ones using both the parametric and the non-parametric test for illustration purposes only. By setting detailed = TRUE, we can obtain the maximum amount of information for certain comparisons. In such cases, it is advisable to use glimpse(). This will make the output (a tibble) easier to read because each row presents one piece of information, rather than having one row with many columns.\n\n# T-Test\ncomp |&gt; t_test(freedom_of_choice ~ gender,\n                var.equal = TRUE,\n                detailed = TRUE) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 15\n$ estimate    &lt;dbl&gt; -0.6120414\n$ estimate1   &lt;dbl&gt; 6.212435\n$ estimate2   &lt;dbl&gt; 6.824477\n$ .y.         &lt;chr&gt; \"freedom_of_choice\"\n$ group1      &lt;chr&gt; \"female\"\n$ group2      &lt;chr&gt; \"male\"\n$ n1          &lt;int&gt; 579\n$ n2          &lt;int&gt; 621\n$ statistic   &lt;dbl&gt; -4.75515\n$ p           &lt;dbl&gt; 2.22e-06\n$ df          &lt;dbl&gt; 1198\n$ conf.low    &lt;dbl&gt; -0.864566\n$ conf.high   &lt;dbl&gt; -0.3595169\n$ method      &lt;chr&gt; \"T-test\"\n$ alternative &lt;chr&gt; \"two.sided\"\n\n\n\n# Welch t-test (var.equal = FALSE by default)\ncomp |&gt; t_test(freedom_of_choice ~ gender,\n                var.equal = FALSE,\n                detailed = TRUE) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 15\n$ estimate    &lt;dbl&gt; -0.6120414\n$ estimate1   &lt;dbl&gt; 6.212435\n$ estimate2   &lt;dbl&gt; 6.824477\n$ .y.         &lt;chr&gt; \"freedom_of_choice\"\n$ group1      &lt;chr&gt; \"female\"\n$ group2      &lt;chr&gt; \"male\"\n$ n1          &lt;int&gt; 579\n$ n2          &lt;int&gt; 621\n$ statistic   &lt;dbl&gt; -4.756287\n$ p           &lt;dbl&gt; 2.21e-06\n$ df          &lt;dbl&gt; 1193.222\n$ conf.low    &lt;dbl&gt; -0.8645067\n$ conf.high   &lt;dbl&gt; -0.3595762\n$ method      &lt;chr&gt; \"T-test\"\n$ alternative &lt;chr&gt; \"two.sided\"\n\n\n\n# Mann-Withney U test\ncomp |&gt;\n  wilcox_test(freedom_of_choice ~ gender,\n              detailed = TRUE) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 12\n$ estimate    &lt;dbl&gt; -0.999948\n$ .y.         &lt;chr&gt; \"freedom_of_choice\"\n$ group1      &lt;chr&gt; \"female\"\n$ group2      &lt;chr&gt; \"male\"\n$ n1          &lt;int&gt; 579\n$ n2          &lt;int&gt; 621\n$ statistic   &lt;dbl&gt; 149937.5\n$ p           &lt;dbl&gt; 4.97e-07\n$ conf.low    &lt;dbl&gt; -0.9999694\n$ conf.high   &lt;dbl&gt; -5.79946e-05\n$ method      &lt;chr&gt; \"Wilcoxon\"\n$ alternative &lt;chr&gt; \"two.sided\"\n\n\nYou might notice that the notation within the functions for group tests looks somewhat different to what we are used to, i.e. we use the ~ (‘tilde’) symbol. This is because some functions take a formula as their attribute, and to distinguish the dependent and independent variables from each other, we use ~. A more generic notation of how formulas in functions work is shown below, where DV stands for ‘dependent variable’ and IV stands for ‘independent variable’:\n\nfunction(formula = DV ~ IV)\n\nEven for multiple groups, group comparisons usually only have one independent variable, i.e. the grouping variable. Grouping variables are generally of the type factor. In the case of two groups, we have two levels present in this factor. For example, our variable gender contains two levels: female and male. If there are multiple groups, the factor comprises various levels, e.g. the variable country includes levels of 48 countries.\nNo matter which test we run, it appears as if the difference is significant. However, how big/important is the difference? The effect size provides the answer to this. The interpretation of the effect size follows the explanations in Chapter 10, where we looked at the strength of the correlation of two variables. However, different analytical techniques require different effect size measures, implying that we have to use different benchmarks. To help us with the interpretation, we can use the effectsize package and their set of interpret_*() functions (see also Indices of Effect Sizes). Sometimes, there are even more than one way of computing the effect size. For example, we can choose between the classic Wilcoxon R or the rank-biserial correlation coefficient for the Mann-Whitney test. In practice, you have to be explicit about how you computed the effect size. The differences between the two measures are often marginal and a matter of taste (or should I say: Your reviewers’ taste). Throughout this chapter, I will rely on the effect sizes most commonly found in Social Sciences publications. However, feel free to explore other indices as well, especially those offered in the effectsize package.\n\n# After parametric test\n(d &lt;-\n    comp |&gt;\n    cohens_d(freedom_of_choice ~ gender,\n             var.equal = TRUE,\n             ci = TRUE))\n\n# A tibble: 1 × 9\n  .y.             group1 group2 effsize    n1    n2 conf.low conf.high magnitude\n* &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;ord&gt;    \n1 freedom_of_cho… female male    -0.275   579   621    -0.39     -0.16 small    \n\n\n\neffectsize::interpret_cohens_d(d$effsize)\n\nCohen's d \n  \"small\" \n(Rules: cohen1988)\n\n\n\n# After non-parametric test\n(wr &lt;-\n    comp |&gt;\n    wilcox_effsize(freedom_of_choice ~ gender,\n                   ci = TRUE))\n\n# A tibble: 1 × 9\n  .y.             group1 group2 effsize    n1    n2 conf.low conf.high magnitude\n* &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;ord&gt;    \n1 freedom_of_cho… female male     0.145   579   621     0.09       0.2 small    \n\neffectsize::interpret_r(wr$effsize)\n\nEffect size (r) \n        \"small\" \n(Rules: funder2019)\n\n\nLooking at our test results, the female Egyptians perceive freedom_of_choice differently from their male counterparts. This is in line with our boxplots. However, the effect sizes tend to be small, which means the differences between the two groups is marginal. Similar to correlations, group comparisons need to be analysed in two stages, answering two questions:\n\nIs the difference between groups significant?\nIf it is significant, is the difference small, medium or large?\n\nCombining both analytical steps gives us a comprehensive answer to our research question and enables us to derive meaningful conclusions. This applies to all group comparisons covered in this book.\n\n\n12.2.2 Two paired groups\nSometimes, we are not interested in the difference between subjects, but within them, i.e. we want to know whether the same person provides similar or different responses at two different times. Thus, it becomes evident that observations need to be somehow linked to each other. Paired groups are frequently used in longitudinal and experimental studies (e.g. pre-test vs post-test). For example, if we look at our imdb_top_250 dataset, we can see that some directors have more than one movie in the top 250. Therefore, we could be curious to know whether earlier movies of directors have been significantly more successful than their later ones.\n\n# Number of movies directed by a particular person\nimdb_top_250 |&gt;\n  group_by(director) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n\n# A tibble: 155 × 2\n   director              n\n   &lt;fct&gt;             &lt;int&gt;\n 1 Christopher Nolan     7\n 2 Martin Scorsese       7\n 3 Stanley Kubrick       7\n 4 Akira Kurosawa        6\n 5 Alfred Hitchcock      6\n 6 Steven Spielberg      6\n 7 Billy Wilder          5\n 8 Charles Chaplin       5\n 9 Hayao Miyazaki        5\n10 Ingmar Bergman        5\n# ℹ 145 more rows\n\n\nFor this investigation, we use the modified dataset dir_mov, which only contains movies of directors who have two or more movies listed in the IMDb Top 250s. Where directors had more than two movies, I randomly sampled two movies. Thus, there is a certain limitation to our dataset.\nWe can use boxplots to compare earlier movies (i.e. 1) with later movies (i.e. 2) across all directors. Thus, each director is reflected in both groups with one of their movies. Therefore, the same directors can be found in each group. As a measure of success, we use the imdb_rating.\n\ndir_mov |&gt;\n  ggplot(aes(x = movie, y = imdb_rating, fill = movie)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplots look almost identical, suggesting that the ratings of movies in both groups have not changed significantly. However, the boxplot can only show a summary statistic for each group. Thus, it only implies that the movies in group 1 have about the same ratings as those in group 2. If we want to visualise how the ratings have changed for each director from the first to the second movie, we can create a point plot and draw lines with geom_line() to connect the movies in each group. A line that moves up indicates that the second movie was rated higher than the first one for a particular director and vice versa.\n\ndir_mov |&gt;\n  ggplot(aes(x = movie, y = imdb_rating, col = director)) +\n  geom_point() +\n  geom_line(aes(group = director)) +\n  \n  # Remove the legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nBased on this plot, we have to revise our interpretation slightly. Directors who received exceptionally high ratings on their first movie (i.e. the top 3 in group 1) scored much lower on the second movie. First, we can notice from our boxplots that these movies count as outliers, and second, obtaining such high scores on a movie is tough to replicate. Very highly rated movies are rare (unfortunately). Needless to say, all these movies are rated as very good; otherwise, they would not be on this list. It is worth noting that the way the y-axis is scaled emphasises differences. Thus, a difference between a rating of 9 and 8.5 appears large. If we change the range of the y-axis to ‘0-10’, the differences appear marginal, but it reflects (1) the possible length of the scale (IMDb ratings range from 1-10) and (2) the magnitude in change relative to the full scale. More often than not, this ‘zoom-in’ effect is sometimes used to create the illusion of significant differences where there are none. Be aware when you present your findings not to develop visualisations that could be misleading. Similarly, always pay attention to the axis in visualisations presented by others.\n\ndir_mov |&gt;\n  ggplot(aes(x = movie, y = imdb_rating, col = director)) +\n  geom_point() +\n  geom_line(aes(group = director)) +\n  \n  # Remove the legend\n  theme(legend.position = \"none\") +\n  \n  # Manuall define the y axis range\n  ylim(0, 10)\n\n\n\n\n\n\n\n\nConsidering the revised plot, we likely can predict what the statistical test will show, i.e. no significant results. Table @ref(tab:comparing-two-groups-paired) summarises which tests and functions need to be performed when our data is parametric or non-parametric. In both cases, the functions are the same as those of the unpaired group comparisons, but we need to add the attribute paired = TRUE. Still, the interpretations between the unpaired and paired tests remain the same. Also, be aware that some tests have changed in name, e.g. the Mann-Whitney U test has become the Wilcoxon Signed Rank Test. Even though we use the same functions as before, by changing the attribute paired to TRUE, we also change the computational technique to obtain the results. Thus, remember that the same function can perform different computations which are not comparable.\n\n\n\nTable 12.1: Comparing two paired groups (effect size functions from package effectsize, except for wilcoxonPairedR() from rcompanion)\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction\nEffect size\nFunction\n\n\n\n\nParametric\nT-Test\nt_test(paired = TRUE)\nCohen’s d\ncohens_d()\n\n\nNon-parametric\nWilcoxon Signed Rank Test\nwilcox_test(paired = TRUE)\nWilcoxon r\nwilcoxonPairedR()\n\n\n\n\n\n\nLet’s apply these functions to find out whether the differences we can see in our plots matter.\n\n# Paired T-Test\ndir_mov |&gt; t_test(imdb_rating ~ movie,\n                   paired = TRUE,\n                   var.equal = TRUE,\n                   detailed = TRUE) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 13\n$ estimate    &lt;dbl&gt; 0.04186047\n$ .y.         &lt;chr&gt; \"imdb_rating\"\n$ group1      &lt;chr&gt; \"1\"\n$ group2      &lt;chr&gt; \"2\"\n$ n1          &lt;int&gt; 43\n$ n2          &lt;int&gt; 43\n$ statistic   &lt;dbl&gt; 0.8717132\n$ p           &lt;dbl&gt; 0.388\n$ df          &lt;dbl&gt; 42\n$ conf.low    &lt;dbl&gt; -0.05504967\n$ conf.high   &lt;dbl&gt; 0.1387706\n$ method      &lt;chr&gt; \"T-test\"\n$ alternative &lt;chr&gt; \"two.sided\"\n\n\n\n# Wilcoxon Signed Rank Test\ndir_mov |&gt; wilcox_test(imdb_rating ~ movie,\n                         paired = TRUE) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 7\n$ .y.       &lt;chr&gt; \"imdb_rating\"\n$ group1    &lt;chr&gt; \"1\"\n$ group2    &lt;chr&gt; \"2\"\n$ n1        &lt;int&gt; 43\n$ n2        &lt;int&gt; 43\n$ statistic &lt;dbl&gt; 278\n$ p         &lt;dbl&gt; 0.562\n\n\nAs expected, the paired tests reveal that the differences in rating between the first and second movies are not significant. Usually, there would be no reason to follow this up with the computation of effect sizes because we only need to do this if the differences are statistically significant. However, nothing can stop us from doing so (for demonstration purposes).\n\n## After T-Test\nd &lt;- cohens_d(imdb_rating ~ movie,\n               data = dir_mov,\n               paired = TRUE,\n               var.equal = TRUE)\n\nglimpse(d)\n\nRows: 1\nColumns: 7\n$ .y.       &lt;chr&gt; \"imdb_rating\"\n$ group1    &lt;chr&gt; \"1\"\n$ group2    &lt;chr&gt; \"2\"\n$ effsize   &lt;dbl&gt; 0.132935\n$ n1        &lt;int&gt; 43\n$ n2        &lt;int&gt; 43\n$ magnitude &lt;ord&gt; negligible\n\n\n\neffectsize::interpret_cohens_d(d$effsize)\n\n   Cohen's d \n\"very small\" \n(Rules: cohen1988)\n\n\n\n# After Wilcoxon Signed Rank Test\nwr &lt;-\n  dir_mov |&gt;\n  wilcox_effsize(imdb_rating ~ movie,\n                   paired = TRUE, ci = TRUE)\n\nglimpse(wr)\n\nRows: 1\nColumns: 9\n$ .y.       &lt;chr&gt; \"imdb_rating\"\n$ group1    &lt;chr&gt; \"1\"\n$ group2    &lt;chr&gt; \"2\"\n$ effsize   &lt;dbl&gt; 0.04477211\n$ n1        &lt;int&gt; 43\n$ n2        &lt;int&gt; 43\n$ conf.low  &lt;dbl&gt; 0.0047\n$ conf.high &lt;dbl&gt; 0.35\n$ magnitude &lt;ord&gt; small\n\n\n\neffectsize::interpret_r(wr$effsize, rules = \"cohen1988\")\n\nEffect size (r) \n   \"very small\" \n(Rules: cohen1988)\n\n\nAs expected, the effect sizes are tiny, irrespective of whether we treat our data as parametric or non-parametric. After all, being a successful director ranked in the IMDb top 250 seems to imply that other movies are equally successful, but remember the limitations of our dataset before drawing your final conclusions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#sec-comparing-more-than-two-groups",
    "href": "12_group_comparison.html#sec-comparing-more-than-two-groups",
    "title": "12  Comparing groups",
    "section": "12.3 Comparing more than two groups",
    "text": "12.3 Comparing more than two groups\nOften we find ourselves in situations where comparing two groups is not enough. Instead, we might be faced with three or more groups reasonably quickly. For example, the wvs_nona dataset allows us to look at 48 different countries, all of which we could compare very effectively with just a few lines of code. In the following chapters, we look at how we can perform the same type of analysis as before, but with multiple unpaired and paired groups using R. Similarly to the two-samples group comparison, we cover the parametric and non-parametric approaches.\n\n12.3.1 Multiple unpaired groups\nHave you ever been wondering whether people in different countries are equally satisfied with their lives? You might have a rough guess that it is not the case because the social, economic and political environment might play an important role. For example, if you live in a country affected by social conflicts, one’s life satisfaction might be drastically lower. In the following, we take a look at three countries Iraq, Japan and Korea. I chose these countries out of personal interest and because they nicely demonstrate the purpose of the chapter, i.e. finding out whether there are differences in the perception of satisfaction across three countries. At any time, feel free to remove the filter() function to gain the results of all countries in the dataset, but prepare for slightly longer computation times. We first create the dataset, which only contains the three desired countries.\n\nmcomp &lt;-\n  wvs_nona |&gt;\n  filter(country == \"Iraq\" |\n           country == \"Japan\" |\n           country == \"Korea\")\n\nSimilar to before, we can use the ggridges package to draw density plots for each group. This has the added benefit that we can compare the distribution of data for each group and see whether the assumption of normality is likely met or not. On the other hand, we lose the option to identify any outliers quickly. You win some, and you lose some.\n\nmcomp |&gt;\n  group_by(country) |&gt;\n  ggplot(aes(x = satisfaction,\n             y = reorder(country, satisfaction),\n             fill = country)) +\n  ggridges::stat_density_ridges(bandwidth = 0.6,\n                                quantile_lines = TRUE,   # adds median indicator\n                                quantiles = (0.5)) +\n  # Remove legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe plot shows us that Japan and Korea appear to be very similar, if not identical (based on the median), but Iraq appears to be different from the other two groups. When performing a multiple group comparison, we can follow similar steps as before with two groups, i.e.\n\nperform the comparison,\ndetermine the effect size, and\ninterpret the effect size.\n\nTable @ref(tab:comparing-multiple-groups-unpaired) summarises which test needs to be chosen to compare multiple unpaired groups and their corresponding effect size measures.\n\n\n\nTable 12.2: Comparing multiple unpaired groups (effect size functions from package effectsize)\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction for test\nEffect size\nFunction for effect size1\n\n\n\n\nParametric\nANOVA\n\nanova_test (assumes equal variances)\noneway.test(var.equal = TRUE/FALSE)\n\nEta squared\neta_squared()\n\n\nNon-parametric\nKruskall-Wallis test\nkruskal_test()\nEpsilon squared (rank)\nrank_epsilon_squared()\n\n\n\n\n\n\nLet’s begin by conducting the group comparison. As you will notice, rstatix currently does not support a parametric test where var.equal = FALSE. Therefore we need to fall back to the underlying function oneway.test(var.equal = FALSE)\n\n# ANOVA\n## equal variances assumed\nmcomp |&gt;\n  anova_test(satisfaction ~ country,\n              detailed = TRUE)\n\nANOVA Table (type II tests)\n\n   Effect      SSn      SSd DFn  DFd       F         p p&lt;.05   ges\n1 country 4258.329 11314.68   2 3795 714.133 5.74e-264     * 0.273\n\n\n\n## Equal variances not assumed\n(oneway_test &lt;- oneway.test(satisfaction ~ country,\n                            data = mcomp,\n                            var.equal = FALSE))\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  satisfaction and country\nF = 663.17, num df = 2.0, denom df = 2422.8, p-value &lt; 2.2e-16\n\n\n\n# Kruskall-Wallis test\nmcomp |&gt; kruskal_test(satisfaction ~ country)\n\n# A tibble: 1 × 6\n  .y.              n statistic    df         p method        \n* &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;         \n1 satisfaction  3798     1064.     2 1.11e-231 Kruskal-Wallis\n\n\nWhile anova_test() does provide the effect size automatically, i.e. generalised eta squared (ges), this is not the case for the other two approaches. Therefore, we have to use the effectsize package to help us out. Packages often can get you a long way and make your life easier, but it is good to know alternatives if a single package does not give you what you need.\n\n# After ANOVA with var.equal = FALSE\neffectsize::eta_squared(oneway_test)\n\n# Effect Size for ANOVA\n\nEta2 |       95% CI\n-------------------\n0.35 | [0.33, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\n# effect size rank epsilon squared\neffectsize::rank_epsilon_squared(mcomp$satisfaction ~ mcomp$gender)\n\nEpsilon2 (rank) |       95% CI\n------------------------------\n2.12e-03        | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe results show that there is a significant and large difference between these groups. You might argue that this is not quite true. Considering our plot, we know that Japan and Korea do not look as if they are significantly different. Multiple group comparisons only consider differences across all three groups. Therefore, if one group differs from the other groups, the test will turn significant and even provide a large enough effect size to consider it essential. However, these tests do not provide information on which differences between groups are significant. To gain more clarification about this, we need to incorporate another step called ‘post-hoc tests’. These tests compare two groups at a time, which is why they are also known as ‘pairwise comparisons’. Compared to regular two-sample tests, these perform corrections of the p-values for multiple testing, which is necessary. However, there are many different ‘post-hoc’ tests you can choose. Field (2013) (p.459) nicely outlines the different scenarios and provides recommendations to navigate this slightly complex field of post-hoc tests. Table @ref(tab:post-hoc-tests) provides an overview of his suggestions.\n\n\n\nTable 12.3: Different post-hoc tests for different scenarios (parametric)\n\n\n\n\n\nEqual sample size\nEqual variances\nPost-hot tests\nFunctions in R\n\n\n\n\nYES\nYES\n\nREGWQ,\nTukey,\nBonferroni\n\n\nmutoss::regwq()2\nrstatix::tukey_hsd()\npairwise.t.test(p.adjust.method = \"bonferroni\")\n\n\n\nNO (slightly different)\nYES\n\nGabriel\n\n\n\n\nYES\nYES\n\nHochberg’s GT2\n\n\nNot available in R and should not be confused with pairwise.t.test(p.adjust.method = \"hochberg\"), which is based on Hochberg (1988). The GT2, however, is based on Hochberg (1974).\n\n\n\nNO (not ideal for small samples)\nNO\n\nGames-Howell\n\n\nrstatix::games_howell_test()\n\n\n\n\n\n\n\nYou might be surprised to see that there are also post-hoc tests for parametric group comparisons when equal variances are not assumed. Would we not have to use a non-parametric test for our group comparison instead? Well, empirical studies have demonstrated that ANOVAs tend to produce robust results, even if the assumption of normality (e.g. Blanca Mena et al. 2017) is not given, or there is some degree of heterogeneity of variance between groups (Tomarken and Serlin 1986). In other words, there can be some lenieancy (or flexibility?) when it comes to the violation of parametric assumptions. If you want to reside on the save side, you should ensure you know your data and its properties. If in doubt, non-parametric tests are also available.\nIf we want to follow up the Kruskal-Wallis test, i.e. the non-parametric equivalent to the one-way ANOVA, we can make use of two post-hoc tests:\n\nDunn Test: rstatix::dunn_test() (Dinno 2015)\nPairwise comparison with Bonferroni (and other) correction: pairwise.wilcox.test().\n\nBelow are some examples of how you would use these functions in your project. However, be aware that some of the post-hoc tests are not well implemented yet in R. Here, I show the most important ones that likely serve you in 95% of the cases.\n\n# POST-HOC TEST FOR PARAMETRIC DATA\n# Bonferroni\npairwise.t.test(mcomp$satisfaction,\n                mcomp$country,\n                p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  mcomp$satisfaction and mcomp$country \n\n      Iraq   Japan\nJapan &lt;2e-16 -    \nKorea &lt;2e-16 1    \n\nP value adjustment method: bonferroni \n\n\n\n# Tukey\nmcomp |&gt; tukey_hsd(satisfaction ~ country)\n\n# A tibble: 3 × 9\n  term    group1 group2 null.value estimate conf.low conf.high        p.adj\n* &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 country Iraq   Japan           0   2.29      2.13      2.45  0.0000000141\n2 country Iraq   Korea           0   2.26      2.10      2.43  0.0000000141\n3 country Japan  Korea           0  -0.0299   -0.189     0.129 0.898       \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\n\n# Games-Howell\nmcomp |&gt; games_howell_test(satisfaction ~ country)\n\n# A tibble: 3 × 8\n  .y.          group1 group2 estimate conf.low conf.high    p.adj p.adj.signif\n* &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 satisfaction Iraq   Japan    2.29      2.11      2.47  0        ****        \n2 satisfaction Iraq   Korea    2.26      2.11      2.42  5.71e-11 ****        \n3 satisfaction Japan  Korea   -0.0299   -0.178     0.118 8.84e- 1 ns          \n\n\n\n# POST-HOC TEST FOR NON-PARAMETRIC DATA\nmcomp |&gt; dunn_test(satisfaction ~ country)\n\n# A tibble: 3 × 9\n  .y.       group1 group2    n1    n2 statistic         p     p.adj p.adj.signif\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       \n1 satisfac… Iraq   Japan   1200  1353     29.2  4.16e-187 1.25e-186 ****        \n2 satisfac… Iraq   Korea   1200  1245     27.6  8.30e-168 1.66e-167 ****        \n3 satisfac… Japan  Korea   1353  1245     -1.02 3.10e-  1 3.10e-  1 ns          \n\n\n\n# or\npairwise.wilcox.test(mcomp$satisfaction,\n                     mcomp$country,\n                     p.adjust.method = \"holm\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  mcomp$satisfaction and mcomp$country \n\n      Iraq    Japan  \nJapan &lt; 2e-16 -      \nKorea &lt; 2e-16 0.00093\n\nP value adjustment method: holm \n\n\nAs we can see, no matter which function we use, the interpretation of the results remain the same on this occasion. This is, of course reassuring to know, because it implies that the method chosen does not change the outcome.\n\n\n12.3.2 Multiple paired groups\nWhen comparing multiple paired groups, matters become slightly more complicated. On the one hand, our groups are not really groups anymore because our data refer to the same group of people, usually over an extended period of time. In experimental studies, this can also refer to different ‘treatments’ or ‘conditions’. This is similar to comparing two paired groups. On the other hand, we also have to deal with yet another assumption: Sphericity.\n\n12.3.2.1 Testing the assumption of sphericity\nSphericity assumes that the variance of covariate pairs (i.e. any combination of the groups/treatments) are roughly equal. This might sound familiar to the assumption of homogeneity of variance in between-subject ANOVAs, only that we look at differences between pairs and not between two different participant groups. Thus, it seems less surprising that the parametric test to compare multiple paired groups is also called ‘repeated measures ANOVA’.\nTo illustrate the concept of sphericity, let’s look at an example. Assume we conduct a longitudinal study that involves five university students who started their studies in a foreign country. We ask them to complete a survey that tests their level of integration into the local community at three points in time: month 1 (m1), month 4 (m4) and month 8 (m8). This gives us the following data set:\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.double), round, 0)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\nacculturation\n\n# A tibble: 5 × 4\n  name        m1    m4    m8\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Waylene      2     3     5\n2 Nicole       1     3     6\n3 Mikayla      2     3     5\n4 Valeria      1     3     5\n5 Giavanni     1     3     5\n\n\nIf each month measured a different group of participants, we would compare the differences between each month regarding homogeneity. However, since we look at paired data, we need to consider the differences in pairs of measures. In this study, the following combinations are possible:\n\nm1 and m4\nm4 and m8\nm1 and m8\n\nLet’s add the differences between measures for each participant based on these pairs and compute the variances across these values using the function var().\n\n# Compute the differences across all three pairs of measurements\ndifferences &lt;-\n  acculturation |&gt;\n  mutate(m1_m4 = m1 - m4,\n         m4_m8 = m4 - m8,\n         m1_m8 = m1 - m8)\n\ndifferences\n\n# A tibble: 5 × 7\n  name        m1    m4    m8 m1_m4 m4_m8 m1_m8\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Waylene      2     3     5    -1    -2    -3\n2 Nicole       1     3     6    -2    -3    -5\n3 Mikayla      2     3     5    -1    -2    -3\n4 Valeria      1     3     5    -2    -2    -4\n5 Giavanni     1     3     5    -2    -2    -4\n\n\n\n# Compute the variance for each pair\ndifferences |&gt;\n  summarise(m1_m4_var = var(m1_m4),\n            m4_m8_var = var(m4_m8),\n            m1_m8_var = var(m1_m8))\n\n# A tibble: 1 × 3\n  m1_m4_var m4_m8_var m1_m8_var\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1       0.3       0.2       0.7\n\n\nWe can tell that the differences across groups are relatively small when comparing m1_m4_var and m4_m8_var. However, the value for m1_m8_var appears bigger. So, how can we be sure whether the assumption of sphericity is violated or not? Similar to many of the other assumptions we covered, a significance test provides the answer to this question. For multiple paired groups, we use Mauchly’s Test of Sphericity. If the test is significant, the variances across groups are not equal. In other words, a significant Mauchly test implies a violation of sphericity. The rstatix package includes Mauchly’s Test of Sphericity in its anova_test(). Thus, we get both with one function. However, our data in this example is not tidy (remember the definition from Chapter @ref(inspecting-raw-data)), because there are multiple observation per row for each individual and the same variable. Instead, we need to ensure that each observation for a given variable has its own row. In short, we first need to convert it into a tidy dataset using the function pivot_longer(). Feel free to ignore this part for now, because we will cover pivoting datasets in greater detail in Chapter @ref(chi-squared-test).\n\n# Convert into tidy data\nacc_long &lt;-\n  acculturation |&gt;\n  pivot_longer(cols = c(m1, m4, m8),\n               names_to = \"month\",\n               values_to = \"integration\")\n\nacc_long\n\n# A tibble: 15 × 3\n   name     month integration\n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;\n 1 Waylene  m1              2\n 2 Waylene  m4              3\n 3 Waylene  m8              5\n 4 Nicole   m1              1\n 5 Nicole   m4              3\n 6 Nicole   m8              6\n 7 Mikayla  m1              2\n 8 Mikayla  m4              3\n 9 Mikayla  m8              5\n10 Valeria  m1              1\n11 Valeria  m4              3\n12 Valeria  m8              5\n13 Giavanni m1              1\n14 Giavanni m4              3\n15 Giavanni m8              5\n\n\n\n# Perform repeated-measures ANOVA\n# plus Mauchly's Test of Sphericity\nacc_long |&gt;\n  rstatix::anova_test(dv = integration,\n                      wid = name,\n                      within = month,\n                      detailed = TRUE)\n\nANOVA Table (type III tests)\n\n$ANOVA\n       Effect DFn DFd   SSn SSd    F        p p&lt;.05   ges\n1 (Intercept)   1   4 153.6 0.4 1536 2.53e-06     * 0.987\n2       month   2   8  36.4 1.6   91 3.14e-06     * 0.948\n\n$`Mauchly's Test for Sphericity`\n  Effect     W     p p&lt;.05\n1  month 0.417 0.269      \n\n$`Sphericity Corrections`\n  Effect   GGe     DF[GG]    p[GG] p[GG]&lt;.05   HFe     DF[HF] p[HF] p[HF]&lt;.05\n1  month 0.632 1.26, 5.05 0.000162         * 0.788 1.58, 6.31 3e-05         *\n\n\nNot only do the results show that the differences across measures are significant, but that our Mauchly’s Test of Sphericity is not significant. Good news on all fronts.\nThe output from anova_test() also provides information about Sphericity Corrections. We need to consider these scores if the sphericity assumption is violated, i.e. Mauchly’s Test of Sphericity is significant. We commonly use two methods to correct the degrees of freedom in our analysis which will change the p-value of our tests: Greenhouse-Geisser correction (Greenhouse and Geisser 1959) and Huynh-Field correction (Huynh and Feldt 1976). Field (2013) explains that if the Greenhouse-Geisser estimate (i.e. GGe) is greater than 0.74, it is advisable to use the Huynh-Field correction instead. For each correction, the anova_test() provides corrected p-values. In our example, we do not have to worry about corrections, though. Still, it is very convenient that this function delivers the corrected results as well.\n\n\n12.3.2.2 Visualising and computing multiple paired group comparisons\nSo far, we blindly assumed that our data are parametric. If our assumptions for parametric tests are violated, we can draw on a non-parametric equivalent called Friedman Test. Table @ref(tab:comparing-multiple-groups-paired) summarises the parametric and non-parametric tests as well as their respective effect sizes.\n\n\n\nTable 12.4: Comparing multiple paired groups (effect size functions from package effectsize)\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction\nEffect size\nFunction\n\n\n\n\nParametric\nRepeated measures ANOVA\nanova_test()\nEta squared\neta_squared()\n\n\nNon-parametric\nFriedman Test\nfriedman_test()\nKendall’s R\nkendalls_w()\n\n\n\n\n\n\nInstead of using a fictional example as we did in the previous chapter, let’s draw on real observations using the dataset wvs_waves. This dataset contains similar data to wvs but offers multiple measures as indicated by the variable wave for several countries. For example, we might be interested to know whether satisfaction changed over the years. This time, our unit of analysis is not individuals, but countries. Therefore, we compare the same countries (not individuals) over time.\nLet’s start by visualising the data across all time periods using geom_boxplot(). I also added the mean for each wave since this is what we will eventually compare in our repeated-measures ANOVA.\n\n# Compute the mean for each wave\nwave_means &lt;-\n  wvs_waves |&gt;\n  group_by(wave) |&gt;\n  summarise(w_mean = mean(satisfaction))\n\nwave_means\n\n# A tibble: 7 × 2\n  wave  w_mean\n  &lt;fct&gt;  &lt;dbl&gt;\n1 w1      7.14\n2 w2      6.83\n3 w3      7.11\n4 w4      7.46\n5 w5      7.54\n6 w6      7.44\n7 w7      7.60\n\n\n\n# Plot the boxplots\nwvs_waves |&gt;\n  ggplot(aes(x = wave,\n             y = satisfaction,\n             fill = wave)) +\n  geom_boxplot() +\n\n  # Add the means\n  geom_point(data = wave_means,\n             aes(x = wave,\n                 y = w_mean),\n             shape = 4)\n\n\n\n\n\n\n\n\nIn total, we plotted seven boxplots that represent each wave of data collection. We can tell that satisfaction with life has improved slightly, especially from wave 3 (w3) to wave 4 (w4).\nNext, we would have to check the assumptions for parametric tests. Since we have covered this in Chapter @ref(sources-of-bias), we only consider Mauchly’s Test of Sphericity by running anova_test().\n\n# Compute repeated measures ANOVA and\n# Mauchly's Test of Sphericity\nwvs_waves |&gt;\n  rstatix::anova_test(dv = satisfaction,\n                      wid = id,\n                      within = wave)\n\nANOVA Table (type III tests)\n\n$ANOVA\n  Effect DFn  DFd     F        p p&lt;.05   ges\n1   wave   6 1794 5.982 3.33e-06     * 0.015\n\n$`Mauchly's Test for Sphericity`\n  Effect     W     p p&lt;.05\n1   wave 0.903 0.067      \n\n$`Sphericity Corrections`\n  Effect   GGe        DF[GG]    p[GG] p[GG]&lt;.05   HFe        DF[HF]   p[HF]\n1   wave 0.968 5.81, 1736.38 4.57e-06         * 0.989 5.94, 1774.66 3.7e-06\n  p[HF]&lt;.05\n1         *\n\n\nWe first look at the sphericity assumption which we did not violate, i.e. \\(p &gt; 0.05\\). Thus, using the ANOVA test is appropriate. Next, we inspect the ANOVA test results and find that the differences are significant as well, i.e. \\(p &lt; 0.05\\). Still, the effect size ges is small.\nFrom our plot, we know that some of the waves are fairly similar, and we need to determine which pair of waves are significantly different from each other. Therefore, we need a post-hoc test that allows us to perform pairwise comparisons. By now, this should sound familiar (see also Table 12.3).\nSince we assume our data is parametric and the groups are equally large for each wave (\\(n = 300\\)), we can use T-Tests with a Bonferroni correction.\n\npairwise.t.test(wvs_waves$satisfaction,\n                wvs_waves$wave,\n                p.adjust.method = \"bonferroni\",\n                paired = TRUE)\n\n\n    Pairwise comparisons using paired t tests \n\ndata:  wvs_waves$satisfaction and wvs_waves$wave \n\n   w1      w2      w3      w4      w5      w6     \nw2 1.00000 -       -       -       -       -      \nw3 1.00000 1.00000 -       -       -       -      \nw4 1.00000 0.01347 0.78355 -       -       -      \nw5 0.30433 0.00033 0.11294 1.00000 -       -      \nw6 1.00000 0.00547 0.68163 1.00000 1.00000 -      \nw7 0.05219 0.00023 0.03830 1.00000 1.00000 1.00000\n\nP value adjustment method: bonferroni \n\n\nBased on these insights, we find that mainly w2 shows significant differences with other waves. This is not a coincidence because w2 has the lowest mean of all waves. Similarly, w7, which reports the highest mean for satisfaction, also reports several significant differences with other groups.\nGiven the above, we can confirm that our people’s satisfaction with life in each country has changed positively, but the change is minimal (statistically).\nTo finish this chapter, I want to share the non-parametric version of this test: the Friedman test and its function friedman_test() from the rstatix package.\n\n# NON-PARAMETRIC COMPARISON\nwvs_waves |&gt; rstatix::friedman_test(satisfaction ~ wave | id)\n\n# A tibble: 1 × 6\n  .y.              n statistic    df            p method       \n* &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        \n1 satisfaction   300      43.4     6 0.0000000966 Friedman test\n\n\n\n# Effect size\n(kw &lt;- effectsize::kendalls_w(satisfaction ~ wave | id,\n                       data = wvs_waves))\n\nKendall's W |       95% CI\n--------------------------\n0.02        | [0.02, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::interpret_kendalls_w(kw$Kendalls_W)\n\n[1] \"slight agreement\"\n(Rules: landis1977)\n\n\nThe non-parametric test confirms the parametric test from before. However, the interpretation of the effect size Kendall's W requires some explanation. First, the effect size for Kendall’s W can range from 0 (small effect size) to 1 (large effect size). Usually, this effect size is considered when comparing inter-rater agreement, for example, the level of agreement of a jury in ice skating during the Olympic games. Thus, the interpretation offered by the function interpret_kendalls_w() follows this logic. As such, it is worth considering a slightly amended interpretation of Kendall’s W if our data is not based on ratings. This is shown in Table 12.5 based on Landis and Koch (1977) (p. 165). Thus, we could report that the effect size of the significant differences is ‘very small’.\n\n\n\nTable 12.5: Interpretation benchmarks for the effect size Kendall’s W\n\n\n\n\n\n\n\n\n\n\nKendall’s W\nInterpretation (Landis and Koch 1977)\nAlternative interpretation\n\n\n\n\n0.00-0.02\nSlight agreement\nVery small\n\n\n0.21-0.40\nFair agreement\nSmall\n\n\n0.41-0.60\nModerate agreement\nModerate\n\n\n0.61-0.80\nSubstantial agreement\nLarge\n\n\n0.81-1.00\nAlmost perfect agreement\nVery large",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#sec-chi-squared-test",
    "href": "12_group_comparison.html#sec-chi-squared-test",
    "title": "12  Comparing groups",
    "section": "12.4 Comparing groups based on factors: Contingency tables",
    "text": "12.4 Comparing groups based on factors: Contingency tables\nSo far, our dependent variable was always a numeric one. However, what if both independent and dependent variables are categorical? In such cases, we can only count the number of occurrences for each combination of the categories.\nFor example, we might recode our previous dependent variable satisfaction into a dichotomous one, i.e. participants are either happy or not. This is also known as a binary/logical variable (see Chapter @ref(change-data-types)). Since the original scale ranges from 1-10, we assume that participants who scored higher than 5 are satisfied with their lives, while those who scored lower are categorised as unsatisfied.\n\n# Create a dichotomous/binary variable for satisfaction\nwvs_nona &lt;-\n  wvs_nona |&gt;\n  mutate(satisfaction_bin = as_factor(ifelse(satisfaction &gt; 5,\n                                             \"satisfied\",\n                                             \"unsatisfied\"))\n         )\n\nYour first intuition is likely to ask: Are there more satisfied or unsatisfied people in my sample?\n\nwvs_nona |&gt; count(satisfaction_bin)\n\n# A tibble: 2 × 2\n  satisfaction_bin     n\n  &lt;fct&gt;            &lt;int&gt;\n1 satisfied        52587\n2 unsatisfied      16991\n\n\nThe results reveal that considerably more people are satisfied with their life than there are unsatisfied people. In the spirit of group comparisons, we might wonder whether gender differences might exist among the satisfied and unsatisfied group of people. Thus, we want to split the satisfied and unsatisfied responses into male and female groups. We can do this by adding a second argument to the function count().\n\nwvs_nona |&gt; count(satisfaction_bin, gender)\n\n# A tibble: 4 × 3\n  satisfaction_bin gender     n\n  &lt;fct&gt;            &lt;fct&gt;  &lt;int&gt;\n1 satisfied        female 27677\n2 satisfied        male   24910\n3 unsatisfied      female  8833\n4 unsatisfied      male    8158\n\n\nThis tibble reveals that there are more female participants who are satisfied than male ones. However, the same is true for the category unsatisfied. Using absolute values is not very meaningful when the sample sizes of each group are not equal. Thus, it is better to use the relative frequency instead and adding it as a new variable.\n\nct &lt;-\n  wvs_nona |&gt;\n  count(satisfaction_bin, gender) |&gt;\n  group_by(gender) |&gt;\n  mutate(perc = round(n / sum(n), 3)) # compute the percentages\n\nct\n\n# A tibble: 4 × 4\n# Groups:   gender [2]\n  satisfaction_bin gender     n  perc\n  &lt;fct&gt;            &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 satisfied        female 27677 0.758\n2 satisfied        male   24910 0.753\n3 unsatisfied      female  8833 0.242\n4 unsatisfied      male    8158 0.247\n\n\nThe relative frequency (perc) reveals that female and male participants are equally satisfied and unsatisfied. In other words, we could argue that gender does not explain satisfaction with life because the proportion of male and female participants is almost identical.\nA more common and compact way to show such dependencies between categorical variables is a contingency table. To convert our current table into a contingency table, we need to map the levels of satisfaction_bin as rows (i.e. satisfied and unsatisfied), and for gender, we want each level represented as a column (i.e. male and female). This can be achieved with the function pivot_wider(). It turns our ‘long’ data frame into a ‘wide’ one. Therefore, we basically perform the opposite of what the function pivot_longer() did earlier. However, this means that our data is not ‘tidy’ anymore. Let’s take a look at the output first to understand better what we try to achieve.\n\nct |&gt; pivot_wider(id_cols = satisfaction_bin,\n                   names_from = gender,\n                   values_from = perc)\n\n# A tibble: 2 × 3\n  satisfaction_bin female  male\n  &lt;fct&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 satisfied         0.758 0.753\n2 unsatisfied       0.242 0.247\n\n\nThe resulting tibble looks like a table as we know it from Excel. It is much more compact than the long format. However, what are the different arguments I passed to the function pivot_wider()? Here is a more detailed explanation of what we just did:\n\nid_cols refers to one or more columns that define the groups for each row. In our case, we wanted to group observations based on whether these reflect the state of satisfied or unsatisfied. Thus, each group is only represented once in this column. The underlying concept is comparable to using the function group_by() together with summarise(), which happens to be the same as using count().\nnames_from defines which variable should be translated into separate columns. In other words, we replace the column gender and create a new column for each level of the factor, i.e. male and female.\nvalues_from requires us to specify which variable holds the values that should be entered into our table. Since we want our percentages included in the final output, we use perc.\n\nIt is essential to note that we did not change any values but rearranged them. However, we lost one variable, i.e. n. Where has it gone? When using pivot_wider() we have to make sure we include all variables of interest. By default, the function will drop any other variables not mentioned. If we want to keep n, we can include it as another variable that is added to values_from.\n\nct |&gt; pivot_wider(id_cols = satisfaction_bin,\n                   names_from = gender,\n                   values_from = c(perc, n))\n\n# A tibble: 2 × 5\n  satisfaction_bin perc_female perc_male n_female n_male\n  &lt;fct&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;  &lt;int&gt;\n1 satisfied              0.758     0.753    27677  24910\n2 unsatisfied            0.242     0.247     8833   8158\n\n\nThe table has become even wider because we have two more columns to show the absolute frequency per gender and satisfaction_bin.\nAs you already know, there are situations where we want to turn a ‘wide’ data frame into a ‘long’ one. We performed such a step earlier and there is an in-depth example provided in Chapter @ref(reviewing-the-outliers). Knowing how to pivot dataset is an essential data wrangling skill and something you should definitely practice to perfection.\nContingency tables are like plots: They provide an excellent overview of our data structure and relationships between variables. However, they provide no certainty about the strength of relationships. Therefore, we need to draw on a set of statistical tests to gain further insights.\nSimilar to previous group comparisons, we can distinguish between paired and unpaired groups. I will cover each comparison in turn and allude to plotting two or more categories in a ggplot().\n\n12.4.1 Unpaired groups of categorical variables\nIn the introduction to this chapter, we covered a classic example of an unpaired comparison of two groups (male and female) regarding another categorical variable, i.e. satisfaction_bin. The tables we produced offered detailed insights into the distribution of these categories. However, it is always helpful to have a visual representation. Plotting two categorical variables (i.e. factors) in ggplot2 can be achieved in many different ways. Three commonly used options are shown in Figure @ref(fig:three-plots-satisfaction-gender).\n\n# Plot with absolute frequencies (stacked)\nabsolute_stacked &lt;-\n  wvs_nona |&gt;\n  ggplot(aes(x = satisfaction_bin,\n             fill = gender)) +\n  geom_bar() +\n  ggtitle(\"Stacked absolute frequency\") +\n  theme(legend.position = \"none\")\n\n# Plot with absolute frequencies (grouped)\nabsolute_grouped &lt;-\n  wvs_nona |&gt;\n  ggplot(aes(x = satisfaction_bin,\n             fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  ggtitle(\"Grouped absolute frequency\") +\n  theme(legend.position = \"none\")\n\n# Plot with relative frequencies\nrelative &lt;-\n  wvs_nona |&gt;\n  ggplot(aes(x = satisfaction_bin,\n             fill = gender)) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Relative frequency\") +\n  theme(legend.position = \"none\")\n\n# Plot all three plots with 'patchwork' package\nabsolute_stacked + absolute_grouped + relative + plot_spacer()\n\n\n\n\n\n\n\nFigure 12.1: Three ways to plot frequencies.\n\n\n\n\n\nA more elegant and compact way of visualising frequencies across two or more categories are mosaic plots. These visualise relative frequencies for both variables in one plot. For example, consider the following mosaic plot created with the package ggmosaic and the function geom_mosaic().\n\nlibrary(ggmosaic)\nwvs_nona |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(satisfaction_bin, gender),\n                            fill = gender)) +\n  theme_mosaic()\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nℹ Please use `unite()` instead.\nℹ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\nFigure 12.2: A mosaic plot which visualises the relationship of two categorical variables.\n\n\n\n\n\nIt might not be evident from our data, but frequencies of each variable determine the bars’ height and width. In other words, the width of the bars (i.e. x-axis) is determined by the relative frequency of female and male participants in our sample. On the other hand, the height of each bar (i.e. the y-axis) is specified by the relative frequency of satisfied and unsatisfied. This results in a square block that is divided by the respective relative distributions. Let me share an example where it is more apparent what a mosaic plot aims to achieve.\n\ndata |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(married, gender),\n                  fill = married)) +\n  theme_mosaic()\n\n\n\n\n\n\n\n\nIn this example, we can tell that there were more male participants than females because the bar for male is much wider. Apart from that, we notice that female participants have more missing values NA for the variable married. However, more female participants reported that they are married, i.e. answered with yes. While mosaic plots make for impressive visualisations, we must be mindful that more complex visualisations are always more challenging to understand. Thus, it is wise to plan the usage of such plots carefully depending on your audience.\nThroughout the remaining chapters, I will use the mosaic plot to illustrate distributions. The main difference to regular bar plots is the function product(), which allows us to define different variables of interest instead of using x and y in the aes() of ggplot(). It is this function that enables a mosaic visualisation. Apart from that, the same ggplot2 syntax applies.\nWhen it comes to the computational side of things, we have to distinguish whether our two variables create a 2-by-2 matrix, i.e. both variables only have two levels. Depending on which scenario applies to our analysis, a different statistical test has to be performed. For some tests, a minimum frequency for each value in our contingency table (i.e. cells) also needs to be achieved. This is usually a matter of sample size and diversity in a sample. Table 12.6 provides an overview of the different scenarios and indicates the functions we have to use to conduct our analysis in R.\n\n\n\nTable 12.6: Statistical tests to compare two unpaired categorical variables. Effect sizes are computed using the effectsize package.\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix\nCondition\nTest\nFunction in R\nEffect size\n\n\n\n\n2x2\n&lt; 10 obs.\nFisher’s Exact Test\nfisher.test()\nphi()\n\n\n2x2\n&gt; 10 obs.\nChi-squared Test with Yate’s Continuity Correction\ninfer::chisq_test(correct = TRUE)3\nphi()\n\n\nn x n\n&gt; 5 obs. (80% of cells)\nChi-squared Test\ninfer::chisq_test()\ncramers_v()\n\n\n\n\n\n\nTo cut a long story short, we only need to be worried about 2x2 contingency tables where the values in some (or all) cells are lower than 10. In those cases, we need to rely on the Fisher’s Exact Test. In all other cases, we can depend on the Pearson Chi-squared Test to do our bidding. The function infer::chisq_test() is based on the function chisq.test(), which automatically applies the required Yate’s Continuity Correction if necessary.\nFor every test that involves two categorical variables, we have to perform three steps:\n\nCompute a contingency table and check the conditions outlined in Table 12.6.\nConduct the appropriate statistical test.\nIf the test is significant, compute the effect size and interpret its value.\n\nConsidering our example from the beginning of the chapter, we are confronted with a 2x2 table that has the following distribution:\n\nct |&gt; pivot_wider(id_cols = satisfaction_bin,\n                   names_from = gender,\n                   values_from = n)\n\n# A tibble: 2 × 3\n  satisfaction_bin female  male\n  &lt;fct&gt;             &lt;int&gt; &lt;int&gt;\n1 satisfied         27677 24910\n2 unsatisfied        8833  8158\n\n\nWe easily satisfy the requirement for a regular Chi-squared test with Yate’s Continuity Correction because each row has at least a value of 10. However, for demonstration purposes, I will show how to compute both the Chi-squared test and the Fisher’s Exact test for the same contingency table.\n\n# Fisher's Exact Test\nfisher.test(wvs_nona$satisfaction_bin, wvs_nona$gender) |&gt;\n  # Make output more readable\n  broom::tidy() |&gt;\n  glimpse()\n\nRows: 1\nColumns: 6\n$ estimate    &lt;dbl&gt; 1.026182\n$ p.value     &lt;dbl&gt; 0.1448926\n$ conf.low    &lt;dbl&gt; 0.991099\n$ conf.high   &lt;dbl&gt; 1.062486\n$ method      &lt;chr&gt; \"Fisher's Exact Test for Count Data\"\n$ alternative &lt;chr&gt; \"two.sided\"\n\n\n\n## Effect size\n(phi &lt;- effectsize::phi(wvs_nona$satisfaction_bin, wvs_nona$gender))\n\nPhi (adj.) |       95% CI\n-------------------------\n4.05e-03   | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::interpret_r(phi$phi, rules = \"cohen1988\")\n\n[1] \"very small\"\n(Rules: cohen1988)\n\n\n\n# Chi-squared test with Yate's continuity correction\nwvs_nona |&gt; infer::chisq_test(satisfaction_bin ~ gender,\n                               correct = TRUE)\n\n# A tibble: 1 × 3\n  statistic chisq_df p_value\n      &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;\n1      2.11        1   0.146\n\n\n\n## Effect size\n(cv &lt;- effectsize::cramers_v(wvs_nona$satisfaction_bin, wvs_nona$gender))\n\nCramer's V (adj.) |       95% CI\n--------------------------------\n4.05e-03          | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::interpret_r(cv$Cramers_v, rules = \"cohen1988\")\n\n[1] \"very small\"\n(Rules: cohen1988)\n\n\nBoth tests reveal that the relationship between our variables is not significant (\\(p &gt; 0.05\\)), and the effect sizes are very small. This result aligns with our visualisation shown in Figure 12.2 because everything looks very symmetrical.\nContingency tables do not always come as 2x2 matrices. Therefore, it makes sense to look at one more example where we have a much larger matrix. Bear in mind that the larger your matrix, the larger your dataset has to be to produce reliable results.\nLet’s explore an example for a contingency table that is a 6x2 matrix. Imagine yourself at a soirée4, and someone might raise the question: Is it true that men are less likely to be married than women? To give you an edge over others at this French evening party, we can take a closer look at this matter with our wvs_nona dataset. We can create a mosaic plot and contingency table which feature relationship_status and gender.\nAs usual, let’s start with a plot first. We create a mosaic plot to visualise all six levels of the factor relationship_status against the two levels of gender.\n\nwvs_nona |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(relationship_status, gender),\n                  fill = relationship_status)) +\n  theme_mosaic()\n\n\n\n\n\n\n\n\nWhile this plot is a lot more colourful than our previous ones, it still suggests that differences across categories are not particularly large. Especially the category married seems to indicate that male and female participants do not differ by much, i.e. their relative frequency is very similar. The only more considerable difference can be found for widowed and single. Apparently, female participants more frequently indicated being widowed while more male participants indicated that they are single.\nNext, we compute the numbers underpinning this plot, comparing the absolute distribution to check our conditions and the relative distribution to interpret the contingency table correctly.\n\n# Check whether we fulfill the criteria\nrel_gen &lt;-\n  wvs_nona |&gt;\n  count(relationship_status, gender)\n\nrel_gen\n\n# A tibble: 12 × 3\n   relationship_status        gender     n\n   &lt;fct&gt;                      &lt;fct&gt;  &lt;int&gt;\n 1 married                    female 20840\n 2 married                    male   19454\n 3 living together as married female  2520\n 4 living together as married male    2138\n 5 separated                  female   953\n 6 separated                  male     626\n 7 widowed                    female  3067\n 8 widowed                    male     822\n 9 single                     female  7373\n10 single                     male    9049\n11 divorced                   female  1757\n12 divorced                   male     979\n\n\n\n# Compute relative frequency\nrel_gen |&gt;\n  group_by(gender) |&gt;\n  mutate(perc = round(n / sum(n), 3)) |&gt;\n  pivot_wider(id_cols = relationship_status,\n              names_from = gender,\n              values_from = perc)\n\n# A tibble: 6 × 3\n  relationship_status        female  male\n  &lt;fct&gt;                       &lt;dbl&gt; &lt;dbl&gt;\n1 married                     0.571 0.588\n2 living together as married  0.069 0.065\n3 separated                   0.026 0.019\n4 widowed                     0.084 0.025\n5 single                      0.202 0.274\n6 divorced                    0.048 0.03 \n\n\nThe contingency table with the relative frequencies confirms what we suspected. The differences are very minimal between male and female participants. In a final step, we need to perform a Chi-squared test to see whether the differences are significant or not.\n\n# Perform Chi-squared test\nwvs_nona |&gt; infer::chisq_test(relationship_status ~ gender)\n\n# A tibble: 1 × 3\n  statistic chisq_df p_value\n      &lt;dbl&gt;    &lt;int&gt;   &lt;dbl&gt;\n1     1669.        5       0\n\n\n\n# Compute effect size\n(cv &lt;- effectsize::cramers_v(wvs_nona$relationship_status,\n                            wvs_nona$gender))\n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.15              | [0.15, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::interpret_r(cv$Cramers_v, rules = \"cohen1988\")\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\nThe statistical results further confirm that the relationship between relationship_status and gender is weak but significant. Therefore, at your next soirée, you can confidently say: “This is bogus. The World Value Survey, which covers 48 different countries, shows that such a relationship is weak considering responses from over 69,000 individuals.” #cheers-to-that\n\n\n12.4.2 Paired groups of categorical variables\nSimilar to paired group comparisons, contingency tables may show paired data. For example, we could be interested in knowing whether an intercultural training we delivered improved participants skills (or at least their personal perception of having improved).\nThe dataset ic_training looks at participants’ confidence in communication with people from diverse cultural backgrounds before and after an intercultural training session. Thus, we might be curious to know whether the training was effective. The dataset contains multiple versions of the same variable measured in different ways. Let’s begin with the variable communication2, which measures improvement as a factor with only two levels, i.e. yes and no. We can start by plotting a mosaic plot.\n\nic_training |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(test, communication2),\n                  fill = test)) +\n  theme_mosaic()\n\n\n\n\n\n\n\n\nThe mosaic plot shows that more participants indicate to be more confident in communicating with culturally others post-training. There were only very few participants who indicated that they have not become more confident.\nIn the next step, we want to compute the contingency table. However, there is one aspect we need to account for: The responses are paired, and therefore we need a contingency table of paired responses. This requires some modification of our data. Let’s start with a classic contingency table as we did for unpaired data.\n\n# Unpaired contingency table\nic_training |&gt;\n  count(test, communication2) |&gt;\n  group_by(test) |&gt;\n  pivot_wider(id_cols = test,\n              names_from = communication2,\n              values_from = n) |&gt;\n  mutate(across(where(is.double), round, 2))\n\n# A tibble: 2 × 3\n# Groups:   test [2]\n  test            yes    no\n  &lt;fct&gt;         &lt;int&gt; &lt;int&gt;\n1 pre_training     18    30\n2 post_training    47     1\n\n\nThis contingency table reflects the mosaic plot we created, but it does not show paired answers. If we sum the frequencies in each cell, we receive a score of 96, which equals the number of observations in ic_training. However, since we have two observations per person (i.e. paired responses), we only have 48 participants. Thus, this frequency table treats the pre and post-training group as independent from each other, which is obviously incorrect.\nThe current dataset includes two rows for each participant. Therefore, if we want to create a contingency table with paired scores, we first need to convert our data so that each participant is reflected by one row only, i.e. we need to make our data frame wider with pivot_wider().\n\n# Creating data frame reflecting paired responses\npaired_data &lt;-\n  ic_training |&gt;\n  pivot_wider(id_cols = name,\n              names_from = test,\n              values_from = communication2)\n\npaired_data\n\n# A tibble: 48 × 3\n   name                       pre_training post_training\n   &lt;chr&gt;                      &lt;fct&gt;        &lt;fct&gt;        \n 1 Hamlin, Christiana         no           no           \n 2 Horblit, Timothy           no           yes          \n 3 Grady, Justin              no           yes          \n 4 Grossetete-Alfaro, Giovana no           yes          \n 5 Dingae, Lori               no           yes          \n 6 el-Sabir, Saamyya          no           yes          \n 7 Reynolds, Tylor            no           yes          \n 8 Aslami, Andrew             no           yes          \n 9 Alexander, Kiri            no           yes          \n10 Guzman Pineda, Timothy     no           yes          \n# ℹ 38 more rows\n\n\nIn simple terms, we converted the column communication2 into two columns based on the factor levels of test, i.e. pre-training and post-training. If we now use these two columns to create a contingency table and mosaic plot, we understand of how the paired distributions between pre-training and post-training look like.\n\n# Contignency table with paired values\nct_paired &lt;-\n  paired_data |&gt;\n  count(pre_training, post_training) |&gt;\n  pivot_wider(names_from = post_training,\n              names_prefix = \"post_training_\",\n              values_from = n,\n              values_fill = 0) # fill empty cells with '0'\nct_paired\n\n# A tibble: 2 × 3\n  pre_training post_training_yes post_training_no\n  &lt;fct&gt;                    &lt;int&gt;            &lt;int&gt;\n1 yes                         18                0\n2 no                          29                1\n\n\n\n# Contigency table with paired percentage values\nct_paired |&gt;\n  mutate(post_training_yes = post_training_yes / nrow(paired_data),\n         post_training_no = post_training_no / nrow(paired_data)) |&gt;\n  mutate(across(where(is.double), round, 2))\n\n# A tibble: 2 × 3\n  pre_training post_training_yes post_training_no\n  &lt;fct&gt;                    &lt;dbl&gt;            &lt;dbl&gt;\n1 yes                       0.38             0   \n2 no                        0.6              0.02\n\n\n\n# Mosaic plot with paired values\npaired_data |&gt;\n  ggplot() +\n  geom_mosaic(aes(x = product(pre_training, post_training),\n                  fill = post_training)\n              ) +\n  theme_mosaic()\n\n\n\n\n\n\n\n\nWith these insights, we can refine our interpretation and state the following:\n\nThere were 19 participants (40%) for whom the training caused no change, i.e. before and after the training their response was still yes or no.\nHowever, for the other 29 participants (60%), the training helped them change from no to yes. This is an excellent result.\nWe notice that no participant scored no after the training was delivered, which is also a great achievement.\n\nLastly, we need to perform a statistical test to confirm our suspicion that the training significantly impacted participants. For paired 2x2 contingency tables, we have to use McNemar’s Test, using the function mcnemar.test(). To determine the effect size, we have to compute Cohen’s g and use the function cohens_g() from the effectsize package to compute it. Be aware that we have to use the paired_data as our data frame and not ic_training.\n\n#McNemar's test\nmcnemar.test(paired_data$pre_training, paired_data$post_training)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  paired_data$pre_training and paired_data$post_training\nMcNemar's chi-squared = 27.034, df = 1, p-value = 1.999e-07\n\n\n\n# Effect size\neffectsize::cohens_g(paired_data$pre_training, paired_data$post_training)\n\nCohen's g |       95% CI\n------------------------\n0.50      | [0.38, 0.50]\n\n\nTo correctly interpret the effect size Cohen (1988) suggest the following benchmarks:\n\n\\(g &lt; 0.05 = negligible\\),\n\\(0.05 \\leq g &lt; 0.15 = small\\),\n\\(0.15 \\leq g &lt; 0.25 = medium\\),\n\\(0.25 \\leq g = large\\).\n\nThus, our effect size is very large, and we can genuinely claim that the intercultural training had a significant impact on the participants. Well, there is only one flaw in our analysis. An eager eye will have noticed that one of our cells was empty, i.e. the top-right cell in the contingency table. The conditions to run such a test are similar to the Chi-squared test. Thus, using the McNemar test is not entirely appropriate in our case. Instead, we need to use the ‘exact McNemar test’, which compares the results against a binomial distribution and not a chi-squared one. More important than remembering the name or the distribution is to understand that the exact test produces more accurate results for smaller samples. However, we have to draw on a different package to compute it, i.e. exact2x2 and its function mcnemar.exact().\n\nlibrary(exact2x2)\nmcnemar.exact(paired_data$pre_training, paired_data$post_training)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  paired_data$pre_training and paired_data$post_training\nb = 0, c = 29, p-value = 3.725e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.0000000 0.1356472\nsample estimates:\nodds ratio \n         0 \n\n\n\ndetach(\"package:exact2x2\", unload = TRUE)\n\nEven after using a more robust computation for our data, the results are still significant. Thus, there is no doubt that the intercultural training helped participants to improve.\nThere are many other combinations of contingency tables. For example, the McNemar test can be extended to a 3x3 or higher matrix, but the rows and columns must be the same length. If you want to compare multiple categorical variables which do not follow a square matrix, it would be necessary to look into a loglinear analysis. While this book does not cover this technique, an excellent starting point is provided by Field (2013) (p. 732ff).\n\n\n12.4.3 A final remark about comparing groups based on categorical data\nAdmittedly, I hardly ever find studies in the field of Social Sciences which work with primarily categorical data. Political Science might be an exception because working, for example, with polling data often implies working with categorical data. A reason you likely will not find yourself in the situation to work with such data and analytical techniques is measurement accuracy. If you look at the dataset ic_training, the variable communication2 was artificially created to turn numeric data into a factor. This is usually not good practice because you lose measurement accuracy, and it can exaggerate differences between groups. Consider the following plots:\n\np1 &lt;-\n  ic_training |&gt;\n  ggplot(aes(x = communication)) +\n  geom_bar() +\n  ggtitle(\"Full scale\")\n\np2 &lt;-\n  ic_training |&gt;\n  ggplot(aes(x = communication3)) +\n  geom_bar() +\n  ggtitle(\"Category with 3 levels\")\n\np3 &lt;-\n  ic_training |&gt;\n  ggplot(aes(x = communication2)) +\n  geom_bar() +\n  ggtitle(\"Category with 2 levels\")\n\n# Combine plots into one single plot with 'patchwork' package\np1 + p2 + p3 + plot_spacer()\n\n\n\n\n\n\n\n\nThe more we aggregate the data into fewer categories, the more likely we increase differences between groups. For example, the plot Category with 3 levels shows that most participants fall into the category medium. However, reducing the categories to two, some participants are classified as yes and some as no. Thus, some respondents who were classified as medium are now in the no category. In reality, we know from our non-categorical measures that several participants will still have improved in confidence but are considered with those who have not improved. This is a major problem because it seems more people are not confident about communicating with people from different cultural backgrounds than there actually are. The accuracy of our measurement scale is poor.\nIn short, I recommend to only use the techniques outlined in this chapter if your data is truly categorical in nature. Thus, when designing your data collection tool, you might also wish to abstain from measuring quantitative variables as categories, for example age, which is a very popular choice among my students. While it might be sometimes more appropriate to offer categories, it is always good to use multiple categories and not just two or three. Every additional level in your factor will increase measurement accuracy and provides access to more advanced analytical techniques. Remember, it is always possible to convert numeric data into a factor, but not the other way around.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#sec-reviewing-group-comparisons",
    "href": "12_group_comparison.html#sec-reviewing-group-comparisons",
    "title": "12  Comparing groups",
    "section": "12.5 Reviewing group comparisons",
    "text": "12.5 Reviewing group comparisons\nIn Social Sciences, we often compare individuals against each other or with themselves over time. As such, understanding how to compare groups of participants is an essential analytical skill. The techniques outlined in this chapter merely provide a solid starting point and likely cover about 90% of the datasets you will encounter or collect. For the other 10% of cases, you might have to look further for more niche approaches to comparing groups. There is also more to know about these techniques from a theoretical and conceptual angle. However, as far as the computation in R is concerned, you should be able to tackle your research projects confidently.\n\n\n\n\nAltman, Douglas G. 1985. “Comparability of Randomised Groups.” Journal of the Royal Statistical Society: Series D (The Statistician) 34 (1): 125–36.\n\n\nBerger, Vance W. 2006. “A Review of Methods for Ensuring the Comparability of Comparison Groups in Randomized Clinical Trials.” Reviews on Recent Clinical Trials 1 (1): 81–86.\n\n\nBlanca Mena, M José, Rafael Alarcón Postigo, Jaume Arnau Gras, Roser Bono Cabré, and Rebecca Bendayan. 2017. “Non-Normal Data: Is ANOVA Still a Valid Option?” Psicothema, 2017, Vol. 29, Num. 4, p. 552-557.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences New York. NY: Academic Press.\n\n\nDinno, Alexis. 2015. “Nonparametric Pairwise Multiple Comparisons in Independent Groups Using Dunn’s Test.” The Stata Journal 15 (1): 292–300.\n\n\nField, Andy. 2013. Discovering Statistics Using IBM SPSS Statistics. Sage Publications.\n\n\nGreenhouse, Samuel W, and Seymour Geisser. 1959. “On Methods in the Analysis of Profile Data.” Psychometrika 24 (2): 95–112.\n\n\nHochberg, Yosef. 1974. “Some Generalizations of the t-Method in Simultaneous Inference.” Journal of Multivariate Analysis 4 (2): 224–34.\n\n\n———. 1988. “A Sharper Bonferroni Procedure for Multiple Tests of Significance.” Biometrika 75 (4): 800–802.\n\n\nHuynh, Huynh, and Leonard S Feldt. 1976. “Estimation of the Box Correction for Degrees of Freedom from Sample Data in Randomized Block and Split-Plot Designs.” Journal of Educational Statistics 1 (1): 69–82.\n\n\nLandis, J Richard, and Gary G Koch. 1977. “The Measurement of Observer Agreement for Categorical Data.” Biometrics, 159–74.\n\n\nTomarken, Andrew J, and Ronald C Serlin. 1986. “Comparison of ANOVA Alternatives Under Variance Heterogeneity and Specific Noncentrality Structures.” Psychological Bulletin 99 (1): 90.\n\n\nYang, Zhilin, Xuehua Wang, and Chenting Su. 2006. “A Review of Research Methodologies in International Business.” International Business Review 15 (6): 601–17. https://doi.org/https://doi.org/10.1016/j.ibusrev.2006.08.003.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "12_group_comparison.html#footnotes",
    "href": "12_group_comparison.html#footnotes",
    "title": "12  Comparing groups",
    "section": "",
    "text": "These functions are taken from the effectsize package.↩︎\nIn order to use this package, it is necessary to install a series of other packages found on bioconductor.org↩︎\ninfer is an R package which is part of tidymodels.↩︎\nA fancy way of saying ‘evening party’ in French.↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparing groups</span>"
    ]
  },
  {
    "objectID": "13_regressions.html",
    "href": "13_regressions.html",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "",
    "text": "13.1 Single linear regression\nA single linear regression looks very similar to a correlation (see Chapter @ref(correlations)), but it is different in that it defines which variable affects another variable, i.e. a directed relationship. I used the terms dependent variable (DV) and independent variable (IV) previously when comparing groups (see Chapter @ref(comparing-groups)), and we will use them here again. In group comparisons, the independent variable was usually a factor, but in regressions, we can use data that is not a categorical variable, i.e. integer, double, etc.\nWhile I understand that mathematical equations can be confusing, they are fairly simple to understand with regressions. Also, when writing our models in R, we will continuously use a formula to specify our regression. Thus, it is advisable to understand them. For example, a single linear regression consists of one independent variable and one dependent variable:\nBeta (\\(\\beta\\)) represents the coefficient of the independent variable, i.e. how much a change in IV causes a change in DV. For example, a one-unit change in IV might mean that DV changes by two units of IV:\nIf we ignore \\(\\beta_0\\) and \\(error\\) for a moment, we find that that if \\(IV = 3\\), our \\(DV = 3*2 = 6\\). Similarly, if \\(IV = 5\\), we find that \\(DV = 10\\), and so on. According to this model, DV will always be twice as large as IV.\nYou might be wondering what \\(\\beta_0\\) stands for. It indicates an offset for each value, also called the intercept. Thus, no matter which value we choose for IV, DV will always be \\(\\beta_0\\) different from IV. It is a constant in our model. This can be best explained by visualising a regression line. Pay particular attention to the expressions after function(x)\n# A: Two models with different beta(0)\nggplot() +\n  geom_function(fun = function(x) 0 + x * 2, col = \"red\") +\n  geom_function(fun = function(x) 1 + x * 2, col = \"blue\") +\n  see::theme_modern()\n# B: Two models with the same beta(0), but different beta(1)\nggplot() +\n  geom_function(fun = function(x) 0 + x * 2, col = \"red\") +\n  geom_function(fun = function(x) 0 + x * 3, col = \"blue\") +\n  see::theme_modern()\nPlot A shows two regressions where only \\(\\beta_0\\) differs, i.e. the intercept. On the other hand, plot B shows what happens if we change \\(\\beta_1\\), i.e. the slope. The two models in plot B have the same intercept and, therefore, the same origin. However, the blue line ascends quicker than the red one because its \\(\\beta_1\\) is higher than for the red model.\nLastly, the \\(error\\) component in the regression model refers to the deviation of data from these regression lines. Ideally, we want this value to be as small as possible.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "13_regressions.html#sec-single-linear-regression",
    "href": "13_regressions.html#sec-single-linear-regression",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "",
    "text": "\\[\nDV = \\beta_{0} + IV * \\beta_{1} + error\n\\]\n\n\n\n\\[\nDV = \\beta_0 + IV * 2 + error\n\\]\n\n\n\n\n\n\n\n\n13.1.1 Fitting a regression model by hand, i.e. trial and error\nIf everything so far sounds all awfully theoretical, let’s try to fit a regression model by hand. First, we need to consider what our model should be able to predict. Let’s say that the number of COVID-19 cases predicts the number of deaths due to COVID-19. Intuitively we would assume this should be a linear relationship because the more cases there are, the more likely we find more deaths caused by it.\n\n# We only select most recent numbers, i.e. \"2021-08-26\"\n# and countries which have COVID cases\ncovid_sample &lt;-\n  covid |&gt;\n  filter(date_reported == \"2021-08-26\" &\n           cumulative_cases != 0)\n\ncovid_sample |&gt;\n  ggplot(aes(x = cumulative_cases,\n             y = cumulative_deaths)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis data visualisation does not show us much. For example, we can see three countries, which appear to have considerably more cases than most other countries. Thus, all other countries are crammed together in the bottom left corner. To improve this visualisation without removing the outliers, we can rescale the x- and y-axis using the function scale_x_continuous() and scale_y_continuous() and apply a \"log\" transformation.\n\ncovid_sample |&gt;\n  ggplot(aes(x = cumulative_cases,\n             y = cumulative_deaths)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log\")\n\n\n\n\n\n\n\n\nAs a result, the scatterplot is now easier to read, and the dots are more spread out. This reveals that there is quite a strong relationship between cumulative_cases and cumulative_deaths. However, similar to before, we should avoid outliers when performing our analysis. For the sake of simplicity, I will limit the number of countries included in our analysis, which also removes the requirement of using scale_x_continuous() and scale_y_continuous().\n\ncovid_sample &lt;-\n  covid_sample |&gt;\n  filter(date_reported == \"2021-08-26\" &\n           cumulative_cases &gt;= 2500 &\n           cumulative_cases &lt;= 150000 &\n           cumulative_deaths &lt;= 3000)\n\nplot &lt;- covid_sample |&gt;\n  ggplot(aes(x = cumulative_cases,\n             y = cumulative_deaths)) +\n  geom_point()\n\nplot\n\n\n\n\n\n\n\n\nWe can try to fit a straight line on top by adjusting the beta values through trial and error. This is effectively what we hope to achieve with a regression: the \\(\\beta\\) values, which best explain our data. Let’s start with the basic assumption of \\(y = x\\) without specific \\(\\beta\\)s, i.e. they are 0.\n\nplot +\n  geom_function(fun = function(x) x, col = \"red\")\n\n\n\n\n\n\n\n\nWhat we try to achieve is that the red line fits nicely inside the cloud of dots. Our simple model provides a very poor fit to our data points, because the dots are considerably below it. This makes sense because \\(y = x\\) would imply that every COVID-19 case leads to a death, i.e. everyone with COVID did not survive. From our own experience, we know that this is luckily not true. Ideally, we want the line to be less steep. We can do this by adding a \\(\\beta_1\\) to our equation. Maybe only 2% of people who got COVID-19 might not have recovered, i.e. \\(\\beta_1 = 0.02\\).\n\nplot +\n  geom_function(fun = function(x) 0.02 * x, col = \"red\")\n\n\n\n\n\n\n\n\nThis time the line looks much more aligned with our observations. However, one could argue that it might have to move a little to the right to cover the observations at the bottom better. Therefore, we should add a \\(\\beta_0\\) to our equation, e.g. -50. This moves the line to the right. I also adjusted \\(\\beta_1\\) ever so slightly to make it fit even better.\n\nplot +\n  geom_function(fun = function(x) -50 + 0.015 * x, col = \"red\")\n\n\n\n\n\n\n\n\nWe finished creating our regression model. If we wanted to express it as a formula, we would write \\(DV = -5 + 0.015 * IV\\). We could use this model to predict how high COVID cases will likely be in other countries not included in our dataset.\n\n\n13.1.2 Fitting a regression model computationally\nEstimating a regression model by hand is not ideal and far from accurate. Instead, we would compute the \\(\\beta\\)s based on our observed data, i.e. cumulative_cases and cumulative_deaths. We can use the function lm() to achieve this. I also rounded (round()) all numeric values to two decimal places to make the output easier to read. We also use tidy() to retrieve a cleaner result from the computation.\n\n# We define our model\nm0 &lt;- lm(cumulative_deaths ~ cumulative_cases, data = covid_sample)\n\n# We clean the output to make it easer to read\nbroom::tidy(m0) |&gt;\n  mutate(across(where(is.numeric),\n                round, 2)\n         )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)         88.1       70.5      1.25    0.21\n2 cumulative_cases     0.01       0       10.7     0   \n\n\nWe first might notice that the p.value indicates that the relationship between cumulative death and cumulative_cases is significant. Thus, we can conclude that countries with more COVID cases also suffer from higher numbers of people who do not recover successfully from it. However, you might be wondering where our \\(\\beta\\) scores are. They are found where it says estimate. The standard error (std.error) denotes the error we specified in the previous equation. In the first row, we get the \\(\\beta_0\\), i.e. the intercept (88.10). This one is larger than what we estimated, i.e. -50. However, \\(\\beta_1\\) is 0.01, which means we have done a very good job guessing this estimate. Still, it becomes apparent that it is much easier to use the function lm() to estimate a model than ‘eyeballing’ it.\nLet me also briefly explain what the function mutate(across(where(is.numeric), round, 2)) does in our computation:\n\nmutate() implies we are changing values of variables,\nacross() indicates that whatever function is placed insight this function will be applied ‘across’ certain columns. We specified our columns with\nwhere(is.numeric), which indicates that all columns should be selected if they are numeric.\nround, 2 represents the function round() and is applied with the paramter 2 to round numbers to two decimal places.\n\nWhile this is might seem like a more advanced method of handling your data, it is good to get into the habit of writing more efficient code, because it tends to be less prone to errors.\nReturning to our data, we can now visualise the computed model (in blue) and our guessed model (in red) in one plot and see the differences. The plot shows that both regression lines are fairly close to each other. Admittedly, it was relatively easy to fit this model by hand. However, it is much more difficult to do so when more than two variables are involved.\n\nplot +\n  geom_function(fun = function(x) -50 + 0.015 * x, col = \"red\") +\n    geom_function(fun = function(x) 88.1 + 0.01 * x, col = \"blue\")\n\n\n\n\n\n\n\n\nWith our final model computed, we also need to check its quality in terms of predictive power based on how well it can explain our observed data. We tested models before when we looked at confirmatory factor analyses for latent variables (see Chapter @ref(latent-constructs)). This time we want to know how accurate our model is in explaining observed data and, therefore, how accurate it predicts future observations. The package performance offers a nice shortcut to compute many different indicators at once:\n\ncheck_model(): Checks for linearity, homogeneity, collinearity and outliers\nmodel_performance(): Tests the quality of our model.\n\nFor now, we are mainly interested in the performance of our model. So, we can compute it the following way:\n\nperformance::model_performance(m0)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |    RMSE |   Sigma\n----------------------------------------------------------------------\n1472.626 | 1472.887 | 1480.319 | 0.548 |     0.543 | 502.573 | 507.891\n\n\nWe are presented with quite a number of performance indicators, and here is how to read them:\n\nAIC stands for Akaike Information Criterion, and the lower the score, the better the model.\nBIC stands for Bayesian Information Criterion, and the lower the score, the better the model.\nR2 stands for R squared (\\(R^2\\)) and is also known as the coefficient of determination. It measures how much the independent variable can explain the variance in the dependent variable. In other words, the higher \\(R^2\\), the better is our model, because our model can explain more of the variance. \\(R^2\\) falls between 0-1, where 1 would imply that our model can explain 100% of the variance in our sample. \\(R^2\\) is also considered a goodness-of-fit measure.\nR2 (adj.) stands for adjusted R squared. The adjusted version of \\(R^2\\) becomes essential if we have more than one predictor (i.e. multiple independent variables) in our regression. The adjustment of \\(R^2\\) accounts for the number of independent variables in our model. Thus, we can compare different models, even though they might have different numbers of predictors. It is important to note that unadjusted \\(R^2\\) will always increase if we add more predictors.\nRMSE stands for Root Mean Square Error and indicates how small or large the prediction error of the model is. Conceptually, it aims to measure the average deviations of values from our model when we attempt predictions. The lower its score, the better, i.e. a score of 0 would imply that our model perfectly fits the data, which is likely never the case in the field of Social Sciences. The RMSE is particularly useful when trying to compare models.\nSigma stands for the standard deviation of our residuals (the difference between predicted and empirically observed values) and is a measure of prediction accuracy. Sigma is ‘a measure of the average distance each observation falls from its prediction from the model’ (Gelman, Hill, and Vehtari 2020) (p. 168).\n\nMany of these indices will become more relevant when we compare models. However, \\(R^2\\) can also be meaningfully interpreted without a reference model. We know that the bigger \\(R^2\\), the better. In our case, it is 0.548, which is very good considering that our model consists of only one predictor. It is not easy to interpret whether a particular \\(R^2\\) value is good or bad. In our simple single linear regression, \\(R^2\\) is literally ‘r squared’, which we already know from correlations and their effect sizes (see Table @ref(tab:effect-size-cohen)). Thus, if we take the square root of \\(R^2\\) we can retrieve the correlation coefficient, i.e. \\(r = \\sqrt{R^2} = \\sqrt{0.548} = 0.740\\). According to J. Cohen (1988), this would count as a large effect size.\nHowever, the situation is slightly more complicated for multiple regressions, and we cannot use Cohen’s reference table by taking the square root of \\(R^2\\). Still, the meaning of \\(R^2\\) and its adjusted version remain the same for our model.\nOnce you have a model and it is reasonably accurate, you can start making predictions. This can be achieved by using our model object m0 with the function add_predictions() from the modelr package. However, first, we should define a set of values for our independent variable, i.e. cumulative_cases, which we store in a tibble using the tibble() function.\n\ndf_predict &lt;-\n  tibble(cumulative_cases = c(100, 1000, 10000))\n\n# Make our predictions\ndf_predict |&gt;\n  modelr::add_predictions(m0) |&gt;\n  mutate(pred = round(pred, 0))\n\n# A tibble: 3 × 2\n  cumulative_cases  pred\n             &lt;dbl&gt; &lt;dbl&gt;\n1              100    90\n2             1000   102\n3            10000   232\n\n\nThe predictions are stored in column pred. Therefore, we know how many deaths from COVID have to be expected based on our model for each value in our dataset.\nSingle linear regressions are simple and an excellent way to introduce novice users of R to modelling social phenomena. However, we hardly ever find that a single variable can explain enough variance to be a helpful model. Instead, we can most likely improve most of our single regression models by considering more variables and using a multiple regression technique.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "13_regressions.html#sec-multiple-regression",
    "href": "13_regressions.html#sec-multiple-regression",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "13.2 Multiple regression",
    "text": "13.2 Multiple regression\nMultiple regressions expand single linear regressions by allowing us to add more variables. Maybe less surprising, computing a multiple regression is similar to a single regression in R because it requires the same function, i.e. lm(). However, we add more IVs. Therefore, the equation we used before needs to be modified slightly by adding more independent variables. Each of these variables will have its own \\(\\beta\\) value:\n\n\\[\nDV = \\beta_{0} + IV_{1} * \\beta_{1} + IV_{2} * \\beta_{2} + ... + IV_{n} * \\beta_{n} + error\n\\]\n\nIn the last section, we wanted to know how many people will likely not recover from COVID. However, it might be even more interesting to understand how we can predict new cases and prevent casualties from the outset. Since I live in the United Kingdom during the pandemic, I am curious whether specific COVID measures help reduce the number of new cases in this country. To keep it more interesting, I will also add Germany to the mix since it has shown to be very effective in handling the pandemic relative to other European countries. Of course, feel free to pick different countries (maybe the one you live in?) to follow along with my example. In Chapter @ref(moderated-regression) it will become apparent why I chose two countries (#spoiler-alert).\nFirst, we create a dataset that only contains information from the United Kingdom and Germany1, using filter().\n\ncovid_uk_ger &lt;-\n  covid |&gt;\n  filter(iso3 == \"GBR\" | iso3 == \"DEU\")\n\nIn the next step, we might want to know how new cases are distributed over time. It is always good to inspect the dependent variable to get a feeling of how much variance there is in our data. Having a more extensive range of data values is ideal because the regression model will consider low and high values of the dependent variable instead of just high or low scores. If you find that your dependent variable shows minimal variance, your model will likely be ‘overfitted’. An overfitted model can very well explain the sample data but performs poorly with a new dataset. This is, of course not desirable, because the value of creating a model is to predict future observations. Let’s plot the DV new_cases across time to see when and how many new COVID cases had to be reported for both countries together.\n\ncovid_uk_ger |&gt;\n  ggplot(aes(x = date_reported,\n             y = new_cases)) +\n  geom_col()\n\n\n\n\n\n\n\n\nWe can tell that there are different waves of new cases with very low and very high values. As such, we should find variables that help us explain when new_cases are high and when they are low. If you have hypotheses to test, you would already know which variables to include in your regression. However, we do not have a hypothesis based on our prior reading or other studies. Thus, we pick variables of interest that we suspect could help us with modelling new COVID cases. For example, we can be fairly confident that the number of new COVID cases should be lower if more safety measures are in place - assuming that they are effective and everyone adheres to them. The covid dataset includes such information evaluated by the WHO, i.e. masks, travel, gatherings, schools and movements. Remember, you can always find out what these variables stand for by typing ?covid into the console. A higher value for these variables indicates that there were more safety measures in place. Scores can range from 0 (i.e. no measures are in place) to 100 (i.e. all WHO measures are in place).\nWe can add multiple variables by using the + symbol in the lm() function.\n\n# Create our model\nm0 &lt;- lm(new_cases ~ masks + movements + gatherings +\n           schools + businesses + travel,\n         data = covid_uk_ger)\n\n# Inspect the model specifications.\n# I always round the p.value since I\n# do not prefer the scientific notation\nbroom::tidy(m0) |&gt;\n  mutate(p.value = round(p.value, 3))\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2585.     745.       -3.47   0.001\n2 masks          -14.8      9.18     -1.61   0.108\n3 movements      -54.7     15.5      -3.52   0    \n4 gatherings     -26.0     19.8      -1.31   0.191\n5 schools        394.      29.7      13.2    0    \n6 businesses      93.7     13.8       6.79   0    \n7 travel          49.5      9.30      5.32   0    \n\n\n\n# Evaluate the quality of our model\nperformance::model_performance(m0)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma\n-----------------------------------------------------------------------------\n25277.622 | 25277.744 | 25318.262 | 0.233 |     0.229 | 10027.204 | 10056.877\n\n\nOverall (and purely subjectively judged), the model is not particularly great because even though we added so many variables, the \\(adjusted \\ R^2\\) is not particularly high, i.e. ‘only’ 0.229. As mentioned earlier, for multiple regression, it is better to look at \\(adjusted \\ R^2\\), because it adjusts for the number of variables in our model and makes the comparison of different models easier. There are a couple more important insights gained from this analysis:\n\nNot all variables appear to be significant. The predictors masks and gatherings are not significant, i.e. p.value &gt; 0.05. Thus, it might be worth removing these variables to optimise the model.\nThe variable, movements seems to reduce new_cases, i.e. it has a negative estimate (\\(\\beta\\)).\nHowever, schools, businesses, and travel have a positive effect on new_cases.\n\nEspecially the last point might appear confusing. How can it be that if more measures are taken, the number of new COVID cases increases? Should we avoid them? We have not considered in our regression that measures might be put in place to reduce the number of new cases rather than to prevent them. Thus, it might not be the case that schools, businesses, and travel predict higher new_cases, but rather the opposite, i.e. due to higher new_cases, the measures for schools, businesses and travel were tightened, which later on (with a time lag) led to lower new_cases. Thus, the relationships might be a bit more complicated, but to keep things simple, we accept that with our data, we face certain limitations (as is usually the case)2.\nAs a final step, we should remove the variables that are not significant and see how this affects our model.\n\nm1 &lt;- lm(new_cases ~ movements + schools + businesses + travel,\n         data = covid_uk_ger)\n\nbroom::tidy(m1) |&gt;\n  mutate(p.value = round(p.value, 3))\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -3024.     724.       -4.18       0\n2 movements      -64.0     13.4      -4.79       0\n3 schools        382.      28.8      13.3        0\n4 businesses      90.5     12.9       7.01       0\n5 travel          45.7      9.17      4.99       0\n\n\n\n# Evaluate the quality of our model\nperformance::model_performance(m1)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma\n-----------------------------------------------------------------------------\n25279.519 | 25279.590 | 25309.999 | 0.229 |     0.227 | 10052.123 | 10073.343\n\n\nComparing our original model m0 with our revised model m1, we can see that our R2 (adj.) barely changed. Thus, m1 is a superior model because it can explain (almost) the same amount of variance but with fewer predictors. The model m1 would also be called a parsimonious model, i.e. a model that is simple but has good predictive power. When reading about multiple regressions, you might often hear people mention the ‘complexity’ of a model, which refers to the number of predictors. The complexity of a model is known as the ‘degrees of freedom (df) of the numerator’ and is computed as \\(number\\ of\\ preditors - 1\\). For example, in our model of 4 independent variables, the df of the numerator is 3. This value is relevant when computing the power of a regression model (see also Chapter @ref(power-analysis)).\n\nperformance::compare_performance(m0, m1)\n\n# Comparison of Model Performance Indices\n\nName | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |      RMSE |     Sigma\n--------------------------------------------------------------------------------------------------------------\nm0   |    lm | 25277.6 (0.721) | 25277.7 (0.716) | 25318.3 (0.016) | 0.233 |     0.229 | 10027.204 | 10056.877\nm1   |    lm | 25279.5 (0.279) | 25279.6 (0.284) | 25310.0 (0.984) | 0.229 |     0.227 | 10052.123 | 10073.343\n\n\nHowever, there are a couple of things we overlooked when running this regression. If you are familiar with regressions already, you might have been folding your hands over your face and burst into tears about the blasphemous approach to linear regression modelling. Let me course-correct at this point.\nSimilar to other parametric approaches, we need to test for sources of bias, linearity, normality and homogeneity of variance. Since multiple regressions consider more than one variable, we must consider these criteria in light of other variables. As such, we have to draw on different tools to assess our data. There are certain pre- and post-tests we have to perform to evaluate and develop a multiple regression model fully:\n\nPre-test: We need to consider whether there are any outliers and whether all assumptions of OLS regression models are met.\nPost-test: We need to check whether our independent variables correlate very strongly with each other, i.e. are there issues of multiple collinearity.\n\nWe already covered aspects of linearity, normality and homogeneity of variance. However, outliers and collinearity have to be reconsidered for multiple regressions.\n\n13.2.1 Outliers in multiple regressions\nWhile it should be fairly clear by now why we need to handle outliers (remember Chapter @ref(dealing-with-outliers)), our approach is somewhat different when we need to consider multiple variables at once. Instead of identifying outliers for each variable independently, we have to consider the interplay of variables. In other words, we need to find out how an outlier in our independent variable affects the overall model rather than just one other variable. Thus, we need a different technique to assess outliers. By now, you might not be shocked to find that there is more than one way of identifying outliers in regressions and that there are many different ways to compute them in R. (P. Cohen, West, and Aiken 2014) distinguishes between where one can find outliers in the model as summarised in ?tbl-outliers-in-multiple-regressions). I offer a selection of possible ways to compute the relevant statistics, but this list is not exhaustive. For example, many of these statistics can also be found using the function influence.measures().\n\nOutlier detection in multiple regressions\n\n\n\n\n\n\n\nOutlier in?\nMeasures\nfunction in R\n\n\n\n\nDependent variable\n\nInternally studentised residuals\n\n\nrstandard() or fortify()\n\n\n\nDependent variable\n\nExternally studentised residuals\n\n\nrstudent()\n\n\n\nIndependent variable\n\nLeverage\n\n\nhatvalues() or fortify()\n\n\n\nIndependent variable\n\nMahalanobis distance\n\n\nmahalanobis()3 or mahalanobis_distance()4\n\n\n\nEntire model\nGlobal measures of influence\n\nDFFITS,\nCook’s d\n\nGlobal measures of influence\n\ndffits()\ncooks.distance() or fortify()\n\n\n\nEntire model\nSpecific measures of influence:\n\nDFBETAS\n\nSpecific measures of influence:\n\ndfbetas()\n\n\n\n\nWhile it might be clear why we need to use different approaches to find outliers in different model components, this might be less clear when evaluating outliers that affect the entire model. We distinguish between global measures of influence, which identify how a single observation affects the quality of the entire model, and specific measures of influence, which determine how a single observation affects each independent variable, i.e. its regression coefficients denoted as \\(\\beta\\). It is recommended to look at all different outlier measures before venturing ahead to perform linear multiple regression. Going through the entire set of possible outliers would go way beyond the scope of this book. So, I will focus on five popular measures which cover all three categories:\n\nDependent variable: Externally studentised residuals\nIndependent variable: Leverage and Mahalanobis distance\nEntire model: Cook’s d and DFBETAS\n\nThe approach taken is the same for the other outlier detection methods, but with different functions. Thus, it should be quite simple to reproduce these as well after having finished the chapters below.\n\n13.2.1.1 Outliers in the dependent variable\nIrrespective of whether we look at independent or dependent variables, we always want to know whether extreme values are present. Here I will use the externally studentised residual, which ’is the preferred statistic to use to identify cases whose (…) values are highly discrepant from their predicted values (P. Cohen, West, and Aiken 2014) (p.401).\nFirst, we need to compute the residuals for our model as a new column in our dataset. Since we also want to use other methods to investigate outliers, we can use the function fortify(), which will add some of the later indicators and creates a tibble that only includes the variables from our model. This one handy function does a lot of things at once. In a second step, we add the studentised residuals using rstudent().\n\n# Create a tibble with some pre-computed stats\nm1_outliers &lt;-\n  fortify(m1) |&gt;\n  as_tibble()\n\nglimpse(m1_outliers)\n\nRows: 1,188\nColumns: 11\n$ new_cases  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ movements  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ schools    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ businesses &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ travel     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ .hat       &lt;dbl&gt; 0.005159386, 0.005159386, 0.005159386, 0.005159386, 0.00515…\n$ .sigma     &lt;dbl&gt; 10077.22, 10077.22, 10077.22, 10077.22, 10077.22, 10077.22,…\n$ .cooksd    &lt;dbl&gt; 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.3…\n$ .fitted    &lt;dbl&gt; -3023.617, -3023.617, -3023.617, -3023.617, -3023.617, -302…\n$ .resid     &lt;dbl&gt; 3023.617, 3023.617, 3023.617, 3023.617, 3023.617, 3023.617,…\n$ .stdresid  &lt;dbl&gt; 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.30…\n\n\n\n# Add the externally studentised residuals\nm1_outliers &lt;-\n  m1_outliers |&gt;\n  mutate(.studresid = rstudent(m1))\n\nglimpse(m1_outliers)\n\nRows: 1,188\nColumns: 12\n$ new_cases  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ movements  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ schools    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ businesses &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ travel     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ .hat       &lt;dbl&gt; 0.005159386, 0.005159386, 0.005159386, 0.005159386, 0.00515…\n$ .sigma     &lt;dbl&gt; 10077.22, 10077.22, 10077.22, 10077.22, 10077.22, 10077.22,…\n$ .cooksd    &lt;dbl&gt; 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.3…\n$ .fitted    &lt;dbl&gt; -3023.617, -3023.617, -3023.617, -3023.617, -3023.617, -302…\n$ .resid     &lt;dbl&gt; 3023.617, 3023.617, 3023.617, 3023.617, 3023.617, 3023.617,…\n$ .stdresid  &lt;dbl&gt; 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.30…\n$ .studresid &lt;dbl&gt; 0.3008218, 0.3008218, 0.3008218, 0.3008218, 0.3008218, 0.30…\n\n\nWith our dataset ready for plotting, we can do exactly that and see which observations are particularly far away from the rest of our .studresid values. To plot each observation separately, we need an id variable for each row. We can quickly add one by using the function rownames_to_column(). This way, we can identify each column and also filter() out particular rows. You might be able to guess why this will come in handy at a later stage of our outlier analysis (hint: Chapter @ref(reviewing-the-outliers)).\n\n# Create an ID column\nm1_outliers &lt;- m1_outliers |&gt; rownames_to_column()\n\nm1_outliers |&gt;\n  ggplot(aes(x = rowname,\n             y = .studresid)) +\n  geom_point(size = 0.5)\n\n\n\n\n\n\n\n\nThe values of the externally studentised residuals can be positive or negative. All we need to know is which values count as outliers and which ones do not. P. Cohen, West, and Aiken (2014) (p. 401) provides some guidance:\n\ngeneral: \\(outlier = \\pm 2\\)\nbigger samples: \\(outlier = \\pm 3\\) or \\(\\pm 3.5\\) or \\(\\pm 4\\)\n\nAs you can tell, it is a matter of well-informed personal judgement. Our dataset consists of over 1200 observations. As such, the data frame certainly counts as large. We can take a look and see how many outliers we would get for each of the benchmarks.\n\nout_detect &lt;-\n  m1_outliers |&gt;\n  mutate(pm_2 = ifelse(abs(.studresid) &gt; 2, \"TRUE\", \"FALSE\"),\n         pm_3 = ifelse(abs(.studresid) &gt; 3, \"TRUE\", \"FALSE\"),\n         pm_35 = ifelse(abs(.studresid) &gt; 3.5, \"TRUE\", \"FALSE\"),\n         pm_4 = ifelse(abs(.studresid) &gt; 4, \"TRUE\", \"FALSE\"))\n\nout_detect |&gt; count(pm_2)\n\n# A tibble: 2 × 2\n  pm_2      n\n  &lt;chr&gt; &lt;int&gt;\n1 FALSE  1132\n2 TRUE     56\n\n\n\nout_detect |&gt; count(pm_3)\n\n# A tibble: 2 × 2\n  pm_3      n\n  &lt;chr&gt; &lt;int&gt;\n1 FALSE  1171\n2 TRUE     17\n\n\n\nout_detect |&gt; count(pm_35)\n\n# A tibble: 2 × 2\n  pm_35     n\n  &lt;chr&gt; &lt;int&gt;\n1 FALSE  1173\n2 TRUE     15\n\n\n\nout_detect |&gt; count(pm_4)\n\n# A tibble: 2 × 2\n  pm_4      n\n  &lt;chr&gt; &lt;int&gt;\n1 FALSE  1179\n2 TRUE      9\n\n\nThe results indicate we could have as many as 56 outliers and as little as 9. It becomes apparent that choosing the right threshold is a tricky undertaking. Let’s plot the data again to make it easier to read and add some of the thresholds. I skip 3.5 since it is very close to 3. I also reorder the observations (i.e. the x-axis) based on .studresid using reorder().\n\nm1_outliers |&gt;\n  ggplot(aes(x = reorder(rowname, .studresid),\n             y = .studresid)) +\n  geom_point(size = 0.5) +\n  geom_hline(yintercept = c(-2, 2), col = \"green\") +\n  geom_hline(yintercept = c(-3, 3), col = \"orange\") +\n    geom_hline(yintercept = c(-4, 4), col = \"red\")\n\n\n\n\n\n\n\n\nAt this point, it is a matter of choosing the threshold that you feel is most appropriate. More importantly, though, you have to make sure you are transparent in your choices and provide some explanations around your decision-making. For example, a threshold of 2 appears too harsh for my taste and identifies too many observations as outliers. On the other hand, using the orange threshold of 3 seems to capture most observations I would consider an outlier because we can also visually see how the dots start to look less like a straight line and separate more strongly. Besides, P. Cohen, West, and Aiken (2014) also suggests that a threshold of 3 is more suitable for larger datasets. Finally, since we have an ID column (i.e. rownames), we can also store our outliers in a separate object to easily reference them later for comparisons with other measures. Again, the purpose of doing this will become evident in Chapter @ref(reviewing-the-outliers).\n\noutliers &lt;-\n  out_detect |&gt;\n  select(rowname, pm_3) |&gt;\n  rename(studresid = pm_3)\n\nThere are still more diagnostic steps we have to take before deciding which observations we want to remove or deal with in other ways (see also Chapter @ref(dealing-with-outliers)).\n\n\n13.2.1.2 Outliers in independent variables\nTo identify outliers in independent variables, we can use Leverage scores or the Mahalanobis distances. Both are legitimate approaches and can be computed very easily.\nFor the leverage scores, we can find them already in our fortify()-ed dataset m1_outliers. They are in the column .hat. An outlier is defined by the distance from the average leverage value, i.e. the further the distance of an observation from this average leverage, the more likely we have to classify it as an outlier. The average leverage is computed as follows:\n\n\\(average\\ leverage = \\frac{k + 1}{n}\\)\n\nIn this equation, k stands for the number of predictors (i.e. 6) and n for the number of observations (i.e. 1188). Therefore, our average leverage can be computed as follows:\n\n(avg_lvg &lt;- (6 + 1) / 1188)\n\n[1] 0.005892256\n\n\nSimilar to before, we find different approaches to setting cut-off points for this indicator. While Hoaglin and Welsch (1978) argue that a distance twice the average counts as an outlier, Stevens (2012) (p. 105) suggests that values three times higher than the average leverage will negatively affect the model. The rationale is the same as for the externally studentised residuals: If the thresholds are too low, we might find ourselves with many observations, which we would have to investigate further. This might not always be possible or even desirable. However, this should not imply that many outliers are not worth checking. Instead, if there are many, one would have to raise questions about the model itself and whether an important variable needs adding to explain a series of observations that appear to be somewhat ‘off’.\nLet’s plot the leverages and use Stevens (2012) benchmark to draw our reference line.\n\nm1_outliers |&gt;\n  ggplot(aes(x = reorder(rowname, .hat),\n             y = .hat)) +\n  geom_point(size = 0.5) +\n  geom_hline(yintercept = 3 * avg_lvg, col = \"orange\")\n\n\n\n\n\n\n\n\nAs before, we want to know which observations fall beyond the threshold.\n\nnew_outliers &lt;-\n  m1_outliers |&gt;\n  mutate(avglvg = ifelse(.hat &gt; 3 * avg_lvg, \"TRUE\", \"FALSE\")) |&gt;\n  select(rowname, avglvg)\n\n# Add new results to our reference list\noutliers &lt;- left_join(outliers, new_outliers, by = \"rowname\")\n\nLooking at our reference object outliers, you will notice that both methods (studresid and avglvg) detect different outliers. Thus, the detection of outliers depends on where we look, i.e. dependent variable or independent variable.\nThe second method I will cover in this section is the Mahalanobis distance. Luckily the rstatix package includes a handy function mahalanobis_distance() which automatically detects outliers and classifies them for us.\n\nmhnbs_outliers &lt;-\n  m1_outliers |&gt;\n  select(new_cases:travel) |&gt;\n  rstatix::mahalanobis_distance() |&gt;\n  rownames_to_column() |&gt;\n  select(rowname, mahal.dist, is.outlier) |&gt;\n  rename(mhnbs = is.outlier)\n\n# Add new results to our reference list\noutliers &lt;- left_join(outliers, mhnbs_outliers, by = \"rowname\")\n\nglimpse(outliers)\n\nRows: 1,188\nColumns: 5\n$ rowname    &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ studresid  &lt;chr&gt; \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALS…\n$ avglvg     &lt;chr&gt; \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALS…\n$ mahal.dist &lt;dbl&gt; 5.215, 5.215, 5.215, 5.215, 5.215, 5.215, 5.215, 5.215, 5.2…\n$ mhnbs      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n\n\n\n# We need to remove mahal.dist because it does not indicate\n# whether a value is an outlier #data-cleaning\noutliers &lt;- outliers |&gt; select(-mahal.dist)\n\nWhile it is very convenient that this function picks the cut-off point for us, it might be something we would want more control over. As we have learned so far, choosing the ‘right’ cut-off point is essential. Since the values follow a chi-square distribution, we can determine the cut-off points based on the relevant critical value at the chosen p-value. R has a function that allows finding the critical value for our model, i.e. qchisq().\n\n(mhnbs_th &lt;- qchisq(p = 0.05,\n                   df = 4,\n                   lower.tail = FALSE))\n\n[1] 9.487729\n\n\nThe p-value reflects the probability we are willing to accept that our result is significant/not significant (remember Type I error in Chapter @ref(power-analysis)). The value df refers to the degrees of freedom, which relates to the number of independent variables, i.e. 4. Thus, it is fairly simple to identify a cut-off point yourself by choosing the p-value you consider most appropriate. The function mahalanobis_distance() assumes \\(p = 0.01\\), which is certainly a good but strict choice.\nIf we want to plot outliers as before, we can reuse our code from above and replace it with the relevant new variables.\n\nmhnbs_outliers |&gt;\n  ggplot(aes(x = reorder(rowname, mahal.dist),\n             y = mahal.dist)) +\n  geom_point(size = 0.5) +\n  geom_hline(yintercept = mhnbs_th, col = \"orange\")\n\n\n\n\n\n\n\n\nLooking at our outliers, we notice that the Mahalanobis distance identifies more outliers than the leverage, but the same ones.\n\noutliers |&gt; filter(avglvg == \"TRUE\")\n\n# A tibble: 18 × 4\n   rowname studresid avglvg mhnbs\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;\n 1 224     FALSE     TRUE   TRUE \n 2 225     FALSE     TRUE   TRUE \n 3 226     FALSE     TRUE   TRUE \n 4 227     FALSE     TRUE   TRUE \n 5 969     FALSE     TRUE   TRUE \n 6 970     FALSE     TRUE   TRUE \n 7 971     FALSE     TRUE   TRUE \n 8 972     FALSE     TRUE   TRUE \n 9 973     FALSE     TRUE   TRUE \n10 974     FALSE     TRUE   TRUE \n11 975     FALSE     TRUE   TRUE \n12 976     FALSE     TRUE   TRUE \n13 977     FALSE     TRUE   TRUE \n14 978     FALSE     TRUE   TRUE \n15 979     FALSE     TRUE   TRUE \n16 980     FALSE     TRUE   TRUE \n17 981     FALSE     TRUE   TRUE \n18 982     FALSE     TRUE   TRUE \n\n\nThus, whether you need to use both approaches for the same study is questionable and likely redundant. Still, in a few edge cases, you might want to double-check the results, especially when you feel uncertain which observations should be dealt with later. But, of course, it does not take much time to consider both options.\nBesides looking at each side of the regression separately, we might also consider whether removing an observation significantly affects all variables. This is done with outlier detection diagnostics which consider the entire model.\n\n\n13.2.1.3 Outlier detection considering the entire model: Global measures\nTo assess the global impact of outliers on the entire regression model, Cook’s d (Cook and Weisberg 1982) is a popular method in the Social Sciences. It measures to which extend a single observation can affect the predictive power of our model to explain all other observations. Obviously, we do not wish to keep observations that make predicting most of the other observations more challenging. However, as shown in Table @ref(tab:outliers-in-multiple-regressions), there are different approaches to this, but some are somewhat redundant. For example, P. Cohen, West, and Aiken (2014) highlight that DDFITS and Cook’s d are ‘interchangeable statistics’ (p. 404). Thus, there is no point in demonstrating both since they function similarly. Your decision might be swayed by the preferences of a publisher, editor, lecturer, supervisor or reviewer. Here, I will focus on Cook’s d since it is the approach I see most frequently used in my field. By all means, feel encouraged to go the extra mile and perform the same steps for the DDFITS.\nThe good news, fortify() automatically added .cooksd to our dataset m1_outliers. Thus, we can immediately compute whether outliers exist and inspect them visually as we did before. A promising starting point to find outliers via the Cook’s D is to plot its distribution.\n\nm1_outliers |&gt;\n  ggplot(aes(x = rowname,\n             y = .cooksd)) +\n  geom_col()\n\n\n\n\n\n\n\n\nInspecting this barplot, we can tell that some observations have much higher .cooksd values than any other observation. Once again, we first need to decide on a benchmark to determine whether we can consider these values as outliers. If we follow Cook and Weisberg (1982), values that are higher than 1 (i.e. \\(d &gt; 1\\)) require reviewing. Looking at our plot, none of the observations reaches 1, and we need not investigate outliers. Alternatively, P. Cohen, West, and Aiken (2014) suggest that other benchmarks are also worth considering, for example, based on the critical value of an F distribution, which we can determine with the function qf(). This requires us to determine two degrees of freedom (\\(df_1\\) and \\(df_2\\)) and a p-value. To determine these values, P. Cohen, West, and Aiken (2014) suggests \\(p = 0.5\\) and the following formulas to determine the correct \\(df\\):\n\n\\(df_1 = k +1\\)\n\\(df_2 = n - k - 1\\)\n\nLike the critical value for average leverage, \\(k\\) reflects the number of predictors, and \\(n\\) refers to the sample size. Thus, we can determine the critical value, and therefore our cut-off point as follows:\n\nqf(p = 0.5,\n   df1 = 4 + 1,\n   df2 = 1188 - 6 - 1)\n\n[1] 0.8707902\n\n\nThis score would also imply that we have no particular outliers to consider, because the highest value in .cooksd is 0.09 computed via max(m1_outliers$.cooksd).\nWe might be led to believe that our work is done here, but P. Cohen, West, and Aiken (2014) recommends that any larger deviation is worth inspecting. We do notice that relative to other observations, some cases appear extreme. Ideally, one would further investigate these cases and compare the regression results by removing such extreme cases iteratively. This way, one can assess whether the extreme observations genuinely affect the overall estimates of our model. This would imply repeating steps we already covered earlier when performing multiple regressions with and without outliers. Thus, I will forgo this step here.\nIn the last chapter about outliers (Chapter @ref(reviewing-the-outliers)), we will rerun the regression without outliers to see how this affects our model estimates. However, before we can do this, we have to cover one more method of detecting outliers.\n\n\n13.2.1.4 Outlier detection considering the entire model: Specific measures\nWhile Cook’s d helps us identify outliers that affect the quality of the entire model, there is also a way to investigate how outliers affect specific predictors. This is achieved with DFBETAS, which, similar to previous methods, assesses the impact of outliers by removing them and measures the impact of such removal on other parts of the regression model.\nThe function dbetas() takes our model m1 and returns the statistics for each predictor in the model. Thus, we do not receive a single score for each observation but multiple for each specific predictor, i.e. each specific measure.\n\nm1_dfbetas &lt;-\n  dfbetas(m1) |&gt;\n  as_tibble() |&gt;               # convert to tibble for convenience\n  rownames_to_column()          # add our rownames\n\nglimpse(m1_dfbetas)\n\nRows: 1,188\nColumns: 6\n$ rowname       &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n$ `(Intercept)` &lt;dbl&gt; 0.02166365, 0.02166365, 0.02166365, 0.02166365, 0.021663…\n$ movements     &lt;dbl&gt; -0.0008908373, -0.0008908373, -0.0008908373, -0.00089083…\n$ schools       &lt;dbl&gt; -0.01412839, -0.01412839, -0.01412839, -0.01412839, -0.0…\n$ businesses    &lt;dbl&gt; -0.002957931, -0.002957931, -0.002957931, -0.002957931, …\n$ travel        &lt;dbl&gt; -0.005546516, -0.005546516, -0.005546516, -0.005546516, …\n\n\nAll we have to do now is to compare the scores against a benchmark for each variable in this dataset, and we know which observations substantially affect one of the predictors. P. Cohen, West, and Aiken (2014) provides us with the following recommendations for suitable thresholds:\n\nsmall or moderate datasets: \\(DFBETAS &gt; \\pm 1\\)\nlarge datasets: \\(DFBETAS &gt; \\pm\\frac{2}{\\sqrt(n)}\\)\n\nSince our dataset falls rather into the ‘large’ camp, we should choose the second option. Again, \\(n\\) stands for the sample size. Let’s create an object to store this value.\n\n(dfbetas_th &lt;- 2 / sqrt(1188))\n\n[1] 0.05802589\n\n\nFor demonstration purposes, I will pick movements to check for outliers. If this was a proper analysis for a project, you would have to compare this indicator against each variable separately. As the benchmarks indicate, the DFBETAS value can be positive or negative. So, when we compare the calculated values with it, we can look at the absolute value, i.e. use abs(), which turns all values positive. This makes it much easier to compare observations against a threshold and we could do this for any assessment of outliers against a benchmark.\n\n# Check whether values exceed the threshold\ndfbetas_check &lt;-\n  m1_dfbetas |&gt;\n  mutate(dfbetas_movements = ifelse(abs(movements) &gt; dfbetas_th,\n                                \"TRUE\",\n                                \"FALSE\")) |&gt;\n  select(rowname, dfbetas_movements)\n\n# Add result to our outliers object\noutliers &lt;-\n  outliers |&gt;\n  left_join(dfbetas_check, by = \"rowname\")\n\nglimpse(outliers)\n\nRows: 1,188\nColumns: 5\n$ rowname           &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"…\n$ studresid         &lt;chr&gt; \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\"…\n$ avglvg            &lt;chr&gt; \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\"…\n$ mhnbs             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ dfbetas_movements &lt;chr&gt; \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\", \"FALSE\"…\n\n\n\n# some housekeeping, i.e. making all columns &lt;lgl&gt; except for rowname\noutliers &lt;-\n  outliers |&gt;\n  mutate(rowname = as_factor(rowname)) |&gt;\n  mutate_if(is.character, as.logical)\n\nglimpse(outliers)\n\nRows: 1,188\nColumns: 5\n$ rowname           &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ studresid         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ avglvg            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ mhnbs             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ dfbetas_movements &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n\n\nOf course, we can also visualise the outliers as we did before.\n\nm1_dfbetas |&gt;\n  ggplot(aes(x = rowname,\n             y = movements)) +\n  geom_point(size = 0.5) +\n  geom_hline(yintercept = dfbetas_th, col = \"red\") +\n  geom_hline(yintercept = -dfbetas_th, col = \"red\")\n\n\n\n\n\n\n\n\nI want to take this opportunity to show that sometimes we can make visualisations even simpler. Remember, we used abs() to make all values positive? We can apply the same principle here. This way, we only need one line to indicate the threshold. In addition, we could also plot multiple variables at once. Instead of defining the aes() inside the ggplot() function, we can define it independently for each geom_point(). What do you think about the following version of the same plot?\n\nm1_dfbetas |&gt;\n  ggplot() +\n  geom_point(aes(x = reorder(rowname, abs(movements)),\n                 y = abs(movements),\n                 col = \"movements\"),\n             size = 0.5,\n             alpha = 0.5) +\n  geom_point(aes(x = rowname,\n                 y = abs(travel),\n                 col = \"travel\"),\n             size = 0.5,\n             alpha = 0.5) +\n  geom_hline(yintercept = dfbetas_th, col = \"#FF503A\") +\n  scale_color_manual(values = c(\"movements\" = \"#2EA5FF\",\n                                \"travel\" = \"#7DB33B\")) +\n\n  # Label the legend appropriately\n  labs(col = \"COVID measures\")\n\n\n\n\n\n\n\n\nAfter reorder()ing the variable movements and plotting travel as well, we notice that there might be slightly fewer outliers for movements than for travel. Thus, some observations affect some predictors more strongly than others.\n\n\n13.2.1.5 Reviewing the outliers\nAfter all this hard work and what turned out to be a very lengthy chapter, we finally arrive at the point where we check which observations we might wish to remove or handle in some shape or form. First, we want to know which observations are affected. Therefore we need to review our reference data in the object outliers. We want to see all observations identified by one or more diagnostic tools as an outlier. There are two ways to achieve this. First, we can use what we have learned so far and filter() each column for the value TRUE. This is the hard way of doing it, and if you have many more columns, this will take a little while and potentially drive you insane. The easy (and clever) way of filtering across multiple columns can be achieved by turning multiple columns into a single column. In the tidyverse, this is called pivot_longer() and we performed it earlier in Chapter @ref(sphericity). Thus, it might seem less complicated than expected. Let’s do this step-by-step as we did for pivot_wider() in Chapter @ref(chi-squared-test). Currently, our data has five columns, of which four are different measures to detect outliers. Our goal is to create a table that has only three columns:\n\nrowname, which we keep unchanged because it is the ID that identifies each observation in our data\noutlier_measure, which is the variable that indicates which measure was used to find an outlier.\nis.outlier, which contains the values from the tibble, i.e. the cell values of TRUE and FALSE.\n\nHere is an example of what we want to achieve. Imagine we have a smaller dataset, which contains three columns, two of which are outlier detection measures, i.e. studresid and mhbns:\n\ndata &lt;- tribble(\n  ~rowname,   ~ studresid,    ~mhbns,\n         \"1\",       FALSE,     FALSE,\n         \"2\",        TRUE,     FALSE,\n         \"3\",        TRUE,      TRUE\n)\n\n\ndata\n\n# A tibble: 3 × 3\n  rowname studresid mhbns\n  &lt;chr&gt;   &lt;lgl&gt;     &lt;lgl&gt;\n1 1       FALSE     FALSE\n2 2       TRUE      FALSE\n3 3       TRUE      TRUE \n\n\nTo filter for outliers using a single column/variable, we need to rearrange the values in studresid and mhbns. We currently have one column which captures the rowname, and two other columns which are both outlier detection methods. Thus, we could argue that studresid and mhbns measure the same, but with different methods. Therefore we can combine them into a factor, e.g. is.outlier, with two levels reflecting each outlier detection method. At the moment, we have two values recorded for every row in this dataset. For example, the first observation has the values studresid == FALSE and mhnbs == FALSE. If we want to combine these two observations into a single column, we need to have two rows with the rowname 1 to ensure that each row still only contains one observation, i.e. a tidy dataset. Therefore, we end up with more rows than our original dataset, hence, a ‘longer’ dataset. Here is how we can do this automatically:\n\ndata_long &lt;-\n  data |&gt;\n  pivot_longer(cols = !rowname,\n               names_to = \"outlier_measure\",\n               values_to = \"is.outlier\")\n\ndata_long\n\n# A tibble: 6 × 3\n  rowname outlier_measure is.outlier\n  &lt;chr&gt;   &lt;chr&gt;           &lt;lgl&gt;     \n1 1       studresid       FALSE     \n2 1       mhbns           FALSE     \n3 2       studresid       TRUE      \n4 2       mhbns           FALSE     \n5 3       studresid       TRUE      \n6 3       mhbns           TRUE      \n\n\nInstead of three rows, we have six, and all the outlier detection values (i.e. all TRUE and FALSE values) are now in one column, i.e. is.outlier. However, what exactly did just happen in this line of code? In light of what we specified above, we did three things inside the function pivot_longer():\n\nWe excluded rownames from being pivoted, i.e. cols = !rowname.\nWe specified a column where all the column names go to, i.e. names_to = \"outlier_measure\".\nWe defined a column where all cell values should be listed, i.e. values_to = \"is.outlier\".\n\nFrom here, it is straightforward to count() the number of is.outlier per rowname and filter() out those that return a value of TRUE. Everything we learned is now easily applicable without overly complicated functions or many repetitions. As mentioned earlier, pivoting datasets is an essential data wrangling skill, not matter which software you prefer.\n\ndata_long |&gt;\n  count(rowname, is.outlier) |&gt;\n  filter(is.outlier == \"TRUE\")\n\n# A tibble: 2 × 3\n  rowname is.outlier     n\n  &lt;chr&gt;   &lt;lgl&gt;      &lt;int&gt;\n1 2       TRUE           1\n2 3       TRUE           2\n\n\nEt voilà. We now counted the number of times an observation was detected considering both measures. This is scalable to more than two measures. Thus, we can apply it to our data as well.\n\noutliers_true &lt;-\n  outliers |&gt;\n  pivot_longer(!rowname,\n               names_to = \"measure\",\n               values_to = \"is.outlier\") |&gt;\n  count(rowname, is.outlier) |&gt;\n  filter(is.outlier == \"TRUE\")\n\noutliers_true\n\n# A tibble: 85 × 3\n   rowname is.outlier     n\n   &lt;fct&gt;   &lt;lgl&gt;      &lt;int&gt;\n 1 224     TRUE           2\n 2 225     TRUE           2\n 3 226     TRUE           2\n 4 227     TRUE           2\n 5 344     TRUE           1\n 6 345     TRUE           1\n 7 346     TRUE           1\n 8 349     TRUE           1\n 9 350     TRUE           1\n10 351     TRUE           1\n# ℹ 75 more rows\n\n\nThe result is a tibble which tells us that we identified 85 distinct outliers. This might sound like a lot, but we also have to remember that our dataset consists of 1188 observations, i.e. 7% of our data are outliers.\nSince the output is relatively long, we might want to plot the outcome to get an idea of the bigger picture.\n\noutliers_true |&gt;\n  ggplot(aes(x = reorder(rowname, n),\n             y = n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nA few observations were detected by three out of the four methods we used to detect outliers. There are some more which were detected by two different methods. However, there are also more than half of our outliers which were only detected by one method.\nAs highlighted in Chapter @ref(dealing-with-outliers), there are many ways we can go about outliers. To keep our analysis simple, I intend to remove those observations detected by multiple methods rather than only by one. Whether this approach is genuinely appropriate is a matter of further investigation. I take a very pragmatic approach with our sample data. However, it is worth reviewing the options available to transform data, as covered by @field2013discovering (p.203). Data transformation can help to deal with outliers instead of removing or imputing them. Still, data transformation comes with severe drawbacks and in most cases, using a bootstrapping technique to account for violations of parametric conditions is often more advisable. Bootstrapping refers to the process of randomly resampling data from your dataset and rerun the same test multiple times, e.g. 2000 times. Thus, there will be some samples that do not include the outliers. The process is somewhat similar to multiple imputation (see Chapter @ref(replacing-removing-missing-data)). Still, instead of estimating a specific value in our dataset, we estimate statistical parameters, e.g. confidence intervals or regression coefficients (i.e. estimates). The package rsample allows implementing bootstrapping straightforwardly. For an example of a detailed look at a bootstrapped regression, see Chapter ???. However, bootstrapping comes at the cost of lower power because we are running the test so many times.\nFor now, I settle on removing outliers entirely. The easiest way to remove these outliers is to combine our outliers dataset with our regression (m1_outliers). Now you will notice that having rowname as an ID allows us to match the values of each table to each other. Otherwise, this would not be possible.\n\n# Include outliers which were detected by multiple methods\noutliers_select &lt;-\n  outliers_true |&gt;\n  filter(n &gt; 1)\n\n# Keep only columns which are NOT included in outliers_select\nm1_no_outliers &lt;- anti_join(m1_outliers, outliers_select, by = \"rowname\")\n\nm1_no_outliers\n\n# A tibble: 1,152 × 13\n   rowname new_cases movements schools businesses travel    .hat .sigma  .cooksd\n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 1               0         0       0          0      0 0.00516 10077.  9.39e-5\n 2 2               0         0       0          0      0 0.00516 10077.  9.39e-5\n 3 3               0         0       0          0      0 0.00516 10077.  9.39e-5\n 4 4               0         0       0          0      0 0.00516 10077.  9.39e-5\n 5 5               0         0       0          0      0 0.00516 10077.  9.39e-5\n 6 6               0         0       0          0      0 0.00516 10077.  9.39e-5\n 7 7               0         0       0          0      0 0.00516 10077.  9.39e-5\n 8 8               0         0       0          0      0 0.00516 10077.  9.39e-5\n 9 9               0         0       0          0      0 0.00516 10077.  9.39e-5\n10 10              0         0       0          0      0 0.00516 10077.  9.39e-5\n# ℹ 1,142 more rows\n# ℹ 4 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .stdresid &lt;dbl&gt;,\n#   .studresid &lt;dbl&gt;\n\n\nThe function anti_join() does exactly what the name implies. It takes the first data frame (i.e. m1_outliers) and removes values that are included in the second data frame (i.e. outliers_select()). This is tremendously helpful when performing such complex outlier detection and removing them all in one go.\nAs the final step, we want to compare how the removal of outliers affected our model. Ideally, we managed to improve the regression. Thus, we compare our old model m1 with a new model m2.\n\n# Original regression\nm1 &lt;- lm(new_cases ~ movements + schools + businesses + travel,\n         data = covid_uk_ger)\n\n# Regression with outliers removed\nm2 &lt;- lm(new_cases ~ movements + schools + businesses + travel,\n         data = m1_no_outliers)\n\n# Compare the parameters between models\n# I added some reformatting, because\n# I prefer a regular tibble over a pre-formatted\n# table. Technically you only need the first line.\nparameters::compare_parameters(m1, m2) |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient.m1, p.m1, Coefficient.m2, p.m2) |&gt;\n  mutate(across(where(is.double), round, 3))\n\n# A tibble: 5 × 5\n  Parameter   Coefficient.m1  p.m1 Coefficient.m2  p.m2\n  &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)        -3024.      0       -1543.   0.031\n2 movements            -64.0     0         -15.8  0.174\n3 schools              382.      0         320.   0    \n4 businesses            90.5     0          82.7  0    \n5 travel                45.7     0          -1.27 0.875\n\n# Compare the performance between models\nperformance::compare_performance(m1, m2)\n\n# Comparison of Model Performance Indices\n\nName | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |      RMSE |     Sigma\n--------------------------------------------------------------------------------------------------------------\nm1   |    lm | 25279.5 (&lt;.001) | 25279.6 (&lt;.001) | 25310.0 (&lt;.001) | 0.229 |     0.227 | 10052.123 | 10073.343\nm2   |    lm | 24099.6 (&gt;.999) | 24099.7 (&gt;.999) | 24129.9 (&gt;.999) | 0.179 |     0.177 |  8398.110 |  8416.395\n\n\nIf you look at R2 (adj.) you might feel a sense of disappointment. How does the model explain less variance? If we consider AIC, BIC and RMSE, we improved the model because their values are lower for m2. However, it seems that the outliers affected R2, which means they inflated this model indicator. In other words, our regression explains less than we hoped for. However, we can be more confident that the predictions of this model will be more accurate.\nIf we inspect the estimates more closely, we also notice that movements (where we removed outliers) and travel are not significant anymore (i.e. p.m2 &gt; 0.05). We should remove these from our model since they do not help explain the variance of new_cases.\n\nm3 &lt;- lm(new_cases ~ schools + businesses,\n         data = m1_no_outliers)\n\nparameters::model_parameters(m3) |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient, p) |&gt;\n  mutate(across(where(is.double), round, 3))\n\n# A tibble: 3 × 3\n  Parameter   Coefficient     p\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)     -1664.  0.016\n2 schools           307.  0    \n3 businesses         78.9 0    \n\nperformance::model_performance(m3)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |     RMSE |    Sigma\n---------------------------------------------------------------------------\n24098.209 | 24098.243 | 24118.406 | 0.178 |     0.176 | 8407.515 | 8418.483\n\n\nSimilar to before, after removing the insignificant predictors, we end up with an equally good model (in terms of \\(adjusted\\ R^2\\), but we need fewer variables to explain the same amount of variance in new_cases. Our final model m3 is a parsimonious and less complex model.\n\n\n\n13.2.2 Standardised beta (\\(\\beta\\)) vs. unstandardised beta (\\(B\\))\nIn multiple regressions, we often use variables that are measured in different ways and which use different measurement units, e.g. currency, age, etc. Thus, it is fairly difficult to compare their regression coefficients without standardising the measurement units. Standardised scores (also called z-scores) imply that each variable has a distribution where the mean equals 0 and the standard deviation equals 1. By transforming our variables to z-scores, they become ‘unit free’ (P. Cohen, West, and Aiken 2014) (p. 25) and, therefore, we can easily compare them irrespective of their actual measurement units. Here is an example with a simplified dataset:\n\n# Create some data\ndata &lt;- tibble(x = c(1, 2, 3, 4))\n\n# Compute z-scores\ndata &lt;-\n  data |&gt;\n  mutate(x_scaled = scale(x))\n\n# Compare the mean and sd for each variable\n# and put it into a nice table\ntibble(\n  variable = c(\"x\", \"x_scaled\"),\n  mean = c(mean(data$x), mean(data$x_scaled)),\n  sd = c(sd(data$x), sd(data$x_scaled))\n)\n\n# A tibble: 2 × 3\n  variable  mean    sd\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 x          2.5  1.29\n2 x_scaled   0    1   \n\n\nIf you feel that my choice of values has likely affected the outcome, please feel free to change the values for x in this code chunk to whatever you like. The results for x_scaled will always remain the same. There is something else that remains the same: the actual distribution. Because we are only rescaling our data, we are not affecting the differences between these scores. We can show this in a scatterplot and by running a correlation. Both will show that these variables are perfectly correlated. Therefore, our transformations did not affect the relative relationship of values to each other.\n\ndata |&gt;\n  ggplot(aes(x = x,\n             y = x_scaled)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\n\n\n\n\ncorrelation::correlation(data)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(2) |         p\n----------------------------------------------------------------\nx          |   x_scaled | 1.00 | [1.00, 1.00] |  Inf | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 4\n\n\nIt is not a bad idea for regressions to report both unstandardised (\\(B\\)) and standardised (\\(\\beta\\)) values for each predictor. However, one could argue that it is a bit redundant. If you only want to report one estimate, it is common to provide the \\(\\beta\\) values rather than the unstandardised equivalent.\nUnfortunately, this is not the end of the story because ‘standardisation’ can mean different things. We can generate standardised \\(\\beta\\) after the fact (post-hoc) or decide to ‘refit’ our regression with standardised predictors, i.e. before we run the regression. Both options are appropriate, but the scores will differ slightly. However, the main idea remains the same: We want to compare different regression coefficients with each other. In R, many different packages offer standardised estimates. One package I particularly recommend is parameters. It provides various options for returning our estimates from a linear model by specifying the standardize argument (see detailed documentation). Of course, you can also opt to scale the variables by hand, using the scale() function, as we did in the previous example. All three approaches are shown in the following code chunk.\n\n# Scaling 'by hand'\nm3_scaled &lt;- lm(scale(new_cases) ~\n                  scale(schools) +\n                  scale(businesses),\n                data = m1_no_outliers)\n\nparameters::model_parameters(m3_scaled)\n\nParameter   | Coefficient |   SE |        95% CI |  t(1149) |      p\n--------------------------------------------------------------------\n(Intercept) |    2.00e-15 | 0.03 | [-0.05, 0.05] | 7.49e-14 | &gt; .999\nschools     |        0.29 | 0.03 | [ 0.23, 0.34] |     9.90 | &lt; .001\nbusinesses  |        0.21 | 0.03 | [ 0.16, 0.27] |     7.38 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\n# Scaling using 'parameters' package with refit\n# This is equivalent to scaling 'by hand'\nparameters::model_parameters(m3, standardize = \"refit\")\n\nParameter   | Coefficient |   SE |        95% CI |   t(1149) |      p\n---------------------------------------------------------------------\n(Intercept) |   -4.99e-15 | 0.03 | [-0.05, 0.05] | -1.87e-13 | &gt; .999\nschools     |        0.29 | 0.03 | [ 0.23, 0.34] |      9.90 | &lt; .001\nbusinesses  |        0.21 | 0.03 | [ 0.16, 0.27] |      7.38 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\n# Scaling using `parameters` package without refitting the model\nparameters::model_parameters(m3, standardize = \"posthoc\")\n\nParameter   | Std. Coef. |   SE |         95% CI | t(1149) |      p\n-------------------------------------------------------------------\n(Intercept) |       0.00 | 0.00 | [ 0.00,  0.00] |   -2.41 | 0.016 \nschools     |       0.29 | 0.03 | [ 0.23,  0.34] |    9.90 | &lt; .001\nbusinesses  |       0.21 | 0.03 | [ 0.16,  0.27] |    7.38 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nI prefer to standardise using the parameters package because it makes my life easier, and I need to write less code. I can also easily compare results before scaling my variables. The choice is yours, of course. Also, I tend to use standardize = refit in most regressions, since it is more in line with what most Social Scientists seem to report in their publications.\n\n\n13.2.3 Multicollinearity: The dilemma of highly correlated independent variables\nWe finished fitting our model, and we are likely exhausted but also happy that we accomplished something. However, we are not yet done with our analysis. One of the essential post-tests for multiple regressions is a test for multicollinearity or sometimes referred to as collinearity. The phenomenon of multicollinearity defines a situation in which some of our independent variables can be explained by other independent variables in our regression model. In other words, there exists a substantial correlation (a linear relationship) between two or more independent variables. However, it is crucial to not confuse this with correlations between a dependent variable and independent variables. Remember, a regression reflects the linear relationship between the predictor variables and the outcome variable. As such, we do hope to find a correlation between these variables, just not among the independent variables.\nIf we have evidence that multicollinearity exists in our data, we face some problems (P. Cohen, West, and Aiken 2014; Field 2013; Grandstrand 2004):\n\nWe cannot trust our regression coefficients, i.e. our estimates (\\(\\beta\\)).\nSince two independent variables can explain the same variance, it is unclear which one is important since both can be easily interchanged without affecting the model much.\nIt is impossible to test any hypotheses since we cannot trust our estimates.\nWe likely underestimate the variance our model can explain, i.e. our \\(R^2\\).\nWe produce more Type II errors, i.e. we likely reject significant predictors due to statistical insignificance.\n\nThe Variance Inflation Factor (VIF) and its little sibling Tolerance are methods to identify issues of multicollinearity. Compared to detecting outliers, it is very simple to compute these indicators and surprisingly uncomplicated to interpret them. The package performance offers a simple function called check_collinearity() which provides both. The tolerance can be calculated from the VIF as \\(tolerance = \\frac{1}{VIF}\\).\n\nperformance::check_collinearity(m3)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n    schools 1.18 [1.12, 1.28]         1.09      0.85     [0.78, 0.89]\n businesses 1.18 [1.12, 1.28]         1.09      0.85     [0.78, 0.89]\n\n\nAs the output already indicates, there is a Low Correlation between our independent variables. So, good news for us. In terms of interpretations, we find the following benchmarks as recommendations to determine multicollinearity (Field 2013) :\n\nVIF &gt; 10: Evidence for multicollinearity\nmean(VIF) &gt; 1: If the mean of all VIFs lies substantially above 1, multicollinearity might be an issue.\nTolerance &lt; 0.1: Multicollinearity is a severe concern. This is the same condition as VIF &gt; 10.\nTolerance &lt; 0.2: Multicollinearity could be a concern. This is the same condition as VIF &gt; 5.\n\nHowever, P. Cohen, West, and Aiken (2014) warns to not rely on these indicators alone. There is sufficient evidence that lower VIF scores could also cause issues. Thus, it is always important to still investigate relationships of independent variables statistically (e.g. correlation) and visually (e.g. scatterplots). For example, outliers can often be a cause for too high or too low VIFs. In short, the simplicity in the computation of these indicators should not be mistaken as a convenient shortcut.\nMulticollinearity can also exist among residuals, i.e. the error terms that are associated with each predictor. Regressions assume that errors are independent. The Durbin-Watson Test offers an equally easy way to detect multicollinearity among residuals. We can use the car package and the function durbinWatsonTest() to retrieve the relevant information.\n\ncar::durbinWatsonTest(m3) |&gt;\n  broom::tidy()\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1     0.135       0           0.930 Durbin-Watson Test two.sided  \n\n\nThe statistic of the Durbin-Watson Test can range from 0 to 4, where 2 indicates no correlation. Luckily for us, we do not have to guess whether the difference from our computation is significantly different from 2 because we also get the p.value for such test. In our case, our model suffers from multicollinearity of error terms because \\(p.value &lt; 0.05\\).\nIf we wanted to remedy multicollinearity, we could consider, for example:\n\nRevisiting the regression model and potentially dropping those variables that measure the same or similar underlying factors (P. Cohen, West, and Aiken 2014).\nWe could collect more data because a larger dataset will always increase the precision of the regression coefficients (P. Cohen, West, and Aiken 2014).\nUse different modelling techniques (P. Cohen, West, and Aiken 2014) which can resolve such issues, for example, using generalised least squares (GLS) models and transform some variables of concern (Grandstrand 2004).\n\nThe cause for this multicollinearity of residuals is often rooted in either ‘clustered data’ or a ‘serial dependency’ (P. Cohen, West, and Aiken 2014). In our dataset, both could apply. First, we have data that was collected over time, which could lead to wrong standard errors if not accounted for. Second, we have data from two different countries (United Kingdom and Germany). Thus, our data might be clustered.\nTo remove serial dependency effects, we would have to transform data and account for the correlation across time. Such a technique is shown in P. Cohen, West, and Aiken (2014) (p.149). Furthermore, to counteract the issue of clustered data, we need to use multilevel regression models, also known as hierarchical regressions, which we will cover in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "13_regressions.html#sec-hierarchical-regression",
    "href": "13_regressions.html#sec-hierarchical-regression",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "13.3 Hierarchical regression",
    "text": "13.3 Hierarchical regression\nAdding independent variables to a regression model often happens in a stepwise approach, i.e. we do not add all independent variables at once. Instead, we might first add the essential variables (based on prior research), run the regression, and then examine the results. After that, we add more variables that could explain our dependent variable and rerun the regression. This results in two models which we can compare and identify improvements.\nIn hierarchical regressions, we most frequently distinguish three types of independent variables, which also reflect the order in which we add variables to a multiple regression:\n\nControl variables: These are independent variables that might affect the dependent variable somehow, and we want to make sure its effects are accounted for. Control variables tend to be not the primary focus of a study but ensure that other independent variables (the main effects) are not spurious. Control variables are added first to a regression.\nMain effects variables: These are the independent variables that the researcher expects will predict the dependent variable best. Main effects variables are added after any control variables.\nModerating variables: These are variables that attenuate the strength of the relationship between independent and dependent variables. They are also referred to as interactions or interaction terms. Moderating variables are added last, i.e. after main effects variables and control variables.\n\nIn the field of Social Sciences, it is rare not to find control variables and/or moderators in multiple regressions because the social phenomena we tend to investigate are usually affected by other factors as well. Classic control variables or moderators are socio-demographic variables, such as age and gender. The following two chapters cover control variables and moderation effects separately from each other. However, it is not unusual to find both types of variables in the same regression model. Thus, they are not mutually exclusive approaches but simply different types of independent variables.\n\n13.3.1 Regressions with control variables\nHierarchical regression implies that we add variables step-by-step, or as some call it, ‘block-wise’. Each block represents a group of variables. Control variables tend to be the first block of variables added to a regression model. However, there are many other ways to perform multiple regression, for example, starting with all variables and removing those that are not significant, which P. Cohen, West, and Aiken (2014) calls a ‘tear-down’ approach (p. 158). As the title indicates, we take less of an exploratory approach to our analysis because we define the hierarchy, i.e. the order, in which we enter variables. In the Social Sciences, this is often the preferred method, so I cover it in greater detail.\nHowever, I should probably explain what the purpose of entering variables in a stepwise approach is. As we discussed in the previous chapter, we sometimes face issues of multicollinearity, which makes it difficult to understand which variables are more important than others in our model. Therefore, we can decide to enter variables that we want to control first and then create another model containing all variables. The procedure is relatively straightforward for our research focus when predicting new_cases of COVID-19:\n\nCreate a model m1 (or whatever name you want to give it) containing the dependent variable new_cases and a control variable, for example country.\nInspect the results of this model and note down the performance measures.\nCreate another model m2 and include the control variable country and all other independent variables of interest, i.e. schools, and businesses.\nInspect the results of this model and note down the performance measures.\nCompare models m1 and m2 to see whether they are significantly different from each other. We can use anova() to perform this step.\n\nLet’s put these steps into action and start with formulating our first model. I choose country as a control variable because we have sufficient evidence that clustered data could be a reason for the high autocorrelation of residuals we found in Chapter @ref(multicollinearity-the-dilemma-of-highly-correlated-independent-variables). For all computations in this section, we use the original dataset, i.e. covid_uk_ger, because we changed the model and therefore would have to revisit outliers from scratch, which we shall skip. We also have to remove observations with missing data to allow comparisons of models and ensure they have the same degrees of freedom (i.e. the same number of observations). So, we begin by selecting our variables of interest and then remove missing data.\n\nhr_data &lt;-\n  covid_uk_ger |&gt;\n  select(new_cases, country, schools, businesses) |&gt;\n  droplevels() |&gt;\n  na.omit()\n\nYou find a new function called droplevels() in this code chunk. This function removes unused factor levels from a factor. To demonstrate this more clearly, let me share an example:\n\n# A factor with all levels\n(gender &lt;- as_factor(c(\"female\", \"male\", \"NA\")))\n\n[1] female male   NA    \nLevels: female male NA\n\n\n\n# Subset our data by picking the first two values only\n(gender2 &lt;- gender[1:2])\n\n[1] female male  \nLevels: female male NA\n\n\n\n# Dropping unused levels\n(gender3 &lt;- gender[1:2] |&gt; droplevels())\n\n[1] female male  \nLevels: female male\n\n\nThe problem we encounter is that even though we only have data points with female and male values in gender2, it still keeps the level of NA, based on the original factor gender. In my experience, it is always good to remove unused categories because sometimes they can lead to unexpected outcomes. Thus, it is a matter of good practice than an absolute necessity.\nWe have to recode factors into so-called dummy variables or indicator variables. Dummy variables represent categories as 0s (i.e. FALSE for this observation) and 1s (TRUE for this observation). The good news is, the lm() function will do this automatically for us. If you want to inspect the coding ex-ante, you can use the function contrasts().\n\ncontrasts(hr_data$country)\n\n               United Kingdom\nGermany                     0\nUnited Kingdom              1\n\n\nThe column reflects the coding and the rows represent the levels of our factor. If we had more levels, we will find that the coding will always be the number of levels minus 1. This is a common mistake that novice analysts easily make. You might think you need to have a dummy variable for Germany (i.e. 0 and 1) and a dummy variable for United Kingdom (i.e. 0 and 1). However, all you really need is one variable, which tells us whether the country is the United Kingdom (i.e. 1) or not (i.e. 0).\nIf our control variable has more than two levels, for example, by adding Italy, the dummy coding will change to the following:\n\nhr_uk_ger_ita &lt;-\n  covid |&gt;\n  filter(country == \"United Kingdom\" |\n           country == \"Germany\" |\n           country == \"Italy\") |&gt;\n  droplevels()\n\ncontrasts(hr_uk_ger_ita$country)\n\n               Italy United Kingdom\nGermany            0              0\nItaly              1              0\nUnited Kingdom     0              1\n\n\nWith this new dataset of three countries, our regression would include more control variables because we create a new control variable for each level of the factor (minus one level!). For an example of such a scenario, please look at the case study in Chapter ???.\nReturning to our hierarchical regression, we can build our first model, i.e. m1, which only contains our control variable country.\n\nm1 &lt;- lm(formula = new_cases ~ country,\n         data = hr_data)\n\nparameters::model_parameters(m1, standardize = \"refit\")\n\nParameter                | Coefficient |   SE |         95% CI | t(1186) |      p\n---------------------------------------------------------------------------------\n(Intercept)              |       -0.18 | 0.04 | [-0.26, -0.10] |   -4.53 | &lt; .001\ncountry [United Kingdom] |        0.37 | 0.06 | [ 0.25,  0.48] |    6.40 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\nperformance::model_performance(m1)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma\n-----------------------------------------------------------------------------\n25542.724 | 25542.744 | 25557.964 | 0.033 |     0.033 | 11258.076 | 11267.564\n\n\nOur control variable turns out to be significant for our model, but it explains only a small proportion of the variance in new_cases. If you are testing hypotheses, you would consider this a good result because you do not want your control variables to explain too much variance. At the same time, it is a significant variable and should be retained in our model. Let’s construct our next model, m1, by adding the main effects variables and comparing our models.\n\nm2 &lt;- lm(formula = new_cases ~\n           schools +\n           businesses +\n           country,\n         data = hr_data)\n\nparameters::compare_parameters(m1, m2, standardize = \"refit\") |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient.m1, p.m1, Coefficient.m2, p.m2) |&gt;\n  mutate(across(where(is.double), round, 3))\n\n# A tibble: 4 × 5\n  Parameter                Coefficient.m1  p.m1 Coefficient.m2  p.m2\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)                      -0.183     0         -0.235     0\n2 country (United Kingdom)          0.365     0          0.469     0\n3 schools                          NA        NA          0.328     0\n4 businesses                       NA        NA          0.246     0\n\n\n\nperformance::compare_performance(m1, m2)\n\n# Comparison of Model Performance Indices\n\nName | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |      RMSE |     Sigma\n--------------------------------------------------------------------------------------------------------------\nm1   |    lm | 25542.7 (&lt;.001) | 25542.7 (&lt;.001) | 25558.0 (&lt;.001) | 0.033 |     0.033 | 11258.076 | 11267.564\nm2   |    lm | 25234.1 (&gt;.999) | 25234.1 (&gt;.999) | 25259.5 (&gt;.999) | 0.257 |     0.255 |  9870.095 |  9886.753\n\n\nAfter adding all our variables, \\(adjusted\\ R^2\\) went up from 0.033 to 0.255. While this might seem like a considerably improvement, we have to perform a statistical test to compare the two models. This leads us back to comparing groups, and in many ways, this is what we do here by using the function anova(), but we compare models based on the residual sum of squares, i.e. the amount of error the models produce. Thus, if the ANOVA returns a significant result, m1 shows a significantly reduced residual sum of squares compared to m2. Therefore, m2 would be the better model.\n\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: new_cases ~ country\nModel 2: new_cases ~ schools + businesses + country\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1186 1.5057e+11                                   \n2   1184 1.1573e+11  2 3.4839e+10 178.21 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results confirm that our model significantly improved. You might argue that this is not surprising because we added those variables which already worked in the final model of Chapter @ref(multicollinearity-the-dilemma-of-highly-correlated-independent-variables). However, the important takeaway is that our control variable country helps us explain more variance in new_cases. Comparing the model with and without the control variable, we would find that the \\(adjusted\\ R^2\\) improves our model by about 25%. As such, it is worth keeping it as part of our final regression model. Here is evidence of this improvement:\n\nm0 &lt;- lm(formula = new_cases ~\n           schools +\n           businesses,\n         data = hr_data)\n\nperformance::compare_performance(m0, m2)\n\n# Comparison of Model Performance Indices\n\nName | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |      RMSE |     Sigma\n--------------------------------------------------------------------------------------------------------------\nm0   |    lm | 25309.1 (&lt;.001) | 25309.2 (&lt;.001) | 25329.5 (&lt;.001) | 0.207 |     0.206 | 10195.397 | 10208.294\nm2   |    lm | 25234.1 (&gt;.999) | 25234.1 (&gt;.999) | 25259.5 (&gt;.999) | 0.257 |     0.255 |  9870.095 |  9886.753\n\n\n\nanova(m0, m2)\n\nAnalysis of Variance Table\n\nModel 1: new_cases ~ schools + businesses\nModel 2: new_cases ~ schools + businesses + country\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1185 1.2349e+11                                   \n2   1184 1.1573e+11  1 7754483300 79.332 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are many more combinations of models we could test. For example, instead of specifying two models (m1, m2), we could also add independent variables one at a time and compare the resulting models to a baseline model (m0). Thus, we end up with more models to compare, for example:\n\nm1 &lt;- lm(formula = new_cases ~ country, data = hr_data)\nm2 &lt;- lm(formula = new_cases ~ country + schools, data = hr_data)\nm3 &lt;- lm(formula = new_cases ~ country + schools + businesses, data = hr_data)\n\nAll these decisions have to be guided by the purpose of your research and whether you explore your data or have pre-defined hypotheses. However, the steps remain the same in terms of computation in R.\n\n\n13.3.2 Moderated regression\nWhile regressions with control variables help us decide whether to include or exclude variables by controlling for another variable, moderation models imply that we expect a certain interaction between an independent variable and a so-called moderator. A moderator also enters the equation on the right-hand side and therefore constitutes an independent variable, but it is usually entered after control variables and main effects variables.\nIn our quest to find a model that predicts new COVID cases, let’s assume that country is not a control variable but a moderating one. We could suspect that the measures for businesses and schools were significantly differently implemented in each country. Therefore, the strength of the relationship between businesses/schools and new_cases varies depending on the country. In other words, we could assume that measures showed different effectiveness in each of the countries.\nWhen performing a moderation regression, we assume that the relationship between independent variables and the dependent variable differs based on a third variable, in our case, country. Thus, if we visualise the idea of moderation, we would plot the data against each other for each country separately and fit two (instead of one) regression lines by defining the colours of each group with col = country.\n\n# Plotting the interaction between schools and country\nhr_data |&gt;\n  ggplot(aes(x = schools,\n             y = new_cases,\n             col = country)) +\n  geom_jitter(width = 5,\n              size = 0.5,\n              alpha = 0.5) +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\n\n\nFigure 13.1: Plotting moderation effects for schools.\n\n\n\n\n\n\n# Plotting the interaction between schools and country\nhr_data |&gt;\n  ggplot(aes(x = businesses,\n             y = new_cases,\n             col = country)) +\n  geom_jitter(width = 5,\n              size = 0.5,\n              alpha = 0.5) +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\nPlotting moderation effects for business.\n\n\n\n\nThere are three interesting insights we can gain from these plots:\n\nIn terms of business measures, the United Kingdom seems to have more observations at the lower end, i.e. taking less protective measures than Germany. In contrast, for schools, it is the opposite.\nFor schools, the UK reports more frequently higher numbers of new_cases even if measures taken are high, which results in a steeper regression line.\nHowever, for businesses, the United Kingdom had barely any new_cases when the measures were high, but Germany still reports high numbers of new cases when tight measures were taken.\n\nGiven the difference in the slope of the regression lines, we have to assume that the \\(\\beta\\)s for Germany are quite different from the ones for the United Kingdom. It seems that the relationship between new_cases and business/schools can be partially explained by country. Therefore, we could include country as a moderator and introduce the interaction of these variables with each other into a new model, i.e. m4.\n\nm4 &lt;- lm(formula = new_cases ~\n           schools +\n           businesses +\n           schools * country +  # moderator 1\n           businesses * country, # moderator 2\n         data = hr_data)\n\nparameters::compare_parameters(m0, m4, standardize = \"refit\") |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient.m0, p.m0, Coefficient.m4, p.m4) |&gt;\n  mutate(across(where(is.double), round, 3))\n\n# A tibble: 6 × 5\n  Parameter                            Coefficient.m0  p.m0 Coefficient.m4  p.m4\n  &lt;chr&gt;                                         &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)                                   0         1         -0.229 0    \n2 schools                                       0.366     0          0.077 0.104\n3 businesses                                    0.168     0          0.183 0    \n4 country (United Kingdom)                     NA        NA          0.546 0    \n5 schools × country (United Kingdom)           NA        NA          0.274 0    \n6 businesses × country (United Kingdo…         NA        NA          0.371 0    \n\n\n\nperformance::compare_performance(m0, m4)\n\n# Comparison of Model Performance Indices\n\nName | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |      RMSE |     Sigma\n--------------------------------------------------------------------------------------------------------------\nm0   |    lm | 25309.1 (&lt;.001) | 25309.2 (&lt;.001) | 25329.5 (&lt;.001) | 0.207 |     0.206 | 10195.397 | 10208.294\nm4   |    lm | 25149.9 (&gt;.999) | 25150.0 (&gt;.999) | 25185.5 (&gt;.999) | 0.310 |     0.307 |  9510.456 |  9534.564\n\n\n\nanova(m0, m4)\n\nAnalysis of Variance Table\n\nModel 1: new_cases ~ schools + businesses\nModel 2: new_cases ~ schools + businesses + schools * country + businesses * \n    country\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1185 1.2349e+11                                   \n2   1182 1.0745e+11  3 1.6035e+10 58.795 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe moderation effect is quite strong. Compared to our baseline model m0, which only contains businesses and schools as predictors, our new model m4 considerably outperforms it with an \\(adjusted\\ R^2 = 0.307\\). Our previous model with country as a control variable achieved an \\(adjusted\\ R^2 = 0.255\\). I would argue this is quite an improvement.\nWhile significant moderation effects help us make better predictions with our model, they come with a caveat: Interpreting the relative importance of the main effects becomes considerably more difficult because their impact depends on the level of our moderation variable. As such, you cannot interpret the main effects without considering the moderation variable as well. You might have noticed that our variable schools is not significant anymore. However, this does not imply it is not essential because its significance depends on the level of our moderator, i.e. whether we look at Germany or the United Kingdom.\n\n\n13.3.3 Centering predictors: Making \\(\\beta\\)s more interpretable\nIn Chapter @ref(standardised-beta-vs-unstandardised-beta), we covered procedures to standardise \\(\\beta\\) coefficients, which allow us to compare independent variables based on different measurement units or if we want to compare coefficients across multiple models. Whenever we perform a regression with an interaction term, for example, the moderation regression in the previous chapter, we often have to make similar adjustments called ‘centering’.\nTo explain why we might need centering, we should look at an example. Let’s consider a simple model using the wvs_nona dataset. Assume we want to know whether freedom_of_choice can predict satisfaction with life, moderated by age. If we formulate this as a model, we can write the following:\n\nmodel &lt;- lm(formula = satisfaction ~\n              freedom_of_choice +\n              freedom_of_choice * age,\n            data = wvs_nona)\n\nparameters::model_parameters(model) |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient, t, df_error, p)\n\n# A tibble: 4 × 5\n  Parameter             Coefficient     t df_error         p\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 (Intercept)              4.18     57.7     69574 0        \n2 freedom_of_choice        0.382    40.1     69574 0        \n3 age                     -0.00275  -1.77    69574 0.0772   \n4 freedom_of_choice:age    0.000869  4.21    69574 0.0000251\n\n\n\nperformance::model_performance(model)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n---------------------------------------------------------------------\n2.982e+05 | 2.982e+05 | 2.982e+05 | 0.180 |     0.180 | 2.062 | 2.062\n\n\nThe \\(\\beta\\) score of freedom_of_choice unfolds the most substantial effect on satisfaction, but assumes that age = 0. In regressions with interaction terms, the coefficient \\(\\beta\\) of a variable reflects the change in the dependent variables if other variables are set to zero. If this is a logical value for the variables, then no further steps are required. However, if a score of zero on any predictor is not meaningful, it is recommended to engage in centering variables (see P. Cohen, West, and Aiken 2014; Field 2013). In our model, the coefficient of freedom_of_choice assumes that age is equal to 0. However, no participant in our sample will have been of age 0, which renders any interpretation of the \\(\\beta\\) coefficient meaningless. Instead, it is more meaningful to retrieve a \\(\\beta\\) score which is based on the average age of people in our sample. This is what centering can achieve.\nThere are mainly two very commonly used techniques in the Social Sciences to center variables: grand mean centering and group mean centering. The difference between these two approaches lies in their reference sample. For group mean centering, we compute means for each group independently and for grand mean centering, we calculate the mean for all observations irrespective of any groupings. Centered scores are almost the same as z-scores, but without using standard deviation as a measurement unit. If we divided centered scores by the standard deviation, we would have computed z-scores. The following formula summarises the computation of centered variables:\n\n\\(x_{n_{centered}} = x_n - \\overline{x}\\)\n\nIn other words, to achieve a centered variable (\\(x_{n_{centered}}\\)), we have to take the original observation (\\(x_n\\)) and subtract the mean of the variable (\\(\\bar{x}\\)). How we define \\(\\bar{x}\\) determines whether we perform grand mean centering (using the entire dataset) or group mean centering (using subsets of our data, i.e. groups). We already know all the functions needed to compute centered variables. Since the age of 0 is not meaningful in our study, we should center this variable around its mean5.\n\n# Grand mean centering\nwvs_nona_c &lt;-\n  wvs_nona |&gt;\n  mutate(age_c = age - mean(age))\n\nwvs_nona_c |&gt;\n  select(age, age_c) |&gt;\n  head()\n\n# A tibble: 6 × 2\n    age age_c\n  &lt;dbl&gt; &lt;dbl&gt;\n1    60 17.4 \n2    47  4.41\n3    48  5.41\n4    62 19.4 \n5    49  6.41\n6    51  8.41\n\n\n\n# Group mean centering by country\nwvs_nona |&gt;\n  group_by(country) |&gt;\n  mutate(age_c = age - mean(age)) |&gt;\n  \n  select(age, age_c) |&gt;\n  sample_n(1)\n\nAdding missing grouping variables: `country`\n\n\n# A tibble: 48 × 3\n# Groups:   country [48]\n   country      age   age_c\n   &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Andorra       76  29.2  \n 2 Argentina     38  -4.55 \n 3 Australia     56   1.77 \n 4 Bangladesh    51  14.4  \n 5 Bolivia       38  -0.333\n 6 Brazil        49   5.45 \n 7 Myanmar       46   5.57 \n 8 Chile         30 -15.3  \n 9 China         34 -10.6  \n10 Taiwan        66  17.7  \n# ℹ 38 more rows\n\n\n\n## The group means used\nwvs_nona |&gt;\n  group_by(country) |&gt;\n  summarise(mean = round(mean(age), 0))\n\n# A tibble: 48 × 2\n   country     mean\n   &lt;fct&gt;      &lt;dbl&gt;\n 1 Andorra       47\n 2 Argentina     43\n 3 Australia     54\n 4 Bangladesh    37\n 5 Bolivia       38\n 6 Brazil        44\n 7 Myanmar       40\n 8 Chile         45\n 9 China         45\n10 Taiwan        48\n# ℹ 38 more rows\n\n\nThe mutate() function is the same for both approaches. However, we used group_by() to compute the mean for each country and then centered the scores accordingly for group mean centering. Thus, observations from Australia will be centered around the mean score of 54, while observations from Myanmar will be centered around the mean value of 40.\nReturning to our investigation, I opted to choose grand mean centering. Now we can use our newly centered variable in the regression to see the impact it had on our \\(\\beta\\)s.\n\nmodel_c &lt;- lm(formula = satisfaction ~\n                freedom_of_choice +\n                freedom_of_choice * age_c,\n              data = wvs_nona_c)\n\nparameters::compare_parameters(model, model_c) |&gt;\n  as_tibble() |&gt;\n  select(Parameter, Coefficient.model, p.model, Coefficient.model_c, p.model_c) |&gt;\n  mutate(across(where(is.double), round, 3))\n\n# A tibble: 6 × 5\n  Parameter              Coefficient.model p.model Coefficient.model_c p.model_c\n  &lt;chr&gt;                              &lt;dbl&gt;   &lt;dbl&gt;               &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                        4.18    0                   4.06      0    \n2 freedom of choice                  0.382   0                   0.419     0    \n3 age                               -0.003   0.077              NA        NA    \n4 freedom of choice × a…             0.001   0                  NA        NA    \n5 age c                             NA      NA                  -0.003     0.077\n6 freedom of choice × a…            NA      NA                   0.001     0    \n\n\nIn contrast to standardising the coefficients, it is not necessary to center all predictors in your dataset. For example, we only centered age and not freedom_of_choice in our case, because a value of 0 for the latter variable seems plausible. As we can tell from the output, the \\(\\beta\\) for freedom_of_choice has changed, but all other coefficients remained the same. In general, grand mean centering has the following effects:\n\ncentering does not affect the \\(\\beta\\) of the centered variable (i.e. age), because we subtract a constant (i.e. the mean) from all observations equally,\ncentering does not affect the interaction term (i.e. freedom_of_choice * age_c), which is also known as the ‘higher-order’ predictor,\ncentering changes the \\(\\beta\\) of the independent variable in the interaction term (also known as ’lower-order predictor), (i.e. freedom_of_choice)\ncentering does not change \\(\\beta\\)s of other variables that are not interacting with a centered variable.\ncentering any of the predictors changes the intercept.\n\nTo conclude, centering plays a crucial role if the interpretation of \\(\\beta\\) coefficients is essential. If this is true, we should center all variables where a value of 0 is not meaningful. However, keep in mind that we do not change the model itself, i.e. we cannot improve the performance of a model by centering certain variables. Consequently, if we are only interested in the performance of our model, centering does not matter.\n\nperformance::compare_performance(model, model_c)\n\n# Comparison of Model Performance Indices\n\nName    | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n---------------------------------------------------------------------------------------------------------\nmodel   |    lm | 3.0e+05 (0.500) | 3.0e+05 (0.500) | 3.0e+05 (0.500) | 0.180 |     0.180 | 2.062 | 2.062\nmodel_c |    lm | 3.0e+05 (0.500) | 3.0e+05 (0.500) | 3.0e+05 (0.500) | 0.180 |     0.180 | 2.062 | 2.062",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "13_regressions.html#sec-ols-alternatives",
    "href": "13_regressions.html#sec-ols-alternatives",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "13.4 Other regression models: Alternatives to OLS",
    "text": "13.4 Other regression models: Alternatives to OLS\nThis book only covers linear regressions, which are considered Ordinary Least Squares (OLS) regression models. These are by far the most frequently used models in the Social Sciences. However, what can we do if the assumptions of OLS models are violated, for example, the relationship of variables looks curved or if our dependent variable is dichotomous and not continuous? In such cases, we might have to consider other types of regression models. There are several different approaches, but we most frequently find one of the following:\n\nPolynomial regressions, which are used for curvilinear relationships between variables.\nGeneralised Linear models, such as Logistic regressions (when the predictor is a logical variable) or Poisson regressions (when we model count data, i.e. frequencies and contingency tables).\n\nThese are specialised models, and learning them requires a more in-depth knowledge of their mechanics which would go beyond the scope of a book that intends to cover the basics of conducting statistical analysis in R. However, P. Cohen, West, and Aiken (2014) offers an excellent introduction to both approaches.\n\n\n\n\nBoehmke, Brad, and Brandon Greenwell. 2019. Hands-on Machine Learning with r. Chapman; Hall/CRC.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences New York. NY: Academic Press.\n\n\nCohen, Patricia, Stephen G West, and Leona S Aiken. 2014. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Psychology press.\n\n\nCook, R Dennis, and Sanford Weisberg. 1982. Residuals and Influence in Regression. New York: Chapman; Hall.\n\n\nField, Andy. 2013. Discovering Statistics Using IBM SPSS Statistics. Sage Publications.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGrandstrand, Ove. 2004. “Durbin-Watson Statistic.” In The SAGE Encyclopedia of Social Science Research Methods, edited by Bryman Lewis-Beck Michael S. Thousand Oaks, California. https://methods.sagepub.com/reference/the-sage-encyclopedia-of-social-science-research-methods.\n\n\nHoaglin, David C, and Roy E Welsch. 1978. “The Hat Matrix in Regression and ANOVA.” The American Statistician 32 (1): 17–22.\n\n\nStevens, James P. 2012. Applied Multivariate Statistics for the Social Sciences. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "13_regressions.html#footnotes",
    "href": "13_regressions.html#footnotes",
    "title": "13  Regression: Creating models to predict future observations",
    "section": "",
    "text": "Germany is called ‘Deutschland’ in German, hence the abbreviation ‘DEU’ according to the ISO country code.↩︎\nBe aware that such insights might also fundamentally question the relationship between variables, i.e. are the measures truly independent variables or rather dependent ones?↩︎\nThis function does not take the model as an attribute, but instead requires the data, the column means and the covariance. Thus, we have to specify this function in the following way: mahalanobis(data, colMeans(data), cov(data))↩︎\nFunction from rstatix package which automatically classifies values as outliers for us.↩︎\nIt is not required to choose the mean as a way to center variables. Any value that is meaningful can be used. However, the mean often tends to be the most meaningful value to perform centering.↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression: Creating models to predict future observations</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html",
    "href": "14_mixed_methods.html",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "",
    "text": "14.1 The tidy process of working with textual data\nBefore we dive head-first into this exciting facet of research, we first need to understand how we can represent and work with qualitative data in R. Similar to working with tidy quantitative data, we also want to work with tidy qualitative data. This book adopts the notion of tidy text data from Silge and Robinson (2017), which follows the terminology used in Corpus Linguistics:\nIn other words, what used to be an ‘observation’ is now called a ‘token’. Therefore, a token is the smallest chosen unit of analysis. The term ‘word token’ can easily be confused with ‘word type’. The first one usually represents the instance of a ‘type’. Thus, the frequency of a word type is determined by the number of its tokens. For example, consider the following sentence, which consists of five tokens, but only four types:\n(sentence &lt;- \"This car, is my car.\")\n\n[1] \"This car, is my car.\"\nBecause the word car appears twice, we have two tokens of the word type car in our dataset. However, how would we represent this in a rectangular dataset? If each row represents one token, we would expect to have five rows in our data frame. As mentioned above, the process of converting a text into individual tokens is called ‘tokenization’. To turn our text into a tokenized data frame, we have to perform two steps:\nWhile the first part can be achieved using tibble(), we need a new function to perform tokenisation. The package tidytext will be our primary tool of choice, and it comes with the function unnest_tokens(). Among many other valuable applications, unnest_tokens() can tokenize the text for us.\n# Convert text object into a data frame\n(df_text &lt;- tibble(text = sentence))\n\n# A tibble: 1 × 1\n  text                \n  &lt;chr&gt;               \n1 This car, is my car.\n# Tokenization\nlibrary(tidytext)\n(df_text &lt;- df_text |&gt; unnest_tokens(output = word,\n                                      input = text))\n\n# A tibble: 5 × 1\n  word \n  &lt;chr&gt;\n1 this \n2 car  \n3 is   \n4 my   \n5 car\nIf you paid careful attention, two elements got lost in our data during the process of tokenization: the . and , are no longer included. In most cases, commas and full stops are of less interest because they tend to carry no particular meaning when performing such an analysis. The function unnest_tokens() conveniently removes them for us and also converts all letters to lower-case.\nFrom here onwards we find ourselves in more familiar territory because the variable word looks like any other character variable we encountered before. For example, we can count() the number of occurrences for each word.\ndf_text |&gt; count(word)\n\n# A tibble: 4 × 2\n  word      n\n  &lt;chr&gt; &lt;int&gt;\n1 car       2\n2 is        1\n3 my        1\n4 this      1\nSince we have our data nicely summarised, we can also easily visualise it using ggplot().\ndf_text |&gt;\n  count(word) |&gt;\n  ggplot(aes(x = word,\n             y = n)) +\n  geom_col()\nOnce we have converted our data into a data frame, all the techniques we covered for non-quantitative variables can be applied. It only requires a single function to turn ourselves into novice corpus analysts. If this sounds too easy, then you are right. Hardly ever will we retrieve a tidy dataset that allows us to work with it in the way we just did. Data cleaning and wrangling still need to be performed but in a slightly different way. Besides, there are many other ways to tokenize text than using individual words, some of which we cover in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html#sec-tidy-process-for-textual-data",
    "href": "14_mixed_methods.html#sec-tidy-process-for-textual-data",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "",
    "text": "We (…) define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens.\n\n\n\n\n\nConvert our text object into a data frame, and\nSplit this text into individual words, i.e. tokenization.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html#stop-words-removing-noise-in-the-data",
    "href": "14_mixed_methods.html#stop-words-removing-noise-in-the-data",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "14.2 Stop words: Removing noise in the data",
    "text": "14.2 Stop words: Removing noise in the data\nIn the previous example, we already performed an important data wrangling process, i.e. tokenization. However, besides changing the text format, we also need to take care of other components in our data that are usually not important, for example, removing ‘stop words’. To showcase this step (and a little more), I will draw on the imdb_top_250 dataset.\nIn marketing, finding the right product name is a laborious task and requires careful attention. After all, our first impression of a product is partially informed by its name. Consequently, when promoting a movie, one might wonder whether more popular words in movie titles can lead to greater success, i.e. a higher ranking on IMDb. With the help of the imdb_top_250 dataset, we can empirically investigate this matter. However, we first have to tidy the data and then clean it. An essential step in this process is the removal of ‘stop words’.\n‘Stop words’ are words that we want to exclude from our analysis, because they carry no particular meaning. The tidytext package comes with a data frame that contains common stop words in English.\n\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n\nIt is important to acknowledge that there is no unified standard of what constitutes a stop word. The data frame stop_words covers many words, but in your research context, you might not even want to remove them. For example, if we aim to identify topics in a text, stop words would be equivalent to white noise, i.e. many of them are of no help to identify relevant topics. Instead, stop words would confound our analysis. For obvious reasons, stop words highly depend on the language of your data. Thus, what works for English will not work for German, Russian or Chinese. You will need a separate set of stop words for each language.\nThe removal of stop words can be achieved with a function we already know: anti-join(). In other words, we want to subtract all words from stop_words from a given text. Let’s begin with the tokenization of our movie titles.\n\ntitles &lt;-\n  imdb_top_250 |&gt;\n  unnest_tokens(word, title) |&gt;\n  select(imdb_rating, word)\n\ntitles\n\n# A tibble: 712 × 2\n   imdb_rating word      \n         &lt;dbl&gt; &lt;chr&gt;     \n 1         9.3 the       \n 2         9.3 shawshank \n 3         9.3 redemption\n 4         9.2 the       \n 5         9.2 godfather \n 6         9   the       \n 7         9   dark      \n 8         9   knight    \n 9         9   the       \n10         9   godfather \n# ℹ 702 more rows\n\n\nOur new data frame has considerably more rows, i.e. 712, which hints at around 3 words per title on average. If you are the curious type, like me, we already can peek at the frequency of words in our dataset.\n\ntitles |&gt;\n  mutate(word2 = fct_lump_n(word, 5,\n                            other_level = \"other words\")) |&gt;\n  count(word2) |&gt;\n  ggplot(aes(x = reorder(word2, n),\n             y = n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThe result is somewhat disappointing. None of these words carries any particular meaning because they are all stop words. Thus, we must clean our data before conducting such an analysis.\nI also sneaked in a new function called fct_lump_n() from the forcats package. It creates a new factor level called other words and ‘lumps’ together all the other factor levels. You likely have seen plots before which show a category called ‘Other’. We usually apply this approach if we have many factor levels with very low frequencies. It would not be meaningful to plot 50 words as a barplot which only occurred once in our data. They are less important. Thus, it is sometimes meaningful to pool factor levels together. The function fct_lump_n(word, 5) returns the five most frequently occurring words and pools the other words together into a new category. There are many different ways to ‘lump’ factors. For a detailed explanation and showcase of all available alternatives, have a look at the forcats website.\nIn our next step, we have to remove stop words using the stop_words data frame and apply the function anti_join().\n\ntitles_no_sw &lt;- anti_join(titles, stop_words,\n                          by = \"word\")\n\ntitles_no_sw\n\n# A tibble: 443 × 2\n   imdb_rating word      \n         &lt;dbl&gt; &lt;chr&gt;     \n 1         9.3 shawshank \n 2         9.3 redemption\n 3         9.2 godfather \n 4         9   dark      \n 5         9   knight    \n 6         9   godfather \n 7         9   ii        \n 8         9   12        \n 9         9   angry     \n10         8.9 lord      \n# ℹ 433 more rows\n\n\nThe dataset shrank from 712 rows to 443 rows. Thus, almost 38% of our data was actually noise. With this cleaner dataset, we can now look at the frequency count again.\n\ntitles_no_sw |&gt;\n  count(word) |&gt;\n  filter(n &gt; 1) |&gt;\n  ggplot(aes(x = reorder(word, n),\n             y = n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nTo keep the number of bars somewhat manageable, I removed those words with more less than 1 occurrence in the dataset. Some of the most frequently occurring words include, for example, wild, wars, story, star, rings, lords. If you know Hollywood movies of the past, you will notice that two movies likely cause this result: The ‘Star Wars’ and ‘Lord of the Rings’ series. We also have numbers included in our dataset. Sometimes these are worth removing, but in this case, the number 2 likely indicates a sequel to another movie. Thus, it provides essential information.\nAnother commonly used method for visualising word frequencies are word clouds. You likely have seen them before since they are very popular, especially for websites and poster presentations. Word clouds use the frequency of words to determine the font size of each word and arrange them so that it resembles the shape of a cloud. The package wordcloud and its function wordcloud() make this very easy. You can modify a word cloud in several ways. Here are some settings I frequently change for my data visualisations:\n\nrandom.order = FALSE: This ensures that the most important words appear in the middle of the plot.\nscale = c(): This attribute determines the size difference between the most and least frequent words in the dataset.\nmin.freq: This setting determines which words are included based on their frequency, which is helpful if you have many words to plot.\nmax.words: This attribute limits the number of words that should be plotted and is also useful in cutting down on large datasets.\ncolors: This setting allows you to provide custom colours for different frequencies of words. This substantially improves the readability of your data visualisation.\n\nTo create a word cloud you have to provide at least a column with words and a column with their frequencies. All other settings are entirely customisable and optional. Here is an example based on the data we just used for our bar plot.\n\n# Create a dataframe with the word count\nword_frequencies &lt;-\n  titles_no_sw |&gt;\n  count(word)\n\n# Plot word cloud\nwordcloud::wordcloud(words = word_frequencies$word,\n                     freq = word_frequencies$n,\n                     random.order = FALSE,\n                     scale = c(2, 0.5),\n                     min.freq = 1,\n                     max.words = 100,\n                     colors = c(\"#6FA8F5\",\n                                \"#FF4D45\",\n                                \"#FFC85E\")\n                     )\n\n\n\n\n\n\n\n\nAfter preparing and visualising our data, it is time to perform our analysis. We want to know whether certain words are associated with higher imdb_ratings. If we use the function count() we will lose the variable imdb_rating(). Thus, we have to do this in three steps:\n\nWe create the frequencies for each word with count(), which we already did.\nThen, we left_join() the word frequencies with the original dataset to add the word frequencies.\nCompute a correlation using correlation() from the correlation package.\n\n\n# Combine word_frequencies with titles_no_sw\ntitles_no_sw &lt;- left_join(titles_no_sw, word_frequencies,\n                          by = \"word\")\n\ntitles_no_sw\n\n# A tibble: 443 × 3\n   imdb_rating word           n\n         &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;\n 1         9.3 shawshank      1\n 2         9.3 redemption     1\n 3         9.2 godfather      2\n 4         9   dark           2\n 5         9   knight         2\n 6         9   godfather      2\n 7         9   ii             1\n 8         9   12             2\n 9         9   angry          1\n10         8.9 lord           3\n# ℹ 433 more rows\n\n\n\n# Compute the correlation of imdb_rating and n\ncorr &lt;-\n  titles_no_sw |&gt;\n  select(imdb_rating, n) |&gt;\n  correlation::correlation()\n\ncorr\n\n# Correlation Matrix (pearson-method)\n\nParameter1  | Parameter2 |    r |       95% CI | t(441) |         p\n-------------------------------------------------------------------\nimdb_rating |          n | 0.18 | [0.09, 0.27] |   3.95 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 443\n\n\n\n# Interpret the effect size\neffectsize::interpret_r(corr$r, rules = \"cohen1988\")\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\nConsidering our correlation analysis, we find that more frequently used words tend to be significantly more successful. However, the effect size is too small to be of particular relevance. Besides, as mentioned before, the frequency of words is skewed towards two movie franchises, both of which have been very successful. Thus, we have to be content that there is no secret formula to create movie titles, i.e. choosing popular words from movie titles will not result in a better ranking.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html#sec-n-grams",
    "href": "14_mixed_methods.html#sec-n-grams",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "14.3 N-grams: Exploring correlations of words",
    "text": "14.3 N-grams: Exploring correlations of words\nBesides looking at words in isolation, it is often more interesting to understand combinations of words to provide much-needed context. For example, the difference between ‘like’ and ‘not like’ can be crucial when trying to understand sentiments in data. The co-occurrence of words follows the same idea as correlations, i.e. how often one word appears together with another. If the frequency of word pairs is high, the relationship between these two words is strong. The technical term for looking at tokens that represent pairs for words is ‘bigram’. If we look at more than two words, we would consider them as ‘n-gram’, where the ‘n’ stands for the number of words.\nCreating a bigram is relatively simple in R and follows similar steps as counting word frequencies:\n\nTurn text into a data frame.\nTokenize the variable that holds the text, i.e. unnest_tokens(),\nSplit the two words into separate variables, e.g. word1 and word2, using separate().\nRemove stop words from both variables listwise, i.e. use filter() for word1 and word2.\nMerge columns word1 and word2 into one column again, i.e. unite() them. (optional)\nCount the frequency of bigrams, i.e. count().\n\nThere are a lot of new functions covered in this part. However, by now, you likely understand how they function already. Let’s proceed step-by-step. For this final example about mixed-methods research, we will look at the synopsis of movies provided by IMDb. Since we already have the text in our data frame, we can skip step one. Next, we need to engage in tokenization. While we use the same function as before, we need to provide different arguments to retrieve bigrams. As before, we need to define an output column and an input column. In addition, we also have to provide the correct type of tokenization, i.e. determine token, which we need to set to \"ngrams\". We also need to define the n in ‘ngrams’, which will be 2 for bigrams. After this, we apply count() to our variable bigram, and we achieved our task. If we turn this into R code, we get the following (with considerably less characters than its explanation):\n\n# Create bigrams from variable synopsis\nsynopsis &lt;-\n  imdb_top_250 |&gt;\n  unnest_tokens(bigram, synopsis, token = \"ngrams\", n = 2) |&gt;\n  select(bigram)\n\nsynopsis\n\n# A tibble: 6,094 × 1\n   bigram        \n   &lt;chr&gt;         \n 1 two imprisoned\n 2 imprisoned men\n 3 men bond      \n 4 bond over     \n 5 over a        \n 6 a number      \n 7 number of     \n 8 of years      \n 9 years finding \n10 finding solace\n# ℹ 6,084 more rows\n\n\n\n# Inspect frequency of bigrams\nsynopsis |&gt; count(bigram, sort = TRUE)\n\n# A tibble: 5,035 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 in the      31\n 2 of the      27\n 3 in a        25\n 4 of a        25\n 5 and his     18\n 6 on a        17\n 7 with the    17\n 8 of his      16\n 9 to find     15\n10 a young     14\n# ℹ 5,025 more rows\n\n\nAs before, the most frequent bigrams are those that contain stop words. Thus, we need to clean our data and remove them. This time, though, we face the challenge that the variable bigram has two words and not one. Thus, we cannot simply use anti_join() because the dataset stop_words only contains individual words, not pairs. To remove stop words successfully, we have to separate() the variable into two variables so that each word has its own column. The package tidyr makes this is an effortless task.\n\nbigram_split &lt;- synopsis |&gt;\n  separate(col = bigram,\n           into = c(\"word1\", \"word2\"),\n           sep = \" \")\n\nbigram_split\n\n# A tibble: 6,094 × 2\n   word1      word2     \n   &lt;chr&gt;      &lt;chr&gt;     \n 1 two        imprisoned\n 2 imprisoned men       \n 3 men        bond      \n 4 bond       over      \n 5 over       a         \n 6 a          number    \n 7 number     of        \n 8 of         years     \n 9 years      finding   \n10 finding    solace    \n# ℹ 6,084 more rows\n\n\nWith this new data frame, we can remove stop words in each variable. Of course, we could use anti_join() as before and perform the step twice, but there is a much more elegant solution. Another method to compare values across vectors/columns is the %in% operator. It is very intuitive to use because it tests whether values in the variable on the left of %in% exist in the variable to the right. Let’s look at a simple example. Assume we have an object that contains the names of people related to family and work. We want to know whether people in our family are also found in the list of work. If you remember Table @ref(tab:logical-operators-r), which lists all logical operators, you might be tempted to try family == work. However, this would assess the object in its entirety and not tell us which values, i.e. which names, can be found in both lists.\n\n# Define the two\nfamily &lt;- tibble(name = c(\"Fiona\", \"Ida\", \"Lukas\", \"Daniel\"))\nwork &lt;- tibble(name = c(\"Fiona\", \"Katharina\", \"Daniel\"))\n\n# Compare the two objects using '=='\nfamily$name == work$name\n\nWarning in family$name == work$name: longer object length is not a multiple of\nshorter object length\n\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nAll but one value in family and work are not equal, i.e. they are FALSE. This is because == compares the values in order. If we changed the order, some of the FALSE values would turn TRUE. In addition, we get a warning telling us that these two objects are not equally long because family holds four names, while work contains only three names. Lastly, the results are not what we expected because Fiona and Daniel appear in both objects. Therefore we would expect that there should be two TRUE values. In short, == is not the right choice to compare these two objects.\nIf we use %in% instead, we can test whether each name appears in both objects, irrespective of their length and the order of the values.\n\n# Compare the two objects using '%in%'\nfamily$name %in% work$name\n\n[1]  TRUE FALSE FALSE  TRUE\n\n\nIf we want R to return the values that are the same across both objects, we can ask: which() names are the same?\n\n# Compare the two objects using '%in%'\n(same_names &lt;- which(family$name %in% work$name))\n\n[1] 1 4\n\n\nThe numbers returned by which() refer to the position of the value in our object. In our case, the first value in family is Fiona, and the fourth value is Daniel. It is TRUE that these two names appear in both objects. If you have many values that overlap in a large dataset, you might not want to know the row number but retrieve the actual values. This can be achieved by slice()ing our dataset, i.e. filtering our data frame by providing row numbers.\n\n# Retrieve the values which exist in both objects\nfamily |&gt;\n  slice(same_names)\n\n# A tibble: 2 × 1\n  name  \n  &lt;chr&gt; \n1 Fiona \n2 Daniel\n\n\nWhile this might be a nice little exercise, it is important to understand how %in% can help us remove stop words. Technically, we try to achieve the opposite, i.e. we want to keep the values in word1 and word2 that are not a word in stop_words. If we use the language of dplyr, we filter() based on whether a word in bigram_split is not %in% the data frame stop_words. Be aware that %in% requires a variable and not an entire data frame to work as we intend.\n\nbigram_cleaned &lt;-\n  bigram_split |&gt;\n  filter(!word1 %in% stop_words$word) |&gt;\n  filter(!word2 %in% stop_words$word)\n\nbigram_cleaned &lt;- bigram_cleaned |&gt;\n  count(word1, word2, sort = TRUE)\n\nbigram_cleaned\n\n# A tibble: 1,246 × 3\n   word1   word2        n\n   &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;\n 1 world   war          8\n 2 war     ii           7\n 3 darth   vader        3\n 4 gotham  city         3\n 5 serial  killer       3\n 6 vietnam war          3\n 7 york    city         3\n 8 24      hours        2\n 9 adolf   hitler's     2\n10 army    officer      2\n# ℹ 1,236 more rows\n\n\nThe result is a clean dataset, which reveals that world and war are the most common bigram in the synopsis of our Top 250 IMDb movies.\nSometimes we might wish to re-unite() the two columns that we separated. For example, when plotting the results into a ggplot(), we need both words in one column1 to use the actual bigrams as labels.\n\nbigram_cleaned |&gt;\n  unite(col = bigram,\n        word1, word2,\n        sep = \" \")\n\n# A tibble: 1,246 × 2\n   bigram             n\n   &lt;chr&gt;          &lt;int&gt;\n 1 world war          8\n 2 war ii             7\n 3 darth vader        3\n 4 gotham city        3\n 5 serial killer      3\n 6 vietnam war        3\n 7 york city          3\n 8 24 hours           2\n 9 adolf hitler's     2\n10 army officer       2\n# ℹ 1,236 more rows\n\n\nSo far, we primarily looked at frequencies as numbers in tables or as bar plots. However, it is possible to create network plots of words with bigrams by drawing a line between word1 and word2. Since there will be overlaps across bigrams, they would mutually connect and create a network of linked words.\nI am sure you have seen visualisations of networks before but might not have engaged with them on a more analytical level. A network plot consists of ‘nodes’, which represent observations in our data, and ‘edges’, which represent the link between nodes. We need to define both to create a network.\nUnfortunately, ggplot2 does not enable us to easily create network plots, but the package ggraph offers such features using the familiar ggplot2 syntax. Network plots are usually made using specific algorithms to arrange values in a network efficiently. Thus, if you want your plots to be reproducible, you have to set.seed() in advance. This way, random aspects of some algorithms are set constant.\nThere is one more complication, but with a simple solution. The function ggraph(), which is an equivalent to ggplot(), requires us to create a graph object that can be used to plot networks. The package igraph has a convenient function that produces a graph_from_data_frame(). As a final step we need to choose a layout for the network. This requires some experimentation. Details about different layouts and more for the ggraph package can be found on its website.\n\nlibrary(ggraph)\n\n# Make plot reproducible\nset.seed(1234)\n\n# Create the special igraph object\ngraph &lt;- igraph::graph_from_data_frame(bigram_cleaned)\n\n# Plot the network graph\ngraph |&gt;\n  ggraph(layout = \"kk\") +  # Choose a layout\n  geom_edge_link() +       # Draw lines between nodes\n  geom_node_point()        # Add node points\n\n\n\n\n\n\n\n\nThe result looks like a piece of modern art. Still, it is not easy to understand what the plot shows us. Thus, we need to remove some bigrams that are not so frequent. For example, we could remove those that only appear once.\n\n# Filter bigrams\nnetwork_plot &lt;- bigram_cleaned |&gt;\n  filter(n &gt; 1) |&gt;\n  igraph::graph_from_data_frame()\n\n# Plot network plot\nnetwork_plot |&gt;\n  ggraph(layout = \"kk\") +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\nIt seems that many bigrams were removed. Thus, the network plot looks much smaller and less busy. Nevertheless, we cannot fully understand what this visualisation shows because there are no labels for each node. We can add them using geom_node_text(). I also recommend offsetting the labels with vjust (vertical adjustment) and hjust (horizontal adjustment), making them easier to read. To enhance the network visualisation further, we could colour the edges based on the frequency of each token.\n\n# Plot network plot\nnetwork_plot |&gt;\n  ggraph(layout = \"kk\") +\n  geom_edge_link(aes(col = factor(n))) +\n  geom_node_point() +\n  geom_node_text(aes(label = name),\n                 vjust = 1,\n                 hjust = 1)\n\n\n\n\n\n\n\n\nThe final plot shows that war and other military aspects, e.g. soldiers, army are very prevalent in our dataset. Besides, we also find some famous movie characters in our visualisation, such as darth vader, luke skywalker or indiana jones.\nOur example is a simple analysis based on a relatively small ‘corpus’, i.e. a small dataset. However, the application of n-grams in larger datasets can reveal topic areas and links between them. There are more approaches to exploring topics in large datasets, for example, ‘topic modelling’, which are far more complex, but offer more sophisticated analytical insights.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html#exploring-more-mixed-methods-research-approaches-in-r",
    "href": "14_mixed_methods.html#exploring-more-mixed-methods-research-approaches-in-r",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "14.4 Exploring more mixed-methods research approaches in R",
    "text": "14.4 Exploring more mixed-methods research approaches in R\nThis chapter can only be considered a teaser for mixed-methods research in R. There is much more to know and much more to learn. If this type of work is of interest, especially when working with social media data or historical text documents, there are several R packages I can recommend:\n\ntm: This text mining package includes many functions that help to explore documents systematically.\nquanteda: This package offers natural language processing features and incorporates functions that are commonly seen in corpus analysis. It also provides tools to visualise frequencies of text, for example, wordclouds.\ntopicmodels: To analyse topics in large datasets, it is necessary to use special Natural Language Processing techniques. This package offers ways to perform Latent Dirichlet Allocation (LDA) and Correlated Topic Models (CTM). Both are exciting and promising pathways to large-scale text analysis.\nThe tidytext package we used above also has a lot more to offer than what we covered, for example, performing sentiment analyses.\n\nIf you want to explore the world of textual analysis in greater depth, there are two fantastic resources I wholeheartedly can recommend:\n\nText mining with tidy data principles: This free online course introduces additional use cases of the tidytext package by letting you work with data interactively right in your browser. Julia Silge developed these materials.\nSupervised Machine Learning for Text Analysis in R: This book is the ideal follow-up resource if you worked your way through Silge and Robinson (2017). It introduces the foundations of natural language analysis and covers advanced text analysis methods such as regression, classification, and deep learning.\n\nI hope I managed to showcase the versatility of R to some degree in this chapter. Needless to say, there is also a lot I still have to learn, which makes using R exciting and, more than once, has inspired me to approach my data in unconventional ways.\n\n\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. \"O’Reilly Media, Inc.\".",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "14_mixed_methods.html#footnotes",
    "href": "14_mixed_methods.html#footnotes",
    "title": "14  Mixed-methods research: Analysing qualitative data in R",
    "section": "",
    "text": "With glue::glue() you can also combine values from two different columns directly in ggplot(), but it seems simpler to just use unite(). Remember, there is always more than one way of doing things in R.↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Mixed-methods research: Analysing qualitative data in *R*</span>"
    ]
  },
  {
    "objectID": "15_next_steps.html",
    "href": "15_next_steps.html",
    "title": "15  Where to go from here: The next steps in your R journey",
    "section": "",
    "text": "15.1 GitHub: A Gateway to even more ingenious R packages\nIf you feel you want more R or you are curious to know what other R packages can do to complement your analysis, then GitHub brings an almost infinite amount of options for you. Besides the CRAN versions of packages, you find development versions of R packages on Github. These usually incorporate the latest features but are not yet available through CRAN. Having said that, if you write your next publication, it might be best to work with packages that are released on CRAN. These are the most stable versions. After all, you don’t want the software to fail on you and hinder you from making that world-changing discovery.\nStill, more and more R packages are released every day that offer the latest advancements in statistical computations and beyond. Methods covered in Chapter @ref(mixed-methods-research) are a great example of what has become possible with advanced programming languages. So if you want to live at the bleeding edge of innovative research methods, then look no further.\nHowever, GitHub also serves another purpose: To back up your research projects and work collaboratively with others. I strongly encourage you to create your own GitHub account, even just to try it. RStudio has built-in features for working with GitHub, making it easy to keep track of your analysis and ensure regular backups. Nobody wants to clean up their data over months and lose it because their cat just spilt the freshly brewed Taiwanese High Mountain Oolong Tea over one’s laptop. I use GitHub for many different things, hosting my blog (The Social Science Sofa), research projects, and this book.\nThere is much more to say about GitHub that cannot be covered in this book, but if you seek an introduction, you might find GitHub’s Youtube Channel of interest or their ‘Get started’ guide. Best of all, setting up and using your GitHub account is free.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Where to go from here: The next steps in your *R* journey</span>"
    ]
  },
  {
    "objectID": "15_next_steps.html#sec-next-steps-books",
    "href": "15_next_steps.html#sec-next-steps-books",
    "title": "15  Where to go from here: The next steps in your R journey",
    "section": "15.2 Books to read and expand your knowledge",
    "text": "15.2 Books to read and expand your knowledge\nThere are undoubtedly many more books to read, explore and use. However, my number one recommendation to follow up with this book is ‘R for Data Science’. It takes your R coding beyond the context of Social Sciences and introduces new aspects, such as ‘custom functions’ and ‘looping’. These are two essential techniques if you, for example, have to fit a regression model for +20 subsets of your data, create +100 plots to show results for different countries, or need to create +1000 of individualised reports for training participants. In short, these are very powerful (and efficient) techniques you should learn sooner than later but are less essential for a basic understanding of data analysis in R for the Social Sciences.\nIf you want to expand your repertoire regarding data visualisations, a fantastic starting point represents ‘ggplot2: Elegant Graphics for Data Analysis’ and ‘Fundamentals of Data Visualization’. However, these days I am looking mostly for inspiration and new packages that help me create unique, customised plots for my presentations and publications. Therefore, looking at the source code of plots you like (for example, on GitHub) is probably the best way to learn about ggplot2 and some new techniques of how to achieve specific effects (see also Chapter @ref(next-steps-twitter)).\nIf you are a qualitative researcher, you might be more interested in what else you can do with R to systematically analyse large amounts of textual data (as was shown in Chapter @ref(mixed-methods-research)). I started with the excellent book ‘Text Mining with R: A Tidy Approach’, which introduces you in greater depth to sentiment analysis, correlations, n-grams, and topic modelling. The lines of qualitative and quantitative research become increasingly blurred. Thus, learning these techniques will be essential moving forward and pushing the boundaries of what is possible with textual data.\nR can do much more than just statistical computing and creating pretty graphs. For example, you can write your papers with it, even if you do not write a single line of code. From Chapter @ref(r-markdown-and-r-notebooks), you might remember that I explained that R Markdown files are an alternative to writing R scripts. Suppose you want to deepen your knowledge in this area and finally let go of Microsoft Word, I encourage you to take a peek at the ‘R Markdown Cookbook’ for individual markdown files and ‘bookdown: Authoring Books and Technical Documents with R Markdown’ for entire manuscripts, e.g. journal paper submissions or books. I almost entirely abandoned Microsoft Word, even though it served me well for so many years - thanks.\nLastly, I want to make you aware of another open source book that covers What They Forgot to Teach you About R by Jennifer Bryan and Jim Hester. It is an excellent resource to get some additional insights into how one should go about working in R and RStudio.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Where to go from here: The next steps in your *R* journey</span>"
    ]
  },
  {
    "objectID": "15_next_steps.html#sec-next-steps-online-readings",
    "href": "15_next_steps.html#sec-next-steps-online-readings",
    "title": "15  Where to go from here: The next steps in your R journey",
    "section": "15.3 Engage in regular online readings about R",
    "text": "15.3 Engage in regular online readings about R\nA lot of helpful information for novice and expert R users is not captured in books but online blogs. There are several that I find inspiring and an excellent learning platform, each focusing on different aspects.\nThe most comprehensive hub for all things R is undoubtedly ‘R-bloggers’. It is a blog aggregator which focuses on collecting content related to R. I use it regularly to read more about new packages, new techniques, helpful tricks to become more ‘fluent’ in R or simply find inspiration for my own R packages. More than once, I found interesting blogs by just reading posts on ‘R-bloggers’. For example, the day I wrote this chapter, I learned about emayili, which allows you to write your emails from R using R markdown. So not even the sky is the limit, it seems.\nAnother blog worth considering is the one from ‘Tidyverse’. This blog is hosted and run by RStudio and covers all packages within the tidyverse. Posts like ‘waldo 0.3.0’ made my everyday data wrangling tasks a lot easier because finding the differences between two datasets can be like searching a needle in a haystack. For example, it is not unusual to receive two datasets that contain the same measures, but some of their column names are slightly different, which does not allow us to merge them in the way we want quickly. I previously spent days comparing and arranging multiple datasets with over 100 columns. Let me assure you, it is not really fun to do.\nThe blog ‘Data Imaginist’ by Thomas Lin Pedersen covers various topics around data visualisations. He is well known for his Generative Art, i.e. art that is computationally generated. But one of my favourite packages, patchwork, was written by him and gained immense popularity. It has never been easier to arrange multiple plots with such little code.\nLastly, I want to share with you the blog of Cédric Scherer for those of you who want to learn more about high-quality data visualisations based on ggplot2. His website hosts many visualisations with links to the source code on GitHub to recreate them yourself. It certainly helped me improve the visual storytelling of my research projects.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Where to go from here: The next steps in your *R* journey</span>"
    ]
  },
  {
    "objectID": "15_next_steps.html#sec-next-steps-twitter",
    "href": "15_next_steps.html#sec-next-steps-twitter",
    "title": "15  Where to go from here: The next steps in your R journey",
    "section": "15.4 Join the Twitter community and hone your skills",
    "text": "15.4 Join the Twitter community and hone your skills\nLearning R means you also join a community of like-minded programmers, researchers, hobbyists and enthusiasts. Whether you have a Twitter account or not, I recommend looking at the #RStats community there. Plenty of questions about R programming are raised and answered on Twitter. In addition, people like to share insights from their projects, often with source code published on GitHub. Even if you are not active on social media, it might be worth having a Twitter account just to receive news about developments in R programming.\nAs with any foreign language, if we do not use it regularly, we easily forget it. Thus, I would like to encourage you to take part in Tidy Tuesday. It is a weekly community exercise around data visualisation and data wrangling. In short: You download a dataset provided by the community, and you are asked to create a visualisation as simple or complex as you wish. Even if you only manage to participate once a month, it will make you more ‘fluent’ in writing your code. Besides, there is a lot to learn from others because you are also asked to share the source code. This activity takes place on Twitter, and you can find contributions by using #TidyTuesday. Whether you want to share your plots is up to you, but engaging with this activity will already pay dividends. Besides, it is fun to work with datasets that are not necessarily typical for your field.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Where to go from here: The next steps in your *R* journey</span>"
    ]
  },
  {
    "objectID": "16_case_studies.html",
    "href": "16_case_studies.html",
    "title": "16  Case studies",
    "section": "",
    "text": "16.1 Bootstrapped regression with multi-level control variable and moderation\n# # Bootstrapping the regression\n#\n# parameters::compare_parameters(m1, m2) |&gt;\n#   as_tibble() |&gt;\n#   select(Parameter, Coefficient.m1, Coefficient.m2)\n#\n# # create bootstrapped samples\n# set.seed(1234)\n# boot_sample &lt;- rsample::bootstraps(m1_outliers,\n#                                            times = 500)\n#\n# # Run lm with bootstrapped samples\n#\n# split &lt;- function(split) {\n#   lm(lm(new_cases ~ masks + movements + gatherings + schools + businesses + travel,\n#         data = rsample::analysis(split)))\n# }\n#\n# mb &lt;- boot_sample |&gt;\n#   mutate(lm_mod = map(splits, split),\n#          estimates = map(lm_mod, broom::tidy))\n#\n# model_sum &lt;-\n#   mb |&gt;\n#   unnest(estimates) |&gt;\n#   filter(term != \"(Intercept)\") |&gt;\n#   group_by(term) |&gt;\n#   mutate(estimate = scale(estimate)) |&gt;\n#   ungroup()\n#\n#\n# model_sum |&gt;\n#   ggplot(aes(x = estimate,\n#              y = term,\n#              fill = term)) +\n#   ggridges::geom_density_ridges()\n#\n# #check for multicollinearity\n#\n# ## Durbin-Watson Test\n# car::durbinWatsonTest()",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "18_epilogue.html",
    "href": "18_epilogue.html",
    "title": "Epilogue",
    "section": "",
    "text": "Congratulations! You made it through the book, or you started reading it from the back. Either way, I hope that spending your valuable time with this book was/is enjoyable, insightful and hopefully helpful in whatever grand or small questions you intend to answer through your research.\nFor my part, it has been a very enjoyable experience to put these materials together for those who need them. I also learned several new aspects about R I did not know before, for example, how to use bookdown to write this book. My biggest hope is to convince more researchers in the Social Sciences to try something new that can enrich their work. Whether you decide to adopt R for all your work, intend to move on to other languages like Python, or give up on it entirely, it does not matter. At least you tried, and you are more knowledgeable. Whenever we learn something new, it requires us to push ourselves out of our comfort zone, and I hope this book helped with that.\nWhichever path you choose, I hope your research will have an impact, which, of course, does not depend on the tools you use.",
    "crumbs": [
      "Epilogue"
    ]
  },
  {
    "objectID": "99_appendix.html",
    "href": "99_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Comparing groups",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "99_appendix.html#sec-comparing-two-unpaired-groups",
    "href": "99_appendix.html#sec-comparing-two-unpaired-groups",
    "title": "Appendix",
    "section": "Comparing two unpaired groups",
    "text": "Comparing two unpaired groups\n\n\n\nTable 1.1: Comparing two unpaired groups (effect size functions from package effectsize, except for wilcoxonR() from rcompanion\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction\nEffect size\nFunction\n\n\n\n\nParametric\nT-Test\nWelch T-Test\nt.test(var.equal = TRUE)\nt.test(var.equal = FALSE)\nCohen’s d\ncohens_d()\n\n\nNon-parametric\nMann-Whitney U\nwilcox.test(paired = FALSE)\nRank-biserial r\nor\nWilcoxon R\nrank_biserial()\nor\nwilcoxonR()",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "99_appendix.html#sec-comparing-two-paired-groups",
    "href": "99_appendix.html#sec-comparing-two-paired-groups",
    "title": "Appendix",
    "section": "Comparing two paired groups",
    "text": "Comparing two paired groups\n\n\n\nTable 1.2: Comparing two unpaired groups (effect size functions from package effectsize, except for wilcoxonPairedR() from rcompanion)\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction for test\nEffect size\nFunction for effect size\n\n\n\n\nParametric\nT-Test\nt.test(paired = TRUE)\nCohen’s d\ncohens_d()\n\n\nNon-parametric\nWilcoxon Signed Rank Test\nwilcox.test(paired = TRUE)\nRank biserial r\nor\nWilcoxon r\nrank_biserial()\nor\nwilcoxonPairedR()",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "99_appendix.html#sec-comparing-multiple-unpaired-groups",
    "href": "99_appendix.html#sec-comparing-multiple-unpaired-groups",
    "title": "Appendix",
    "section": "Comparing multiple unpaired groups",
    "text": "Comparing multiple unpaired groups\n\n\n\nTable 1.3: Comparing multiple unpaired groups (effect size functions from package effectsize)\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nTest\nFunction for test\nEffect size\nFunction for effect size\n\n\n\n\nParametric\nANOVA\n\naov() (assumes equal variances)\noneway.test(var.equal = TRUE/FALSE)\n\nEpsilon squared\neta_squared()\n\n\nNon-parametric\nKruskall-Wallis test\nkruskal.test()\nEpsilon squared (rank)\nrank_epsilon_squared()",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aguinis, Herman, Ryan K Gottfredson, and Harry Joo. 2013.\n“Best-Practice Recommendations for Defining, Identifying, and\nHandling Outliers.” Organizational Research Methods 16\n(2): 270–301.\n\n\nAltman, Douglas G. 1985. “Comparability of Randomised\nGroups.” Journal of the Royal Statistical Society: Series D\n(The Statistician) 34 (1): 125–36.\n\n\nBlanca Mena, M José, Rafael Alarcón Postigo, Jaume Arnau Gras, Roser\nBono Cabré, and Rebecca Bendayan. 2017. “Non-Normal Data: Is ANOVA\nStill a Valid Option?” Psicothema, 2017, Vol. 29, Num. 4, p.\n552-557.\n\n\nBoehmke, Brad, and Brandon Greenwell. 2019. Hands-on Machine\nLearning with r. Chapman; Hall/CRC.\n\n\nBuuren, Stef van. 2018. Flexible Imputation of Missing Data.\nCRC press.\n\n\nCambridge Dictionary. 2021. “Concatenate.” https://dictionary.cambridge.org/dictionary/english/concatenate.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences New York. NY: Academic Press.\n\n\nCohen, Patricia, Stephen G West, and Leona S Aiken. 2014. Applied\nMultiple Regression/Correlation Analysis for the Behavioral\nSciences. Psychology press.\n\n\nCook, R Dennis, and Sanford Weisberg. 1982. Residuals and Influence\nin Regression. New York: Chapman; Hall.\n\n\nCousineau, Denis, and Sylvain Chartier. 2010. “Outliers Detection\nand Treatment: A Review.” International Journal of\nPsychological Research 3 (1): 58–67.\n\n\nDinno, Alexis. 2015. “Nonparametric Pairwise Multiple Comparisons\nin Independent Groups Using Dunn’s Test.” The Stata\nJournal 15 (1): 292–300.\n\n\nDong, Yiran, and Chao-Ying Joanne Peng. 2013. “Principled Missing\nData Methods for Researchers.” SpringerPlus 2 (1): 1–17.\n\n\nField, Andy. 2013. Discovering Statistics Using IBM SPSS\nStatistics. Sage Publications.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nGinkel, Joost R van, Marielle Linting, Ralph CA Rippe, and Anja van der\nVoort. 2020. “Rebutting Existing Misconceptions about Multiple\nImputation as a Method for Handling Missing Data.” Journal of\nPersonality Assessment 102 (3): 297–308.\n\n\nGrandstrand, Ove. 2004. “Durbin-Watson Statistic.” In\nThe SAGE Encyclopedia of Social Science Research Methods,\nedited by Bryman Lewis-Beck Michael S. Thousand Oaks, California. https://methods.sagepub.com/reference/the-sage-encyclopedia-of-social-science-research-methods.\n\n\nGreenhouse, Samuel W, and Seymour Geisser. 1959. “On Methods in\nthe Analysis of Profile Data.” Psychometrika 24 (2):\n95–112.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nHenson, Robin K. 2001. “Understanding Internal Consistency\nReliability Estimates: A Conceptual Primer on Coefficient Alpha.”\nMeasurement and Evaluation in Counseling and Development 34\n(3): 177–89.\n\n\nHoaglin, David C, and Roy E Welsch. 1978. “The Hat Matrix in\nRegression and ANOVA.” The American Statistician 32 (1):\n17–22.\n\n\nHochberg, Yosef. 1974. “Some Generalizations of the t-Method in\nSimultaneous Inference.” Journal of Multivariate\nAnalysis 4 (2): 224–34.\n\n\n———. 1988. “A Sharper Bonferroni Procedure for Multiple Tests of\nSignificance.” Biometrika 75 (4): 800–802.\n\n\nHu, Li-tze, and Peter M Bentler. 1999. “Cutoff Criteria for Fit\nIndexes in Covariance Structure Analysis: Conventional Criteria Versus\nNew Alternatives.” Structural Equation Modeling: A\nMultidisciplinary Journal 6 (1): 1–55.\n\n\nHuynh, Huynh, and Leonard S Feldt. 1976. “Estimation of the Box\nCorrection for Degrees of Freedom from Sample Data in Randomized Block\nand Split-Plot Designs.” Journal of Educational\nStatistics 1 (1): 69–82.\n\n\nJakobsen, Janus Christian, Christian Gluud, Jørn Wetterslev, and Per\nWinkel. 2017. “When and How Should Multiple Imputation Be Used for\nHandling Missing Data in Randomised Clinical Trials–a Practical Guide\nwith Flowcharts.” BMC Medical Research Methodology 17\n(1): 1–10.\n\n\nLandis, J Richard, and Gary G Koch. 1977. “The Measurement of\nObserver Agreement for Categorical Data.” Biometrics,\n159–74.\n\n\nLeys, Christophe, Christophe Ley, Olivier Klein, Philippe Bernard, and\nLaurent Licata. 2013. “Detecting Outliers: Do Not Use Standard\nDeviation Around the Mean, Use Absolute Deviation Around the\nMedian.” Journal of Experimental Social Psychology 49\n(4): 764–66.\n\n\nLittle, Roderick J. A. 1988. “A Test of Missing Completely at\nRandom for Multivariate Data with Missing Values.” Journal of\nthe American Statistical Association 83 (404): 1198–1202. https://doi.org/10.1080/01621459.1988.10478722.\n\n\nMüller, Kirill, and Hadley Wickham. 2021. “Column Data\nTypes.” https://tibble.tidyverse.org/articles/types.html.\n\n\nNunally, J. C. 1967. Psychometric Theory. New york: Mc\nGraw-Hill.\n\n\n———. 1978. Psychometric Theory (2nd Edition). New york: Mc\nGraw-Hill.\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92.\n\n\nSchafer, Joseph L. 1999. “Multiple Imputation: A Primer.”\nStatistical Methods in Medical Research 8 (1): 3–15.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy\nApproach. \"O’Reilly Media, Inc.\".\n\n\nStevens, James P. 2012. Applied Multivariate Statistics for the\nSocial Sciences. Routledge.\n\n\nStobierski, Tim. 2021. “Data Wrangling: What It Is and Why It’s\nImportant.” Harvard Business School Online. https://online.hbs.edu/blog/post/data-wrangling.\n\n\nTomarken, Andrew J, and Ronald C Serlin. 1986. “Comparison of\nANOVA Alternatives Under Variance Heterogeneity and Specific\nNoncentrality Structures.” Psychological Bulletin 99\n(1): 90.\n\n\nWest, Stephen G, Aaron B Taylor, Wei Wu, et al. 2012. “Model Fit\nand Model Selection in Structural Equation Modeling.”\nHandbook of Structural Equation Modeling 1: 209–31.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (1): 1–23.\n\n\n———. 2021. The Tidyverse Style Guide. https://style.tidyverse.org.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. O’Reilly\nMedia, Inc.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown:\nThe Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nYang, Zhilin, Xuehua Wang, and Chenting Su. 2006. “A Review of\nResearch Methodologies in International Business.”\nInternational Business Review 15 (6): 601–17.\nhttps://doi.org/https://doi.org/10.1016/j.ibusrev.2006.08.003.",
    "crumbs": [
      "References"
    ]
  }
]